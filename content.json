{"meta":{"title":"lightsmile's Blog","subtitle":"lightsmile","description":"this is a description","author":"lightsmile","url":"http://www.iamlightsmile.com"},"pages":[{"title":"","date":"2018-10-01T10:39:03.456Z","updated":"2017-12-10T11:20:34.000Z","comments":true,"path":"404.html","permalink":"http://www.iamlightsmile.com/404.html","excerpt":"","text":"闲人免进 嘻嘻！就是不给你看！！！"},{"title":"categories","date":"2017-12-21T08:38:30.000Z","updated":"2017-12-21T08:38:48.000Z","comments":false,"path":"categories/index.html","permalink":"http://www.iamlightsmile.com/categories/index.html","excerpt":"","text":""},{"title":"friends","date":"2017-12-09T05:16:32.000Z","updated":"2019-01-07T03:33:32.703Z","comments":true,"path":"friends/index.html","permalink":"http://www.iamlightsmile.com/friends/index.html","excerpt":"","text":"。。。暂时还没有。。。"},{"title":"about","date":"2017-12-09T05:16:32.000Z","updated":"2019-11-14T12:57:03.000Z","comments":true,"path":"about/index.html","permalink":"http://www.iamlightsmile.com/about/index.html","excerpt":"","text":"基础信息 姓名：李德方 性别：男 出生年月：1996.3 就读大学：武汉理工大学 所在院校：计算机科学与技术 专业技能与程度 专业技能 所学程度 备注 自然语言处理 中等 工作后主要工作方向 知识图谱 基础 工作后主要工作方向相关 深度学习 基础 工作后主要工作方向相关 机器学习 基础 工作后主要工作方向相关 数据结构与算法 基础 大学课程水平 Android开发 基础 大学自学，几门课程大作业都是开发的Android应用，有后台可联网的非demo 前端 略知一二 曾学过一段时间，写了一个用于获取Github图片地址的浏览器插件，以及结合aardio写了个mini电商系统 桌面开发 基础 大学自学，主要使用aardio语言，个人很喜欢 小程序 略知一二 在前端的基础上，编写了了todoList作为个人毕业设计 Linux 略知一二 工作后主要在Linux环境下开发 Java 基础 大学课程，曾开发demo管理系统 Python 中等 工作后主要工作语言 个人评价 自学能力较强 责任心比较重 沟通能力良好 自我驱动比较强 其他相关 个人博客站点 Github 简书 知乎 联系方式 手机：15172335885 QQ：1459679436 邮箱：iamlightsmile@qq.com 、iamlightsmile@gmail.com"},{"title":"tags","date":"2017-12-21T08:37:01.000Z","updated":"2017-12-21T08:38:12.000Z","comments":false,"path":"tags/index.html","permalink":"http://www.iamlightsmile.com/tags/index.html","excerpt":"","text":""},{"title":"projects","date":"2017-12-09T05:16:32.000Z","updated":"2019-04-04T01:42:21.040Z","comments":true,"path":"projects/index.html","permalink":"http://www.iamlightsmile.com/projects/index.html","excerpt":"","text":"项目主要包括： 项目名称 简介 相关技术 开发语言 lightNLP 自然语言处理深度学习框架 NLP，深度学习 Python lightKG 知识图谱深度学习框架 KG，深度学习 Python todolist 微信小程序：微计划日程管理 微信小程序 JavaScript GithubImagePace 获取Github图片路径用作Markdown地址的浏览器插件 浏览器插件 JavaScript SchoolInfoPublishSystem Android App：校园信息发布系统 Android Java MyShoppingWeb 网上购物数据库demo系统 aardio aardio 1.lightNLP基于Pytorch和torchtext的自然语言处理深度学习框架，包含序列标注、文本分类、句子关系、文本生成、结构分析、五大功能模块，已实现了命名实体识别、中文分词、词性标注、语义角色标注、情感分析、关系抽取、语言模型、文本相似度、文本蕴含、依存句法分析等功能。框架功能丰富，开箱可用，极易上手！基本都是学习他人实现然后自己修改融合到框架中，没有细致调参，且有不少Bug～ 2.lightKG基于Pytorch和torchtext的知识图谱深度学习框架，包含知识表示学习、实体识别与链接、实体关系抽取、事件检测与抽取、知识存储与查询、知识推理六大功能模块，已实现了命名实体识别、关系抽取、事件抽取、表示学习等功能。框架功能丰富，开箱可用，极易上手！基本都是学习他人实现然后自己修改融合到框架中，没有细致调参，且有不少Bug～ 3.todolist微信小程序:微计划日程管理 4.GithubImagePace我的获取GitHub图片绝对路径用于Markdown文档图片的浏览器插件 5.QCloudSDKJS腾讯云SDK for JS 6.SchoolInfoPublishSystemAndroid App:校园信息发布系统 7.MyShoppingWebaardio 数据库作业：网上购物数据库综合系统"}],"posts":[{"title":"Github上开源Python仓库维护流程","slug":"Github上开源Python仓库维护流程","date":"2019-12-31T03:51:25.000Z","updated":"2019-12-31T04:12:12.272Z","comments":true,"path":"articles/Github上开源Python仓库维护流程/","link":"","permalink":"http://www.iamlightsmile.com/articles/Github上开源Python仓库维护流程/","excerpt":"stepsstep1： 明确更新缘由比如说有用户提出bug的issue，则可以考虑在空闲时分析问题甚至重现问题以寻找到bug，然后修改代码修复bug。 又如对项目进行重构或增加新的feature等，也相应的修改代码。","text":"stepsstep1： 明确更新缘由比如说有用户提出bug的issue，则可以考虑在空闲时分析问题甚至重现问题以寻找到bug，然后修改代码修复bug。 又如对项目进行重构或增加新的feature等，也相应的修改代码。 step2：对修改后代码进行测试对修改后的项目代码进行测试，以确保问题修复，或代码的修改完善并未影响其他的代码的正常运行。 step3: 根据需要修改项目配置和说明文件 在README.md中做说明。 修改setup.py文件中版本号 step4: 提交代码到Github仓库git的add、commit、push三连。 step5: 编译项目并更新到pypi源中参考我的另一篇文章：python库打包分发 | lightsmile’s Blog step6: 友好回复提出问题的用户遵守“富强、民主、文明、和谐、自由、平等、公正、法治、爱国、敬业、诚信、友善”社会主义核心价值观，和用户进行友好沟通，告知用户已解决问题或其他说明。","categories":[],"tags":[]},{"title":"2019回忆总结录","slug":"2019回忆总结录","date":"2019-12-30T12:00:48.000Z","updated":"2019-12-30T13:26:28.382Z","comments":true,"path":"articles/2019回忆总结录/","link":"","permalink":"http://www.iamlightsmile.com/articles/2019回忆总结录/","excerpt":"时光匆匆如流水，眨眼间，2019年就这么过去了。距离你好，2019也马上就一年整了。","text":"时光匆匆如流水，眨眼间，2019年就这么过去了。距离你好，2019也马上就一年整了。 总体回顾这一年，也算奋斗和荒废都有。由于要考研以及积累学习，所以在2018年12月底辞职，然后回家。 在1月份，主要是学习Pytorch和torchtext，开始探索看项目源码。同时计划当一个网上的家教老师，平台是掌门，准备了一段时间，结果面试没有通过，妈了个巴子的。 在2月份开始鼓捣Manjaro，期间重装系统多次，真是痛并快乐着，现在回想起来：一方面在探索的过程中遇到困难，解决困难的历程很爽；另一方面说实在的，并没有积累到太多实用的计算机技术，有点浪费时间了（ps：Linux系统还是单独装吧，装个双系统放在普通硬盘里实在是挺慢的，可能和KDE环境占用内存较大也有关系；最近出于学习效率还是回到了Windows的怀抱，啊好香啊）。其间报了个上海的创意编程（就是教Scratch）老师，也经历了准备面试、远程面试通过等过程，然而在培训过程中感觉不太好于是主动退出了） 到了3月份，之前在网易云课堂报的自然语言处理工程师微专业开班了，于是主要是学习课程资源，积累相关知识；同时开始在Github上寻觅使用Pytorch实现的nlp任务的实现（最开始是文本分类，之后是命名实体识别），开始了不断学习源码之路；其间萌生了用torchtext照着实现一遍以增进自己对任务的理解、以及提升自己的实践动手能力的想法，便有了最初的lightNLP。同时也想着开发其他框架供自己使用，如lightKG、lightText、lightUtils。当时的定位主要是： lightNLP：lightNLP基于Pytorch和torchtext是一个深度学习自然语言处理框架，实现了如文本分类、命名实体识别、语句相似度、文本蕴涵、依存句法分析等功能。 lightKG：lightsmile个人的知识图谱框架 lightUtils：lightsmile个人的工具类库 lightText：lightText是中文文本处理框架，在设计上会包含各种词典资源，一些算法的传统或机器学习实现，包含广泛且更高级的功能，如对文本进行分析，抽取出关键词、分类、进行信息抽取等功能，会依赖于lightNLP的深度学习自然语言处理框架、lightKG的知识图谱框架、lightUitls的其他工具类库，以及包含一些词典资源等等各种资源。 到了4月份，还是继续学习网易云课堂的自然语言处理课程，当然重心已经发生了转移，因为感觉自己在Github上寻找各nlp基础任务的简单实现并阅读源码的过程中学到挺多的，更甚于网易云课堂每周一更的录制课程，所以主要还是看代码，重新实现，以及学习微专业课程。 到了5月份，家里有人生病住院，我相对空闲所以去陪护，用去了一段时间。其他主要是继续上课，和看代码和敲代码，以及一些其他娱乐活动？忘了。 剩下的6~12月份，主要是复习备考，期间有认真复习的日子，也有荒废游戏的日子，甚至于连续几天熬夜打游戏，也有熬夜看小说，并没有多刻苦，也并没有很好地执行计划并坚决执行，甚至到了后期每天的真正学习时间反而更短了。 等考完后，发现心情一般，可能是发挥的一般，尤其是数学，考的极差，做了几套往年的真题，感觉还蛮简单的，于是就放松懈怠了，导致有的大题没写，有的只写了第一问，填空和选择也有不会的；也可能是因为自己平时就没有多努力，多刻苦，所以并没有放松解脱自由的感觉。 感谢学妹临考前每天的鼓励，真的挺感动的。 等考完后，本来想说玩游戏放松，可是不知道玩什么；本来想说看小说，可是备考期间也看了；那些本应该在考完后放松的事情让我在备考期间做了个遍，真想狠狠地扇自己！ 同时自己也有太多想做的事，比如学习Vue、学习爬虫、继续完善lightNLP、继续丰富lightKG、学习fastNLP源码、学习Neo4j、阅读知识图谱技术相关论文、阅读自然语言处理相关论文等等，奈何精力有限，只能一步步慢慢来。 不得不说，这一路走来，有奋斗、也有颓废。关于内心的变化，感觉自己变得比以前更现实了，甚至是相当现实了，比去年还是满脑子空想的我相比现实的太多了。心态也更加平和、冷静，抗干扰能力有所提升，内心不会因为外界的言语有多少波动。不会再把精力浪费在认为不值得的人、不值得的事上面。把名看得更淡，把利看得更多。世界观也更加健全完善，看到世界上发生的许多事情新闻都已经很平静了。尽管几乎去年一整年没有和社会打交道，同样积累了些许人生道理。 不管满意与否，今年的历程对今后的道路会有什么影响，路都已经走过了，也不能再回头了。 无论等一个多月之后公布的考研初试成绩如何，无论在今后找工作的道路上遇到多少艰难险阻，我都会继续追求我的自然语言处理和知识图谱的理想永不放弃。 脑子里又回想起当时那个面临继续工作还是辞职考研两个选择的自己，我可以回答他：这条路走的不后悔。毕竟在月亮和六便士面前，我曾经选择了月亮。 2020，继续冲鸭！","categories":[],"tags":[]},{"title":"最新说明！","slug":"最新说明！","date":"2019-12-24T13:46:22.000Z","updated":"2019-12-30T13:29:02.353Z","comments":true,"path":"articles/最新说明！/","link":"","permalink":"http://www.iamlightsmile.com/articles/最新说明！/","excerpt":"","text":"感觉单纯写博客而言，简书已经挺不错了，所以计划今后的文章会只发到简书中，个人博客站点就先这样吧~ 个人链接：iamlightsmile - 简书","categories":[],"tags":[]},{"title":"如何理解WordEmbedding？","slug":"如何理解WordEmbedding？","date":"2019-04-22T08:15:12.000Z","updated":"2019-04-22T08:33:36.562Z","comments":true,"path":"articles/如何理解WordEmbedding？/","link":"","permalink":"http://www.iamlightsmile.com/articles/如何理解WordEmbedding？/","excerpt":"之前自己也是懵懵懂懂的不太理解，经过一段时间的学习和思考，感觉自己有了新的认识，所以在这里分享一下，也算自己的总结了。 不谈数学原理，我认为理解Word Embedding可以从以下3个角度来理解。","text":"之前自己也是懵懵懂懂的不太理解，经过一段时间的学习和思考，感觉自己有了新的认识，所以在这里分享一下，也算自己的总结了。 不谈数学原理，我认为理解Word Embedding可以从以下3个角度来理解。 1.背景《深度学习》一书中有以下几句话，略有小改： 简单的机器学习算法的性能在很大程度上依赖于给定数据的表示。 使用机器学习来发掘表示本身的方法即表示学习。 从原始数据中提取高层次、抽象的特征是非常困难的。 深度学习通过其他较简单的表示来表达复杂表示。 我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。 分布式表示的思想是系统的每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能输入的表示。 联结主义的中心思想是当网络把大量简单的计算单元连接在一起时可以实现智能行为。 万能近似定理表明，神经网络可以近似从任何有限离散空间映射到另一个的任意函数。 其实我认为深度学习的应用核心就是上面几句话。 接下来我拿传统的确定性算法、机器学习和深度学习做一个简单的对比。 原来对于一些流程比较清晰简单的任务，我们的做法是编写确定性的算法来实现，而对于那些很复杂的，难以用简单的规则去说明的任务通常束手无策。 机器学习的作用则是直接从数据中去学习，总结规律，但通常我们要花费大量的精力在特征工程上面，同时对于许多任务来说，我们很难知道应该提取哪些特征。而深度学习则表示：特征不用你们整，只要你们给的数据足够好，我保证给你们一个更好的模型。 上面这句话的意思是，原来特征工程的累活我们不用做了，深度学习可以自己学到好的特征，同时万能近似定理也确保了深度神经网络可以保证模型学习效果的上限很高，当前前提是我们给的数据比较好。 以上交代了下背景，接下来言归正传。 自然语言要作为神经网络模型的输入之前，我们首先需要将其映射为计算机可以表示的形式。 独热编码最简单，将每一个字或者词都唯一编码成了01向量，除了维度灾难，我认为最大的缺点就是在这一映射过程中丢掉了许多词或者字的词义和语义特征，除了表示唯一以外，不包含任意其它信息。 这时我们应该思考：那什么样的表示才算好的表示？表示中应该保留哪些特征又如何保留这些特征呢？ 怀着对上面问题的疑问，我们来看一下他山之石。 2.卷积神经网络的工作原理我们都知道卷积神经网络中最主要的部分：卷积层、池化层、激活层的作用就是提取和匹配局部特征，将特征提取结果作为全连接层的输入从而得到最终的输出。 其中的每一个卷积核都可以被视为一个特征过滤器，卷积神经网络通过依次的扫描输入并进行卷积运算提取得到哪些位置可能包含哪些特征的信息，随后这些特征位置信息被进一步的提取从而得到更高级、更抽象的特征。 举个例子，如在识别图片中是否包含人时，卷积神经网络大致的作用原理如首先提取出某些位置是否是横线、竖线还是斜线以及颜色等特征，然后对这些特征进一步组合以得到哪些位置是否包含人脸、上肢、下肢等特征，通过提取到的这些特征，神经网络就可以做出决策得到图片中是否包含人的结果。 通过了解卷积神经网络的工作原理，我们可以知道卷积神经网络的最大作用就是可以自动学习并提取局部特征。对于计算机视觉中的图片而言，最微小的组成单元是一个个的像素点，然后局部组合就得到了线条和颜色块等信息特征；而对于自然语言处理而言，最小的组成单元则是一个个的字符，如英文中的‘a’、汉字中的‘我’等。 许多计算机视觉的预训练模型都是通用的，对于具体任务，我们只需要finetune（精调）或者只学习后面的层就可以了。这其中的原理是学到的模型提取特征的能力是可复用的，不依赖于某一具体任务。同样的，如果我们可以从语料中学到词或者字的词义和语义特征就好了，之后可以直接作为词或者字的表示用于模型训练和预测，这就是词的预训练。两者有异曲同工之妙。 3.Harris提出的分布式假说及Firth对此的阐述和论证Harris曾于1954年提出分布式假说：“上下文相似的词，其语义也相似”，后来又经过Firth对该假说进行阐述和论证，“词的语义由其上下文确定”。基于该思想，我们可以从该词在语料中的上下文学习得到该词的语义，同时也可以得到相同上下文下不同的词之间的联系。 结合上文提到的分布式表示的思想，我们可以想到：我们可以用某个词以及该词所指代的实体所具备的属性和特征来表示该词。 再举个不恰当例子： 如何表示“程序猿”和“单身狗”这两个词呢？假设我们有以下特征向量序列： 123456[\"有头发\",\"人傻\",\"钱多\",\"死得快\"] 我们可以设定: 1234567程序猿 = [0.3, 0.6, 0.6, 0.6]``` 同样的，我们可以设定：```python单身狗 = [0.6, 0.7, 0.3, 0.5] 基于此，我们便得到了“程序猿”和“单身狗”的语义相似度为： \\sqrt{( 1 - \\left| 0.3 - 0.6 \\right|)^2 +( 1 - \\left| 0.6 - 0.7 \\right|)^2 + ( 1 - \\left| 0.6 - 0.3 \\right|)^2 + ( 1 - \\left| 0.6 - 0.5 \\right|)^2} = 0.72从中我们可以得到“程序猿”和“单身狗”这两个词还是挺接近的。 注意：以上具体数值和计算公式是自己瞎掰的。 具体到属性特征有哪些以及具体每个词的分量数值应该是多少，这个神经网络是可以自己去学的，只不过学到的可以被视为潜在语义信息，并不是直观的”有头发”, “人傻”, “钱多”, “死得快”等特征，通常都是不可解释的。 总之，通过类似以上的方式，单词的语义信息就被比较有效的编码和表示起来了。这时我们再回顾一下之前的Word2Vec和最近非常火的BERT，则可以被视为以上思想的工程实践。只不过训练方式和优化目标愈加完善，使得词表示可以包含更多更好的语义表示罢了。 总结机器学习也好，深度学习也罢，其任务目标都是想要学习现实世界中某些量与某些量之间的映射变化关系。只不过有的关系是线性的，比较简单，而有的则极其复杂。对于这些复杂的问题，才是深度学习的用武之地。 许多任务都可以被看作是回归或分类问题，正如老子云：“天下皆知美之为美，斯恶已。”，美丑两端即定义一个维度。 对于神经网络的理解，也可以从还原论的哲学思想来入手。 参考 Word Embeddings: Encoding Lexical Semantics [透析]卷积神经网络CNN究竟是怎样一步一步工作的？ 通俗理解word2vec 自然语言处理—-文本表示","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://www.iamlightsmile.com/categories/深度学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://www.iamlightsmile.com/tags/NLP/"},{"name":"深度学习","slug":"深度学习","permalink":"http://www.iamlightsmile.com/tags/深度学习/"}]},{"title":"大学数学与深度学习","slug":"大学数学与深度学习","date":"2019-04-22T07:45:42.000Z","updated":"2019-04-22T08:31:34.082Z","comments":true,"path":"articles/大学数学与深度学习/","link":"","permalink":"http://www.iamlightsmile.com/articles/大学数学与深度学习/","excerpt":"我们都知道，世间的万事万物都是彼此联系和不断发展的。而我们要生存发展进步，则要不断地去尝试探索理解彼此之间到底是如何联系和发展的，要得到那些定性和定量的规律，哲学和数学以及其他学问在此基础上产生不断进化繁衍。 而事物与事物之间的关联法则与映射关系即对应于数学中函数这一概念，函数即是定义和研究自变量和因变量之间的映射关系的。","text":"我们都知道，世间的万事万物都是彼此联系和不断发展的。而我们要生存发展进步，则要不断地去尝试探索理解彼此之间到底是如何联系和发展的，要得到那些定性和定量的规律，哲学和数学以及其他学问在此基础上产生不断进化繁衍。 而事物与事物之间的关联法则与映射关系即对应于数学中函数这一概念，函数即是定义和研究自变量和因变量之间的映射关系的。 事物之间的联系有简单线性的，也有复杂非线性的，对于简单线性的，古人们通过初等数学等知识即可求解，而复杂非线性的则常常无能为力。 而微积分，作为复杂函数计算的有力工具，使得我们可以解决原本无法使用初等数学知识无法解决的问题，进而极大地推动了科学的发展和技术的进步。 然而尽管如此，许多现实中的复杂问题即使是微积分也无能为力，因为我们甚至无法得到其可以用数学公式表达的形式，同时数学作为研究数与形的学问也并非能解决所有问题。 虽然不能直捣黄龙，理解许多世界中的本质规律，但是我们可以通过抽象近似和归纳统计等方式另觅它径，以达曲径通幽之妙。 如通过“以直代曲”的核心思想，我们可以把许多非线性问题近似看作线性问题，从而使用线性代数来研究其规律。 同时我们也可以使用统计学和概率学知识不去细微探索直接探究其本质关联规律，而是在基于统计的基础上跳出局部站在比较宏观的角度建立起现象与现象之间的数学关系，从而得到表层的统计规律。 再如最近很火的神经网络、深度学习等，其本质则是用含有大量参数的神经网络模型不断地优化更新参数，来去尽可能地拟合变量之间的对应关系。","categories":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/categories/数学/"}],"tags":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/tags/数学/"}]},{"title":"数理统计学","slug":"数理统计学","date":"2019-04-22T07:28:33.000Z","updated":"2019-04-22T07:29:44.814Z","comments":true,"path":"articles/数理统计学/","link":"","permalink":"http://www.iamlightsmile.com/articles/数理统计学/","excerpt":"","text":"数理统计学只是从数量表现的层面上来分析问题，完全不触及问题的专业内涵。 数理统计方法是一个中立性的工具，这“中立”的含义是，它既不在任何问题上有何主张，也不维护任何利益或在任何学科中坚持任何学理。 由于数理统计方法只是从表面上的数量关系来分析问题，其结论不可混同于因果关系。","categories":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/categories/数学/"}],"tags":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/tags/数学/"}]},{"title":"向量与矩阵","slug":"向量与矩阵","date":"2019-04-22T07:10:37.000Z","updated":"2019-04-22T08:32:17.552Z","comments":true,"path":"articles/向量与矩阵/","link":"","permalink":"http://www.iamlightsmile.com/articles/向量与矩阵/","excerpt":"线性代数的基本研究单位是向量。 向量可以视为存储信息和结构的基本量。 矩阵既可以视为一组向量的集合，也可以视为一组向量的映射关系。","text":"线性代数的基本研究单位是向量。 向量可以视为存储信息和结构的基本量。 矩阵既可以视为一组向量的集合，也可以视为一组向量的映射关系。","categories":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/categories/数学/"}],"tags":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/tags/数学/"}]},{"title":"大学数学基础体系脉络","slug":"大学数学基础体系脉络","date":"2019-04-22T06:50:35.000Z","updated":"2019-04-22T08:32:26.067Z","comments":true,"path":"articles/大学数学基础体系脉络/","link":"","permalink":"http://www.iamlightsmile.com/articles/大学数学基础体系脉络/","excerpt":"一般来说，问题总是可以分成两类：连续问题和离散问题。相应的，大学数学中高等数学（也就是说微积分）是用来解决连续问题的，关心的函数的变量可以都非常小；而线性代数则是用来解决离散问题的，关心的是维度。","text":"一般来说，问题总是可以分成两类：连续问题和离散问题。相应的，大学数学中高等数学（也就是说微积分）是用来解决连续问题的，关心的函数的变量可以都非常小；而线性代数则是用来解决离散问题的，关心的是维度。 以下是来自万门大学童哲校长在线性代数两日特训班中所画的简单说明图：","categories":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/categories/数学/"}],"tags":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/tags/数学/"}]},{"title":"整体论与还原论","slug":"整体论与还原论","date":"2019-04-22T06:26:43.000Z","updated":"2019-04-22T06:41:30.764Z","comments":true,"path":"articles/整体论与还原论/","link":"","permalink":"http://www.iamlightsmile.com/articles/整体论与还原论/","excerpt":"还原论所谓还原，是一种把复杂的系统（或者现象、过程）层层分解为其组成部分的过程。还原论认为，复杂系统可以通过它各个组成部分的行为及其相互作用来加以解释。还原论方法是迄今为止自然科学研究的最基本的方法，人们习惯于以“静止的、孤立的”观点考察组成系统诸要素的行为和性质，然后将这些性质“组装”起来形成对整个系统的描述。例如，为了考察生命，我们首先考察神经系统、消化系统、免疫系统等各个部分的功能和作用，在考察这些系统的时候我们又要了解组成它们的各个器官，要了解器官又必须考察组织，直到最后是对细胞、蛋白质、遗传物质、分子、原子等的考察。现代科学的高度发达表明，还原论是比较合理的研究方法，寻找并研究物质的最基本构件的做法当然是有价值的。","text":"还原论所谓还原，是一种把复杂的系统（或者现象、过程）层层分解为其组成部分的过程。还原论认为，复杂系统可以通过它各个组成部分的行为及其相互作用来加以解释。还原论方法是迄今为止自然科学研究的最基本的方法，人们习惯于以“静止的、孤立的”观点考察组成系统诸要素的行为和性质，然后将这些性质“组装”起来形成对整个系统的描述。例如，为了考察生命，我们首先考察神经系统、消化系统、免疫系统等各个部分的功能和作用，在考察这些系统的时候我们又要了解组成它们的各个器官，要了解器官又必须考察组织，直到最后是对细胞、蛋白质、遗传物质、分子、原子等的考察。现代科学的高度发达表明，还原论是比较合理的研究方法，寻找并研究物质的最基本构件的做法当然是有价值的。 整体论与还原论相反的是整体论，这种哲学认为，将系统打碎成为它的组成部分的做法是受限制的，对于高度复杂的系统，这种做法就行不通，因此我们应该以整体的系统论观点来考察事物。比如考察一台复杂的机器，还原论者可能会立即拿起螺丝刀和扳手将机器拆散成几千、几万个零部件，并分别进行考察，这显然耗时费力，效果还不一定很理想。整体论者不这么干，他们采取比较简单一些的办法，不拆散机器，而是试图启动运行这台机器，输入一些指令性的操作，观察机器的反应，从而建立起输入──输出之间的联系，这样就能了解整台机器的功能。整体论基本上是功能主义者，他们试图了解的主要是系统的整体功能，但对系统如何实现这些功能并不过分操心。这样做可以将问题简化，但当然也有可能会丢失一些比较重要的信息。 还原论与整体论的关系还原论与整体论之争由来已久，并且激发了脑研究和人工智能领域内的大争论。还原论方法将大脑还原为神经元，然后设法将神经元组装成大脑。人工智能的一个学派认为，通过创造元数字电路，我们能够建造越来越复杂的电路，直到我们创造人工智能。这个学派沿着现代电子计算机这条思路，对“智能”的模仿取得了初步的成功，但深入下去就比较令人失望，因为它甚至连模仿大脑的最简单功能，比如模糊记忆，都无法做到。面对人工智能研究的窘境，一些科学家从研究方法上进行反思，认为还原论方法在人工智能的研究方面没有前途，应设法采取一种更加整体的方法对待大脑，不必纠缠于人脑运作中的一些细小环节，应该建立起把大脑视为整体的模型，将大脑的一些基本功能从一开始就建造在这个模型系统里。神经网络理论基本上就是基于这样一种方法而建立起来的理论模型，这是一种功能主义的整体研究方式。这种方式现在看来也是困难重重，不过它才刚刚起步，其未来的前途如何尚未可知。 我的观点是，还原论与整体论作为两种不同的研究方法，它们本身无所谓优劣之分，我们具体选择哪种方法，这完全视乎具体情形，并取决于我们个人的喜好。在某种情形下我们采取还原的方法，在另外的情形下我们可能会采取整体论的方法，这都是可以的。但是，在大多数情况下，人们倾向于采用还原论方法，这比较可靠，也比较能够满足我们寻根究底的好奇心，所以只要有可能，人们总是乐于使用它。 事实上整体论总是只能进行一些初步的研究，一旦深入下去就必须使用还原论的方法。因此，对待自然界，我们总是首先了解其大致的、整体的规律，这是整体论的方法，接着一定要再对它层层进行还原分解，以此考察和研究它的深层次本质规律。例如为了研究人体的生物性状，我们首先了解各个系统，如消化系统、神经系统、免疫系统等的功能，这时候我们是将各个系统当作一个整体来予以研究的；而接着，我们要继续研究组成系统的各器官的功能，再接着是组织、细胞、分子、原子等层面，这便是一个逐层还原的过程。随着层层还原过程的深入，我们对人体的机制就能够得到越来越多的了解。 是的，对那些过于复杂的系统，比如人的大脑，还原论方法到达一定地步之后就会显得异常繁难，人类的心智看来根本就无法做到将其彻底还原，这时候我们不得不退而求其次，对系统的某些细节忽略不计，从而引进一种比较整体的功能主义研究方式。类似地，对于像“视窗”这样复杂的软件系统，整个系统的逻辑是非常复杂的，如果有人想要模拟而不是剽窃这个系统，最好的办法是：在了解它的功能后再另行编制一个具有几乎相同功能的系统。如果妄想将一台装有“视窗”系统的电脑拆散，从物理的角度了解整个系统的逻辑结构，然后再一一复制出来，这肯定极其艰难甚至劳而无功。所以，对人的大脑采取功能主义的整体论方式进行模拟将比还原论方法也许更为行之有效。 但是，即使对复杂系统的研究，人类的心智有时候会变得一筹莫展，这也并不意味着还原论就没有价值。因为我们需要知道：系统的表现为什么会是这样？如果我们将一部哪怕最简单的计算器拿到古代，古代的科学家也可能被迫采取整体论的方式对它进行研究，他们或许能了解其主要功能，知道它可以用于数字计算，但他们必然不清楚：它为什么会是这样的呢？这时候，他们将会多么的遗憾。对人体的研究，虽然我们很难用原子和分子的行为来计算和推导出人的行为，但我们至少希望通过原子和分子的行为来解释和理解人的行为。很显然，我们需要能够直接描述复杂系统的整体定律，所以我们有化学定律、有混沌定律、有经济学定律和社会学定律，但这些定律不会是最基本的定律，我们会问为什么？为什么这些定律是这个样子？这时候，这些定律需要用个体行为来进行解释，需要用 “部分”的行为来进行解释。 还原论的方法肯定是最基本的科学方法。但由于混沌学说的巨大成功，一些人对整体论产生了过分的自信，在今天的很大部分科学哲学家眼里，还原论变成了坏东西，他们为整体论欢呼雀跃，却想法设法要与还原论划清界限。他们走得太远了，他们将整体论的作用过于夸大了，我们有些哲学家甚至还将整体论当作哲学本体论概念来进行介绍，煞有介事地探讨起“世界是简单还是复杂的”这样一些哲学命题来。他们的道理是，整体不等于部分之和，因此自然界是不可彻底还原的，因此整体论才是最优等的哲学。 有这样一个关于还原论的笑话：老师带学生走进实验室，指着一排玻璃仪器，说那是一个人，因为玻璃瓶里装着人的所有组成物质，包括水、碳、脂肪、蛋白质……。这个笑话的实质是说，还原论者只会将“部分”简单地累加起来形成整体，却愚蠢地并不考虑“部分”之间的相互作用。 我以为，认为还原论忽视了部分之间的相互作用，这样的指责毫无根据。还原论并不忽视“部分”之间的相互作用，相反，还原的目的正是为了更好地考察这种相互作用。通过还原，“部分”之间的相互作用变成了每个“部分”的边界条件，变成了每个“部分”的输入和输出，这使得我们能更精确地考察这种作用，并建立起将这些相互作用联系起来的方程。整体确实不等于部分之和，但整体必定等于部分及其相互作用之和。 有些人认为整体论的定律才是最基本的定律，而个体的行为要通过整体的行为来解释，甚至对人类社会也必须采取整体论的方法，认为如果只考察个体，则可能忽略掉人类社会这个群体的一些性质。这种说法是相当奇怪的，人类社会的所有性质归根结底都可以从个体性质及其相互作用而得到解释，虽然我们为了方便起见，可能采取整体论的研究方式，但肯定只有这种整体论的方式才有可能丢失一些重要的信息，而还原论的方式不会。 我们经常听到这样的训诫：使用还原论要谨慎从事。使用整体论更需谨慎从事。如果只是弄出一个整体论的定律，而个体层次发生的事情都以这个整体的行为来进行解释，这样的理论体系是难以令人信服的。 不过，还原论方法虽为我们所偏爱，但还原的过程必然只能进行到一定的层次，这不仅仅因为我们的心智不够，还有更重要的原因：自然界是不可以彻底还原的。 我们知道，世界是普遍联系的，世界上每个事物都和其他每个事物联系着。但事物之间的联系是怎样实现的呢？传统观点认为：不同的东西通过大量的中介过程统一起来，这就是说，事物之间的联系是层层递进的，是定域性的，任何物体只和其邻近产生即时联系，事物的超距作用是不可能的。世界的可还原性就建立在这样的宇宙绘景中，在这样的宇宙中，我们原则上可以将任何系统从宇宙中孤立出来进行考察，这个系统的边界条件是稳定的、可知的，我们可以通过边界条件的变化掌握和了解这个系统的性质和运行规律。将系统孤立的过程就是一个还原的过程，我们可以将大系统分割成一个个的小系统，小系统再细分为更小的系统，这样层层细分下去，从而我们所处的世界至少在理论上是可以彻底还原的。 然而，量子理论表明，世界的联系并不是定域性的。宇宙中的一切物质都存在着即时的普遍联系。在量子理论中，一切事物的运动都应该用波函数来描述，而波函数是遍布整个宇宙的。我现在坐在椅子上，我的身体伴随着有一个波函数，可以肯定这个波函数的值主要集中在我身体占有的空间内，接近100%，但不可能等于100%，在宇宙的其他地方，比如在火星上也会分布有我的波函数，虽然它们的值很小，非常接近于零，但不可能等于零。如果我的身体有任何的运动或变化，比如我动一下手指头，那么伴随我身体的波函数必然也要发生变化，而这个变化产生的影响将会遍布整个宇宙！火星上的一块石头如果“足够”地灵敏，它将会“感受”到这种影响，这种影响虽然非常非常之小，非常非常接近于零，但毕竟不等于零。在这样的宇宙绘景中，宇宙是一个不可分割的整体，如果我们一定要将某个时空孤立起来进行考察，那么由于宇宙中任何的变化都对它有影响，从而它的边界条件将会是整个宇宙！这个边界条件显然是不可知的。而且，外界对系统的作用也并不局限在边界，而是“深入”到系统内的每一个“部分”，这样系统内部的作用“场”也是不可知的。因此，这样的分割还原就变得没有任何实质性的意义。 所以，当我们用还原论的方法对事物进行考察的时候，我们实际上忽略了事物之间联系的量子效应。这样的“忽略”在通常情况下不会有什么问题，毕竟我的波函数在离开我身体哪怕只有一微米的地方就将衰减到几乎为零，它太小了，完全可以忽略不计。但是在那些必须考虑量子效应的地方，比如亚原子领域、比如宇宙“创生”的过程，这样的忽略就不能允许，这时候我们不能再采用还原论的研究方法，我们必须将整个宇宙都作为一个整体来考察。 很显然，只有在局域性不能忽略的地方，还原论才原则上不可行；在不必考虑局域性的地方，还原论原则上可行！ 那么，在还原论原则上不可行的亚原子领域，还原论就没有价值了吗？我认为，还原论仍然有重大的价值。因为，即便是存在非局域性，导致还原论原则上不可行的领域，我们还是需要了解个体的性质，要通过个体的行为来理解（而不是推导）整体的行为。 参考资源 整体论 还原论 刘劲杨：论整体论与还原论之争","categories":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/categories/哲学/"}],"tags":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/tags/哲学/"},{"name":"抽象","slug":"抽象","permalink":"http://www.iamlightsmile.com/tags/抽象/"}]},{"title":"线性代数","slug":"线性代数","date":"2019-04-22T06:26:05.000Z","updated":"2019-04-22T08:32:44.298Z","comments":true,"path":"articles/线性代数/","link":"","permalink":"http://www.iamlightsmile.com/articles/线性代数/","excerpt":"基本介绍线性代数是研究线性空间及其线性映射的，或者说各种线性结构和态射。 价值由于线性结构非常常见，所以线性代数的价值也相当大。","text":"基本介绍线性代数是研究线性空间及其线性映射的，或者说各种线性结构和态射。 价值由于线性结构非常常见，所以线性代数的价值也相当大。 意义和地位线性代数在数学、物理学和技术学科中有各种重要应用，因而它在各种代数分支中占居首要地位。在计算机广泛应用的今天，计算机图形学、计算机辅助设计、密码学、虚拟现实等技术无不以线性代数为其理论和算法基础的一部分。线性代数所体现的几何观念与代数方法之间的联系，从具体概念抽象出来的公理化方法以及严谨的逻辑推证、巧妙的归纳综合等，对于强化人们的数学训练，增益科学智能是非常有用的。随着科学的发展，我们不仅要研究单个变量之间的关系，还要进一步研究多个变量之间的关系，各种实际问题在大多数情况下可以线性化，而由于计算机的发展，线性化了的问题又可以被计算出来，线性代数正是解决这些问题的有力工具。线性代数的计算方法也是计算数学里一个很重要的内容。线性代数的含义随数学的发展而不断扩大。线性代数的理论和方法已经渗透到数学的许多分支，同时也是理论物理和理论化学所不可缺少的代数基础知识。 “以直代曲”是人们处理很多数学问题时一个很自然的思想。很多实际问题的处理，最后往往归结为线性问题，它比较容易处理。因此，线性代数在工程技术和国民经济的许多领域都有着广泛的应用，是一门基本的和重要的学科。 如果进入科研领域，你就会发现，只要不是线性的东西，我们基本都不会！线性是人类少数可以研究得非常透彻的数学基础性框架。学好线性代数，你就掌握了绝大多数可解问题的钥匙。有了这把钥匙，再加上相应的知识补充，你就可以求解相应的问题。可以说，不学线性代数，你就漏过了95%的人类智慧！非线性的问题极为困难，我们并没有足够多的通用的性质和定理用于求解具体问题。如果能够把非线性的问题化为线性的，这是我们一定要走的方向！ 事实上，微积分“以直代曲”的思想就是将整体非线性化为局部线性的一个经典的例子，尽管高等数学在定义微分时并没有用到一点线性代数的内容。许多非线性问题的处理――譬如流形、微分几何等，最后往往转化为线性问题。包括科学研究中，非线性模型通常也可以被近似为线性模型。随着研究对象的复杂化与抽象化，对非线性问题线性化，以及对线性问题的求解，就难免涉及到线性代数的术语和方法了。从这个意义上，线性代数可以被认为是许多近、现代数学分支的共同基础。 参考 线性代数 把非线性转化成线性","categories":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/categories/数学/"}],"tags":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/tags/数学/"}]},{"title":"概率论与数理统计","slug":"概率论与数理统计","date":"2019-04-22T06:25:41.000Z","updated":"2019-04-22T08:32:38.823Z","comments":true,"path":"articles/概率论与数理统计/","link":"","permalink":"http://www.iamlightsmile.com/articles/概率论与数理统计/","excerpt":"概率论与数理统计的核心是利用微积分工具研究随机现象背后的客观规律性。","text":"概率论与数理统计的核心是利用微积分工具研究随机现象背后的客观规律性。","categories":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/categories/数学/"}],"tags":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/tags/数学/"}]},{"title":"微积分","slug":"微积分","date":"2019-04-22T04:04:24.000Z","updated":"2019-04-22T08:31:55.188Z","comments":true,"path":"articles/微积分/","link":"","permalink":"http://www.iamlightsmile.com/articles/微积分/","excerpt":"1.极限设函数$f(x)$在点$x_0$的某一去心邻域内有定义。若存在常数$A$，对于任意给定的$\\epsilon&gt;0$（不论它多么小），总存在正数$\\delta$，使得当$0&lt;|x-x_0|&lt;\\delta$时，对应的函数值$f(x)$都满足不等式$|f(x)-A|&lt;\\epsilon$，则$A$就叫函数$f(x)$当$x\\to x_0$时的极限，记为 \\lim_{x\\to x_0}f(x)=A或 f(x)\\to A,(x\\to x_0)写成$\\epsilon-\\delta$语言是：$\\lim_{x\\to x_0}f(x)=A\\Leftrightarrow \\forall \\epsilon&gt;0,\\exists \\delta&gt;0$，当$0&lt;|x-x_0|&lt;\\delta$时，$|f(x)-A|&lt;\\epsilon$。","text":"1.极限设函数$f(x)$在点$x_0$的某一去心邻域内有定义。若存在常数$A$，对于任意给定的$\\epsilon&gt;0$（不论它多么小），总存在正数$\\delta$，使得当$0&lt;|x-x_0|&lt;\\delta$时，对应的函数值$f(x)$都满足不等式$|f(x)-A|&lt;\\epsilon$，则$A$就叫函数$f(x)$当$x\\to x_0$时的极限，记为 \\lim_{x\\to x_0}f(x)=A或 f(x)\\to A,(x\\to x_0)写成$\\epsilon-\\delta$语言是：$\\lim_{x\\to x_0}f(x)=A\\Leftrightarrow \\forall \\epsilon&gt;0,\\exists \\delta&gt;0$，当$0&lt;|x-x_0|&lt;\\delta$时，$|f(x)-A|&lt;\\epsilon$。 2.导数参见《高数18讲》p51页 1.导数的概念。 3.增量增量亦称改变量，指的是在一段时间内，自变量取不同的值所对应的函数值之差。 4.微分参见《高数18讲》p53页 5.微分的概念。 自己的浅解微分是相对于某点而言，变量的线形近似增量。 微分的意义参见《高数18讲》p53页中 6.可微的判别【注】(1)。 参考 微分符号 dx、dy 表示什么含义？ - 马同学的回答 - 知乎 积分中dx的意义是什么？ 5.积分参见《高数18讲》p110页 1.原函数与不定积分。 自己浅解微分之和即积分 6.微积分基本定理自己浅解","categories":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/categories/数学/"}],"tags":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/tags/数学/"}]},{"title":"Hexo主题继续优化","slug":"Hexo主题继续优化","date":"2019-04-03T16:13:11.000Z","updated":"2019-04-04T03:16:24.750Z","comments":true,"path":"articles/Hexo主题继续优化/","link":"","permalink":"http://www.iamlightsmile.com/articles/Hexo主题继续优化/","excerpt":"本次优化的方向主要围绕以下几点来展开： 自定义背景文字颜色 添加雪花特效 添加爆炸特效 添加输入特效 我当前使用的主题还是比较流行、作者一直在维护的主题Material-X.","text":"本次优化的方向主要围绕以下几点来展开： 自定义背景文字颜色 添加雪花特效 添加爆炸特效 添加输入特效 我当前使用的主题还是比较流行、作者一直在维护的主题Material-X. 自定义背景文字颜色参见Material-X doc中配色描述，将背景和文字颜色设置为暗色主题，同时将主题设置为深绿色，效果如图所示： 注意：如果是单纯修改以上内容，其实并不能生效，还要关闭主题的_config.yml文件中的cdn服务，如下图所示： 添加雪花特效从站长之家找到的JS制作雪花飘落背景动画特效, 下载后将两个js放入source目录下的js目录中，并在layout目录下的layout.ejs中添加相关script路径，如下图所示： 添加爆炸特效发现生如夏花的博客的点击效果比较炫酷，查看其发布的文章并没有介绍具体实现，于是查看其网页源码，得到了对应的fireworks.js文件，同雪花特效一样添加到指定路径并配置路径，如下图所示： 添加输入特效发现搅拌糖对主题进行了炫酷的DIY，如背景切换，滑动栏自定义等，同上查看了其网页源码，得到cooltext.js文件，同雪花特效一样添加到指定路径并配置路径，如下图所示： 经过以上步骤，hexo主题又得到了进一步的自定义优化。 ps：花了许多时间排查看板娘插件是如何出现的，自己只是安装的依赖库，并没有配置，然而却被渲染到最终的生成页面中，真是百思不得其解，由于个人不太喜欢二次元，同时手机端看起来体验不好（占了很大的空间），最终选择将依赖库移除从而去除掉了看板娘效果。","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://www.iamlightsmile.com/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://www.iamlightsmile.com/tags/Hexo/"}]},{"title":"Manjaro下数据备份","slug":"Manjaro下数据备份","date":"2019-04-03T06:25:51.000Z","updated":"2019-04-03T06:27:52.874Z","comments":true,"path":"articles/Manjaro下数据备份/","link":"","permalink":"http://www.iamlightsmile.com/articles/Manjaro下数据备份/","excerpt":"Linux一时爽，崩后火葬场；Manjaro一时爽，一直用一直爽。 以下简单介绍几种manjaro下进行数据备份的方案选择以及一些使用流程。其中自己编写的使用流程如下 使用百度云盘+ baidupcs-go 使用Google Drive MEGA 使用sftp直连数据备份方案 物理介质 自己搭建私有云 选择网盘服务商 其他方案","text":"Linux一时爽，崩后火葬场；Manjaro一时爽，一直用一直爽。 以下简单介绍几种manjaro下进行数据备份的方案选择以及一些使用流程。其中自己编写的使用流程如下 使用百度云盘+ baidupcs-go 使用Google Drive MEGA 使用sftp直连数据备份方案 物理介质 自己搭建私有云 选择网盘服务商 其他方案 物理介质1. 移动硬盘优势：离线存储，简洁快速，数据安全劣势：丢失无法找回，不能自动同步 2. 移动U盘优势：离线存储，简洁快速，数据安全劣势：丢失无法找回，不能自动同步 自己搭建私有云1. Owncloud优势：数据安全劣势：搭建成本高搭建教程： 如何搭建私密云存储之ownCloud 2. NextCloud优势：数据安全劣势：搭建成本高搭建教程: Ubuntu16.04 搭建NextCloud私有云3. Seafile优势：数据安全劣势：搭建成本高搭建教程： 部署Seafile搭建自己的网盘 选择网盘服务商1. 百度网盘优势：容量大劣势：限速，建议使用 BaiduPCS-Go工具进行管理 2. Google Drive优势：跨平台劣势：需要翻墙，容量小 3. MEGA优势：Linux下使用体验好劣势： 坚果云优势：劣势：容量小 其他方案1. 有道云笔记（VIP版）和印象笔记相比，虽然网页剪裁的功能弱了一些，但是在界面上和markdown支持上都是要优于印象笔记的优势：劣势： 2. 使用sftp连接远程服务器优势：劣势： 使用流程百度网盘+BaiduPCS-Go安装baidupcs-go1yay -S baidupcs-go-bin 启动1baidupcs 如图所示： 登录1BaiduPCS-Go &gt; login 然后输入自己的账号密码，进行验证之后便登录成功了，如图： 使用输入-h可查看相关指令，下载起来速度很快。1BaiduPCS-Go &gt; -h 可参考 BaiduPCS-Go | 百度网盘命令行工具（基于 Go） Google Drive安装kio-gdrive1yay -S kio-gdrive 使用在安装成功后，此时再打开Dolphin（即文件管理器），会发现远程网络中多了一个文件夹，如图：之后我们双击它，得到如图：上图左侧的账号是我之前添加的，此时也可点击 下方+添加Google账号然后选中要同步的账号，选中，如下图所示，然后点击确定。之后右下角提示正在同步文件夹，稍等片刻便可同步成功了，如图： 可参考 KIO GDriveMEGA注册账号登录官网注册账号，填写个人信息，然后打开邮箱进行验证，随后下载客户端应用（对于manjaro来说，直接通过pacman下载就可以，就是这么方便，当然也可以先下载安装包，然后通过pacman -U来安装） 安装1yay -S megasync 设置个人信息首先[Alt] + [Space]打开plasma搜索，输入megasync打开MEGA客户端，然后输入邮箱密码，接着设置同步目录，其他设置自己看情况。如下图： 可参考 网盘可以良心到什么程度? 试试MEGA吧! 使用sftp直连使用方式很简单，如下图：双击连接网络文件夹，选择安全Shell，点击Next填写网络文件夹信息，然后点击保存并连接如果填写无误的话，就连接成功了，如图：","categories":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/categories/Manjaro/"}],"tags":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/tags/Manjaro/"}]},{"title":"Manjaro下vscode中zsh乱码","slug":"Manjaro下vscode中zsh乱码","date":"2019-04-03T06:24:25.000Z","updated":"2019-04-03T06:28:36.842Z","comments":true,"path":"articles/Manjaro下vscode中zsh乱码/","link":"","permalink":"http://www.iamlightsmile.com/articles/Manjaro下vscode中zsh乱码/","excerpt":"解决方案","text":"解决方案 下载Menlo for Powerline字体 1git clone https://github.com/abertsch/Menlo-for-Powerline.git 将字体放到ttf文件夹中 123cd Menlo-for-Powerlinesudo cp *.ttf* /usr/share/fonts/TTF/sudo fc-cache -f -v # 刷新字体 设置vscode中终端的字体为Menlo for Powerline 参考 在Ubuntu 18.04系统下vscode中zsh乱码的解决方法","categories":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/categories/Manjaro/"}],"tags":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/tags/Manjaro/"}]},{"title":"torchtext安装","slug":"torchtext安装","date":"2019-04-03T06:20:52.000Z","updated":"2019-04-03T06:21:33.097Z","comments":true,"path":"articles/torchtext安装/","link":"","permalink":"http://www.iamlightsmile.com/articles/torchtext安装/","excerpt":"","text":"使用如下命令安装torchtext1pip install https://github.com/pytorch/text/archive/master.zip","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://www.iamlightsmile.com/categories/深度学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://www.iamlightsmile.com/tags/NLP/"},{"name":"深度学习","slug":"深度学习","permalink":"http://www.iamlightsmile.com/tags/深度学习/"}]},{"title":"Manjaro下截屏及设置快捷键","slug":"Manjaro下截屏及设置快捷键","date":"2019-04-03T06:19:40.000Z","updated":"2019-04-03T06:28:09.740Z","comments":true,"path":"articles/Manjaro下截屏及设置快捷键/","link":"","permalink":"http://www.iamlightsmile.com/articles/Manjaro下截屏及设置快捷键/","excerpt":"我认为当前Manjaro下最好的截屏工具要属deepin-screenshot了。","text":"我认为当前Manjaro下最好的截屏工具要属deepin-screenshot了。 下载安装 1yay -S deepin-screenshot 配置系统快捷键在【系统设置】-【工作区】-【自定义快捷键】中，点击【编辑】-【新建】-【全局快捷键】-【命令/URL：】然后填写动作名称，如我这里是Deepin截图,然后分别填写注释（非必须）、触发器和动作，如下图： 然后就可以尽情使用啦，不得不感慨deepin真的挺强挺好的！","categories":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/categories/Manjaro/"}],"tags":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/tags/Manjaro/"}]},{"title":"Manjaro下安装使用kenlm","slug":"Manjaro下安装使用kenlm","date":"2019-04-03T06:18:33.000Z","updated":"2019-04-03T06:28:22.471Z","comments":true,"path":"articles/Manjaro下安装使用kenlm/","link":"","permalink":"http://www.iamlightsmile.com/articles/Manjaro下安装使用kenlm/","excerpt":"kenlm是一个linux下快速轻量的语言模型训练工具。","text":"kenlm是一个linux下快速轻量的语言模型训练工具。 下载1git clone https://github.com/kpu/kenlm.git 或者1wget https://kheafield.com/code/kenlm.tar.gz | tar xz 安装依赖1yay -S boost eigen 编译安装以上两种来源区别我也不是很清楚，没有细究。12345mkdir kenlm/buildcd kenlm/buildcmake ..make -j8make install 安装Python库以上下载的文件中有python安装脚本setup.py文件，执行1python setup.py install 或者，1pip install https://github.com/kpu/kenlm/archive/master.zip 或者从pypi源安装，1pip install kenlm 训练模型准备训练数据首先我们需要有一个分好词的语料文件，如： 训练然后使用以下命令训练：1lmplz -o 3 &lt;pku_training.utf8&gt; lm_ng3.arpa 其中-o参数指明n-gram语法为3，&lt;&gt;中的为训练语料路径，后面跟模型保存路径 模型压缩对模型压缩可以提高加载速度，不压缩也可以1build_binary -s lm_ng3.arpa lm_ng3.bin 使用Python接口123456789import kenlmimport jiebamodel = kenlm.LanguageModel(&apos;./lm_ng3.bin&apos;)sent_1 = &apos;哈哈，我是李磊，你好呀&apos;sent_2 = &apos;安赛飞啊，诶爱尔兰&apos;def process(sent): return &apos; &apos;.join(jieba.cut(sent))print(model.score(process(sent_1))print(model.score(process(sent_2)) 结果输出：12-40.40456008911133-47.40667724609375 其中，分数越小，句子分数越低，越”不像“一个句子。 参考 kenlm kenlm语言模型相关，c++、python相关接口 网易云课堂-微专业-AI工程师（自然语言处理方向）","categories":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/categories/Manjaro/"}],"tags":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/tags/Manjaro/"}]},{"title":"Manjaro系统报错集锦","slug":"Manjaro系统报错集锦","date":"2019-04-03T06:17:26.000Z","updated":"2019-10-23T10:47:13.850Z","comments":true,"path":"articles/Manjaro系统报错集锦/","link":"","permalink":"http://www.iamlightsmile.com/articles/Manjaro系统报错集锦/","excerpt":"以下是自己在使用manjaro系统中遇到的错误和相应的解决方案 unable to initialize decompress status for section .debug_info以及file not recognized: file format not recognized 进入grub rescue模式","text":"以下是自己在使用manjaro系统中遇到的错误和相应的解决方案 unable to initialize decompress status for section .debug_info以及file not recognized: file format not recognized 进入grub rescue模式 1. unable to initialize decompress status for section .debug_info以及file not recognized: file format not recognized错误出处在安装kenlm库以及allennlp时报的安装错误; 更新：在安装scrapy框架及其依赖库twisted时也报了同样的错误，解决方法一样。 报错截图 解决方案从arch包源下载旧版的bintools,然后降级安装,如图: 参考 [SOLVED]unable to initialize decompress status for section .debug_info 2. 进入grub rescue模式错误出处在系统启动过程中由于目录分区映射及挂载失败导致 解决方案注意:我这里boot目录和根目录分别挂载在不同分区,所以后续路径可能和其他参考有所不同，在使用时视实际情况而定 确定boot目录和根目录位置12345# ls查看一下设备状态，可使用tab键自动补全，并有各分区提示信息grub rescue&gt; ls hd0, (hd0, gpt1), (hd0, gpt2), (hd0, gpt3)grub rescue&gt; ls (hd0,gpt3)/./ ../ lost+found/ 通过查看找到boot目录和root目录所在分区，比如分别为gpt1和gpt2 设置grub的启动分区和路径 1234567grub rescue&gt; set root=(hd0,gpt1) #设置grub启动分区grub rescue&gt; set prefix=(hd0,gpt1)/grub #设置grub启动路径# 查看一下设置情况,直接输入set可以查看root和prefix的配置grub rescue&gt; setprefix=(hd0,gpt1)/grubroot=hd0,gpt1 加载基本模块 1grub rescue&gt; insmod normal #加载基本模块 进入正常模式 1grub rescue&gt; normal #进入普通模式，出现菜单，如果加载grub.cfg（错误的）可能出现问题，按shift可以出现菜单，之后按c键进入控制台 进入正常模式后就会出现grub&gt;这样的提示符，在这里支持的命令就非常多了。 引导系统 1234grub&gt; set root=(hd0,gpt1) #设置正常启动分区grub&gt; linux /vmlinuz-4.19-x86_64 ro text root=/dev/sda2 #加载内核，进入控制台模式grub&gt; initrd /intel-ucode.img /initramfs-4.19-x86_64.img #加载initrd.imggrub&gt; boot #引导 更新grub 1234# 进入系统后，先更新grubupdate-grub #更新# 修改grub.cfg后，再执行installgrub-install /dev/sda #安装 注意:注意上面的是sda，硬盘号，而不是具体某个分区号，如sda1 参考 grub rescue救援模式的处理 Ubuntu开机出现grub rescue模式修复方法 Ubuntu启动问题以及Grub Rescue修复方法","categories":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/categories/Manjaro/"}],"tags":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/tags/Manjaro/"}]},{"title":"Tensorflow编译血泪史","slug":"Tensorflow编译血泪史","date":"2019-04-03T06:14:59.000Z","updated":"2019-04-03T06:16:39.422Z","comments":true,"path":"articles/Tensorflow编译血泪史/","link":"","permalink":"http://www.iamlightsmile.com/articles/Tensorflow编译血泪史/","excerpt":"为了安装tensorflow，导致我Linux系统重装，Windows系统差点也没了，哎。。。","text":"为了安装tensorflow，导致我Linux系统重装，Windows系统差点也没了，哎。。。 本人笔记本电脑有一个256 SSD和1TB机械硬盘，固态装C盘，机械硬盘装D、E和F，各330G，系统为Win10，显卡为NVIDIA1060。后来打算装manjaro双系统，进行编程和机器学习、深度学习的开发工作，于是F盘分了一半约160G给manjaro系统，其中根目录和家目录等单独划分分区挂载，根目录分区大小为30G，此是前话。 在安装tensorflow之前自己先装了pytorch，因为觉得这个框架代码更优雅，代码风格自己更喜欢。后来在安装tensorflow时发现还挺麻烦，需要独立cuda和cudnn库，不同版本的tensorflow依赖于不同版本的cuda和cudnn，而cuda和cudnn依赖于gcc，如下图所示。 而manjaro系统属于arch系，各软件包滚动更新速度很快，基本总是保持最新，比如系统默认python环境竟然时3.7.2，我的个乖乖，自己的系统里安装的gcc是此时最新的8.2.1，与cuda所需的gcc6冲突，如果要装gcc6还要卸载gcc8，而其他软件包会依赖于gcc8，况且安装旧版本包这种行为一点都不arch，于是使用tensorflow官方预编译好的whl文件安装就不太现实了，只能自己在本机上编译构建了，幸好网上搜到了一篇最近的、讲得很好很详细、和自己情况正相符的一篇帖子：编译 Tensorflow 1.10 + CUDA9.2 + MKL，在这里向作者由衷的表示感谢！ 于是便红红火火按照教程开始来装了，于是问题旧开始出现了。首先问题是当下载了tensorflow源码之后发现自己不能切换安装版本，如果选择默认master以外的分支，则报无法引用到/tensorflow/tools/bazel.rc文件的错误，于是只能在master分支装吧，不管了。如图： 同时发现自己访问github下载文件的速度太慢了，导致bazel程序运行失败，后来找到了相关博客如git clone速度太慢的解决办法进行配置，发现还是未解决，自己在命令前加proxychains代理也不行，因为是程序内部调用系统网络去下载文件，代理命令无效，经过多次尝试后，自己打算通过浏览器下载一个文件试试，如https://github.com/bazelbuild/rules_closure/archive/9889e2348259a5aad7e805547c1a0cf311cfcd91.tar.gz,发现下载的挺快的，是因为代理，而自己直接调用wget命令发现好慢，同时还发现了该文件的最终下载域名为：codeload.github.com，后来自己参考那个教程在https://www.ipaddress.com/里搜索得到了对应的ip并将其和ip加到hosts文件中，如图所示：速度一下就快了不少，虽说只有几十k，但是也比之前的几十几百b强，同时安装也不报下载文件失败访问不了文件的错误了。 后来又遇到：invalid conversion from &#39;const char*&#39; to &#39;char*&#39;这样的代码error，于是网上找到了invalid conversion from ‘const char‘ to ‘char‘ 的解决方法这篇文章，于是便修改了报错处的源码，重新继续编译，看着编译进行的挺顺利，自己还蛮开心的，可谁知命运给自己开的玩笑才刚刚开始。 经过了漫长的编译时间，突然又报错了，说是什么文件访问失败，没有剩余空间了，当时我就蒙了，这是咋回事，后来发现：根目录所在分区满了，安装过程中所有文件都保存在根目录所在分区，即已经到了30G了，使用df -h命令查看所在分区使用率已经到了100%，我擦！！！ 这可如何是好，没想到编译个tensorflow这么占存储空间，后来网上查找相关案例和解决方案，暂时只是把用不到的大软件卸载了，如Clion，发现效果不明显，还是占用了29G多，于是心想：老子不装了，不装了还不行吗，回归pytorch，pytorch才是老纸的真爱，卸载之后发现：使用率还是90%，这这这，后来想到满了那就扩充啊，于是就在网上找扩充根目录分区的方法教程，然而历史说明正是这一步开始使我踏进了深渊。 网上搜了不少方法，发现许多都不好使，同时分区满了我装个软件都装不了了，感觉要炸！发现有个说法说在拓展分区之前要先挂载，于是我就尝试着把根目录所在分区给挂载掉了，在卸载时还提示错误，说device is busy，于是网上找到类似如linux umount命令介绍与device is busy解决方法的答案，于是敲下了罪恶的umount -l /，后来系统崩了，重启，发现又好了，哈哈。 后来就想着硬盘F盘还有剩余空间，想划分出来给根目录所在分区，经过尝试之后发现直接划分不行，因为自己没有搞lvm，不能通过卷组或逻辑卷相关的指令操作来进行，后来发现了一个可行的法子是把硬盘中根目录所在分区位置后面的空间腾出来，然后便可以扩充了，把原来的数据放到其他位置就可以了，于是通过这样的操作进行了var挂载分区的移动，感觉还不错，其中主要参考的是linux(manjaro)磁盘迁移/opt /home,而在进行boot分区的移动时发现自己未成功进行boot分区的重新挂载，于是系统又崩了，重启也报错了，进入grub rescue模式中，此时有点慌了，后来找到类似该篇博客grub rescue救援模式的处理所说内容,重新挂载了boot目录，并且重新生成grub配置文件，于是问题解决了。 之后在进行home分区的操作时自己忘记了备份，直接挂载和格式化掉了（通过 mkfs.ext4 /dev/sda*)，发现出了问题之后重新登录都登录不了，因为相关用户信息都没了，只能进入命令行界面，同时home目录为空，后来在各分区找了半天发现没有找到备份，这时自己真的慌掉了，还有不少数据呢，比如项目代码、还有博客环境配置和博客原文件等，找了半天都没能找到可行的办法，因为系统都登录不进去，连修复软件啥的都安装不了，况且天色已晚，于是就先睡了。 等到第二天自己想到可以在window系统上安装然后修复那个分区的数据吧？于是先尝试了DiskGenius软件，发现好像它识别的分区不全，并且也只能恢复文件，会丢失文件名等信息，这样也仅是得到一些文件，不是整体的恢复分区，而后又下载了testdisk软件，经过一阵蒙蔽的操作之后，发现自己的D盘和E盘也不见了，赶忙重启发现还是没有，由于许多软件都是安装在D盘上，所以导致window系统下的环境也出问题了，要炸啊，幸亏自己的chrome浏览器在C盘装的，又下载了DiskGenius，发现还要注册，还挺麻烦，于是又尝试下了绿色破解版，经过扫描，找到了丢失的D盘和E盘，只是其他数据全部都没有了（指manjaro系统下全部信息）。哎，一声长叹之后只能重新再装系统了。 只是可惜了当时探索了不少软件，同时还有不少有用的数据资料，还有自己最新的代码，以及最新的博客环境配置文件和最新的博客原文件。 而后再划分分区安装的时候，有了之前的教训和探索，自己对分区的理解更加深刻，于是在划分时感觉熟悉了好多，把整个300G空间全部给新系统了，同时多分配了一些空间给根目录和var目录所在分区。后来重装系统，还好自己当时写了安装记录的博客在网上可以看，于是又重新安装配置环境，配置NVIDIA独显，配置科学上网，后来心想以后再做记录先只在简书和有道上吧，自己的博客先不管了吧，毕竟环境也丢了，能找到的是好久之前的了，要配置还挺麻烦，博客内容自己自己也增删改的比较多，再捡起来比较耗时，于是暂时就不考虑维护自己的博客了，之前也尝试在知乎专栏和简书上写文章，发现简书支持Markdown效果很好，而知乎则导入效果很差，于是便最终选择了简书作为最终的自己发布内容的平台啦。 后来重新参考之前提到的编译tensorflow的那篇文章，还是只能master分支，还是要配置hosts文件中github对应域名和ip，不过编译的还挺顺利的，不过还是要好久，最终吃了个晚饭，回来发现又报错了，我真的快要崩溃了，报ImportError: No module named keras.preprocessing的错误，后来我就想是不是版本问题，于是切换r1.12和r1.10，发现还是报bazel.rc文件的错，于是又切换回master分支，继续网上找相关问题和答案，并在github上成功找到了答案：1.10 build fails with “No module named ‘keras_applications’” ,即通过pip安装keras_applications和keras_preprocessing这两个库。 后来重新执行编译命令，又等待了好一会儿（比之前快多了，因为有缓存之前的编译结果），终于成功了哈哈哈！后来经过测试，发现编译成功，tensorflow已经被正常安装在自己的pip列表中了。以下是耗时截图： 安装tensorflow的whl包后，发现安装的就是现行版r1.12。。。 这里是分区使用率截图： 相比而言，pytorch安装就要简单许多了，并且提供了许多预编译好的可选。 在这里自己简单记录一下心酸的历程，也提醒各位看客同样需要编译tensorflow时留意自己的分区使用率～","categories":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/categories/Manjaro/"}],"tags":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/tags/Manjaro/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://www.iamlightsmile.com/tags/Tensorflow/"}]},{"title":"Manjaro下使用图床工具PicGo","slug":"Manjaro下使用图床工具PicGo","date":"2019-04-03T06:12:47.000Z","updated":"2019-04-03T06:16:37.974Z","comments":true,"path":"articles/Manjaro下使用图床工具PicGo/","link":"","permalink":"http://www.iamlightsmile.com/articles/Manjaro下使用图床工具PicGo/","excerpt":"0. 前言在这里简单吐槽以下简书的锁定文章的功能，真是日了狗了！ 因为觉得简书的锁定的文章的功能太恶心，不想受制于人，于是想回到自己的个人博客-Github Pages + Hexo，于是此时图床的问题就来了，原来自己的主要实现逻辑是将图片首先先上传到Github的一个仓库中，写了一个auto_run.sh，然后在浏览器中右击图片得到Markdown格式下的图片连接（自己写了一个浏览器插件），但是还是有些麻烦。","text":"0. 前言在这里简单吐槽以下简书的锁定文章的功能，真是日了狗了！ 因为觉得简书的锁定的文章的功能太恶心，不想受制于人，于是想回到自己的个人博客-Github Pages + Hexo，于是此时图床的问题就来了，原来自己的主要实现逻辑是将图片首先先上传到Github的一个仓库中，写了一个auto_run.sh，然后在浏览器中右击图片得到Markdown格式下的图片连接（自己写了一个浏览器插件），但是还是有些麻烦。 后来网上搜索相关图床工具，得到的方案如下： 搭建私有图床如：使用Chevereto搭建免费私有图床 使用付费图床或免费图床，同时可以配合开源跨平台的的工具PicGo付费的如：又拍云、腾讯云、阿里云、七牛云、微博免费的如：Github 其他相对非主流方案 权衡之后选择使用第二种方案，对于图床而言，自己选择Github，因为有的图床存储还要求备案，如七牛云，很是麻烦。 1. 下载安装PicGo使用如下命令：1yay -S picgo-appimage 这里对于arch系来说不算友好，因为如Windows平台、Mac平台以及其他主流Linux系统如Ubuntu等等都有预编译安装包，而对于arch而言要自己下载编译得到appimage形式的可执行软件包。 2. 设置Github仓库略，可参考最后链接给出文章。 3. 设置仓库信息略，可参考最后链接给出文章。不过我这里不知为何设置网络代理并没有成功,同时实际使用时复制速度挺慢的，不知为何。 4. 上传测试 5. 下载其他依赖如果要使用快捷上传的粘贴板功能时，如图： 还需要下载xclip包1yay -S xclip 6. 总结发现PicGo这个软件还真是挺不错的，同时还了解了appimage这种软件形式 7. 参考 Install Appimage under Arch Linux 图床工具的使用—-PicGo PicGo+GitHub图床，让Markdown飞","categories":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/categories/Manjaro/"}],"tags":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/tags/Manjaro/"}]},{"title":"python库打包分发","slug":"python库打包分发","date":"2019-02-02T05:11:46.000Z","updated":"2019-04-03T05:36:18.022Z","comments":true,"path":"articles/python库打包分发/","link":"","permalink":"http://www.iamlightsmile.com/articles/python库打包分发/","excerpt":"Python库打包分发主要有以下步骤： 注册PyPI账号（执行一次） 安装打包分发工具（执行一次） 编写setup.py文件 通过命令打包分发","text":"Python库打包分发主要有以下步骤： 注册PyPI账号（执行一次） 安装打包分发工具（执行一次） 编写setup.py文件 通过命令打包分发 1.注册PyPI账号进入PyPI的官网，进去注册账号密码，绑定邮箱等。 在成功注册账号之后，创建～/.pypirc文件，在文件中配置自己的PyPI访问地址和账号密码等信息，如下： 123456[distutils]index-servers = pypi[pypi]username:xxxpassword:xxx 2.安装打包分发工具这里通过twine来打包安装。 通过以下命令下载： 1pip install twine 3.编写setup.py文件格式不再详述，具体查看参考1. 举例如： 123456789101112131415161718192021222324252627282930313233from distutils.core import setupimport setuptoolswith open('./README.md', 'r', encoding='utf8') as f: long_description = f.read()with open('./requirements.txt', 'r', encoding='utf8') as f: install_requires = list(map(lambda x: x.strip(), f.readlines()))setup( name='lightNLP', version='0.3.2.0', description=\"lightsmile's nlp library\", author='lightsmile', author_email='iamlightsmile@gmail.com', url='https://github.com/smilelight/lightNLP', packages=setuptools.find_packages(), install_requires=install_requires, long_description=long_description, long_description_content_type='text/markdown', license='Apache-2.0', classifiers=[ 'Development Status :: 4 - Beta', 'Operating System :: OS Independent', 'Intended Audience :: Developers', 'License :: OSI Approved :: BSD License', 'Programming Language :: Python', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: 3.7', 'Topic :: Software Development :: Libraries' ],) 4.通过命令打包分发打包在setup.py文件目录下执行： 1python setup.py sdist bdist_wheel 分发在setup.py文件目录下执行： 1twine upload dist/* 后记之后便可以登录PyPI网站查看自己的projects了。需要注意的是新建包的名字不能在忽视大小写情况下和其他包重复。 参考 Python 库打包分发(setup.py 编写)简易指南 pypi twine","categories":[{"name":"Python","slug":"Python","permalink":"http://www.iamlightsmile.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://www.iamlightsmile.com/tags/Python/"}]},{"title":"Manjaro设置交换分区","slug":"Manjaro设置交换分区","date":"2019-01-24T05:11:13.000Z","updated":"2019-04-03T05:36:20.482Z","comments":true,"path":"articles/Manjaro设置交换分区/","link":"","permalink":"http://www.iamlightsmile.com/articles/Manjaro设置交换分区/","excerpt":"0.前言在安装Manjaro系统的时候发现自己没有设置交换分区，用htop命令发现是空的，所以这里通过后续的命令配置交换分区。","text":"0.前言在安装Manjaro系统的时候发现自己没有设置交换分区，用htop命令发现是空的，所以这里通过后续的命令配置交换分区。 1.查看磁盘信息使用 1sudo fdisk -l 得到如下信息： 2.查看挂载状态使用 1sudo blkid -o list 得到如下信息： 3.设置交换分区使用 1mkswap /dev/sd** 其中**用具体某个分区名替换，如： 4.启用交换分区使用 1swapon /dev/sd** 其中**用具体某个分区名替换，如： 5.查看交换分区状态使用 1swapon -s 或者 1free -m 如： 6.总结至此便设置交换分区成功了。 7.参考 Swap (简体中文)) Linux系统下如何查看所有存储设备（磁盘分区）","categories":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/categories/Manjaro/"}],"tags":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/tags/Manjaro/"}]},{"title":"人生苦短，我用Manjaro！","slug":"人生苦短，我用Manjaro！","date":"2019-01-23T05:10:46.000Z","updated":"2019-11-14T03:20:43.762Z","comments":true,"path":"articles/人生苦短，我用Manjaro！/","link":"","permalink":"http://www.iamlightsmile.com/articles/人生苦短，我用Manjaro！/","excerpt":"想必每一个程序员心里都有一个Linux。 更新:为了便于自己下次重装系统时更方便一些，这里再把过程详细记录一下。","text":"想必每一个程序员心里都有一个Linux。 更新:为了便于自己下次重装系统时更方便一些，这里再把过程详细记录一下。 0.前言一直想学Linux，之前尝试了Ubuntu和centos，虽然两者非常大众化只是觉得有点丑，后来通过搜索了解到了Arch Linux和Manjaro，简单查看之后，觉得非常喜欢Manjaro的众多特性，于是就想着自己装一下，作为自己以后常用的Linux系统来工作。 关于Manjaro的桌面主题，主要有XFce、Gnome、KDE，同时还有社区版Deepin等，简单的对比之后，决定使用KDE，因为感觉功能更丰富。 之前在公司自己尝试着使用虚拟机装了一版，简单尝试了一下，体验还不错，只是不能使用Nvidia显卡来跑GPU程序，这就比较不爽了，毕竟是虚拟机里的东西，到底不如真机下爽快。 趁着这段时间在家相对空闲，同时Windows10下面的Pytorch突然报错（后来通过pip重新安装已修复），有点不能忍，于是打算在自己的电脑上折腾一下双系统：Windows 10 + Manjaro（KDE）。 根据我的个人体验而言，如果机子配置还可以的话，还是选择KDE版本吧，使用起来比Gnome版还是要方便高级，deepin的没试，不过风格也还不错的。 整体下来流程如下： 下载Manjaro系统镜像文件 制作启动盘及划分efi分区 安装系统 基本配置 安装常用应用 科学上网 使用Nvidia独显 系统安装过程中常见问题 更换主题 添加桌面小部件 1.下载Manjaro系统镜像文件从此处选择下载：https://www.manjaro.cn/153 建议选择中科大的镜像源，好像清华的有些时间没更新了呢。 2.制作启动盘及划分efi分区此处仅介绍Windows环境下 从rufus官网下载启动盘刻录工具 选择ISO镜像刻录到U盘，注意选择DD模式 使用diskpart划分efi分区 详情可参考此教程：manjaro双系统双硬盘的安装 3.安装系统 在电脑启动时，按F2进入BIOS系统设置，设置boot启动顺序 随后F10启动系统，按F12后选择USB启动 在进入Manjaro的boot页面时，可以选择lang为中文，driver这里我选择不开源的nonfree版本好像不行，所以我选择了默认的free版本 在分区划分时可以选择并存安装或取代一个分区，这里自己选择手动分区，划分以下分区： 挂载路径 中文目录 建议大小(至少) 其他说明 / 根目录 40G 略 /boot 启动目录 500M 略 /boot/efi 啥意思不重要 10M 略 /home 家目录 40G 平时自己的数据资料等都放下这个目录下面 /var 变量分区 30G 存放在正常运行的系统中其内容不断变化的文件 /srv 服务进程相关 10G 存放服务进程所需的数据文件和一些服务的执行脚本 /opt 其他软件安装目录 30G 发行版附加的一些软件包，如anaconda、pycharm的安装目录 注意： 不要单独挂载/usr目录到单独的分区，我就是因为这个导致了错误重装系统 如果要安装一些比较大型的软件和包时，那么建议多划分一些磁盘大小给根目录，我有两次是因为拓展根目录重装的系统 注意保留一个分区作为swap交换分区，空间大小为内存大小的一半比较合适（其实我也不是很清楚，好像其他资料有这么说） 系统安装流程可以主要参考：manjaro双系统双硬盘的安装。 关于磁盘分配可参考：manjaro 安装分区以及配置方案 4.基本配置在安装完系统后重启可能进不去桌面，此时可以在Grub菜单启动界面按[E]编辑，并在后面加上（注意空格）nouveau.modeset=1 acpi_osi=! acpi_osi=&#39;Windows 2009&#39;，然后按F10进入系统，在进入系统之后，修改grub的配置文件sudo nano /etc/default/grub，比如我的系统中部分参数如下：，随后使用sudo update-grub命令更新grub文件，在之后使用如果还是不能进入系统甚至进入grub rescue模式，原因可能是分区挂载映射问题等，可以参考我的另一篇文章。 在进入系统后要做一些常规配置。 配置更新源并更新系统 配置中国mirrors1sudo pacman-mirrors -i -c China -m rank 随后在出现的窗口中选择更新源，我选择了所有。。。哈哈 添加archlinuxcn和antergos库在/etc/pacman.conf后面添加以下内容： 1234567[archlinuxcn]SigLevel = TrustAllServer = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/$arch[antergos]SigLevel = TrustAllServer = https://mirrors.tuna.tsinghua.edu.cn/antergos/$repo/$arch 同步并更新系统 1sudo pacman -Syyu 安装archlinuxcn签名钥匙&amp;antergos签名钥匙 1sudo pacman -S archlinuxcn-keyring antergos-keyring 安装配置中文输入法 安装搜狗拼音输入法和fcitx管理工具: 123sudo pacman -S fcitx-sougoupinyinsudo pacman -S fcitx-imsudo pacman -S fcitx-configtool 添加配置~/.xprofile文件： 123export GTK_MODULE=fcitxexport QT_IM_MODULE=fcitxexport XMODIFIERS=\"@im=fcitx\" 可参考：Manjaro linux 安装与配置 5.安装常见应用可参考：Manjaro安装后你需要这样做 以下是一些我的常用应用列表： 软件名称 软件说明 typora Markdown编辑软件 deepin-screenshot Deepin截图工具，相对很好用 latte-dock Dock栏 tmux 终端复用工具 bat 终端文件查看工具 gitkraken 跨平台git可视化操作软件 shadowsocks-qt5 科学上网 visual-studio-code-bin VSCode htop 进程查看工具 neofetch 终端查看配置 zsh 好用的shell yay 比pacman更全更好用 google-chrome chrome浏览器 pycharm 强大的Python集成开发环境 anaconda 数据科学集成环境 electronic-wechat Linux下的微信 netease-cloud-music 网易云音乐 ncdu 磁盘占用分析器 xflux flux护眼程序 redshift 同flux,护眼程序 其他的还可以安装deepin系列的微信、QQ和迅雷，只不过我这里不知为何没有安装成功。 如果有道云笔记有Linux版就爽歪歪了。 6.科学上网 使用yay -S shadowsocks-qt5安装shadowsocks软件，选择qt时直接enter选择所有 打开shadowsocks软件，填写配置信息，或者从图片导入等，然后测试延迟，点击连接，对于某一个连接，可以选择程序启动时自动连接，并且可以设置程序在系统登录时启动。如图： Shadowsocks pac代理 安装genpac 1pip install genpac 命令行生成pac文件 1genpac --proxy=\"SOCKS5 127.0.0.1:1080\" --gfwlist-proxy=\"SOCKS5 127.0.0.1:1080\" -o autoproxy.pac --gfwlist-url=\"https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt\" 设置系统自动代理在设置—&gt;网络设置—&gt;代理设置中选择自动代理，URL填写生成的PAC文件地址，file://文件路径/文件名(可以直接把文件拖到URL栏),如图：不过说实在的，不知为何我这里并没有代理成功，玉于是选择了全局代理：可参考：Manjaro17.0.1(xfce)+SS+PAC模式配置笔记linux PAC 自动代理 规则设置 终端代理 安装Proxychains 1yay -S proxychains-ng 编辑proxychains配置将最后面的socks4 127.0.0.1 9095 改为 socks5 127.0.0.1 1080如图： 可参考： Manjaro安装记录 7.使用Nvidia独显使用bumblebee版1. 安装驱动及第三方程序在[系统设置]-[硬件设定]里面选择紫色框住的video-hybrid-intel-nvidia-bumblebee，右键点击安装（不得不提真的超方便啊，和其他系统或发行版相比；注意我这里的显卡是1060，如果是9*0的版本，可能安装的是下面390xx的那个版本） 2. 修改配置文件 修改/etc/bumblebee/bumblebee.conf文件将Driver的值设置为nvidia，来让其使用nvidia驱动，其次将nvidia配置下的PMMethod的值设置为bbswitch，让它使用刚刚安装的bbswitch来进行显卡的切换。 修改（没有则新建）/etc/modprobe.d/bbswitch.conf文件添加options bbswitch load_state=0 unload_state=0来设置bbswitch的状态。 重启电脑，不然系统还不知道bbswitch模块 加载bbswitch模块 1modprobe bbswitch 将用户添加到bumblebee组中 1gpasswd -a username bumblebee 上面的username替换为自己的用户名，如我的是lightsmile 设置bumblebeed服务为开机自启 1systemctl enable bumblebeed 启用bumblebeed服务 1systemctl start bumblebeed 切换使用独显和集显。 打开独立显卡命令： 1sudo tee /proc/acpi/bbswitch &lt;&lt;&lt; ON 关闭独立显卡命令： 1sudo tee /proc/acpi/bbswitch &lt;&lt;&lt; OFF 查看nvidia显卡相关信息 1nvidia-smi 如图所示： 可参考：arch如何使用独显和Manjaro linux 安装与配置 8.系统安装过程中常见问题常见问题可参考：Manjaro linux 安装与配置 9.更换主题自己探索如图所示： 10.添加桌面小部件自己探索桌面右键，选择添加部件，然后在左侧的部件栏中自行选择想要添加的部件，然后可以在桌面中拖放，选择位置，如图： 11.小彩蛋1. 吃豆人 打开/etc/pacman.conf文件 在“# Misc options”部分，去掉“Color”前的“#”。 添加“ILoveCandy”。12.后记 最初后记安装系统容易，不过配置nvidia驱动则比较麻烦，笔者前前后后重装了不下十余次系统，苦心人天不负，最终得以成功安装，真是坑了个爹啦！ 不过最终桌面效果如下，也算小有所获啦～ 新版后记Linux一时爽，崩后火葬场。望大家小心谨慎使用系统，注意及时数据备份，要不是用于编程开发，还是Windows爸爸管着好啊！ 13.参考 Manjaro安装与基本配置 linux PAC 自动代理 规则设置 pyCharm最新2019激活码 manjaro安装及设置 人生苦短，我用Manjaro Bumblebee (简体中文) - ArchWiki) Manjaro安装以及美化教程 Manjaro安装后你需要这样做 Manjaro Deepin安装使用分享 安装Arch Linux之后要做的几件事情 我的 Manjaro🐧 - 网上冲浪指南 Manjaro linux 安装与配置 arch如何使用独显","categories":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/categories/Manjaro/"}],"tags":[{"name":"Manjaro","slug":"Manjaro","permalink":"http://www.iamlightsmile.com/tags/Manjaro/"}]},{"title":"个性化hexo主题","slug":"个性化hexo主题","date":"2019-01-07T02:48:35.000Z","updated":"2019-01-20T01:51:59.959Z","comments":true,"path":"articles/个性化hexo主题/","link":"","permalink":"http://www.iamlightsmile.com/articles/个性化hexo主题/","excerpt":"个性化hexo主题1. 换Material X主题原来自己用的是indigo主题，还是蛮material的，只是后来发现material x的主题更好看，插件更加丰富一些，于是就探索切换了一番，哈哈哈。","text":"个性化hexo主题1. 换Material X主题原来自己用的是indigo主题，还是蛮material的，只是后来发现material x的主题更好看，插件更加丰富一些，于是就探索切换了一番，哈哈哈。 1. 下载主题，设置切换首先把主题下下来，然后在config文件里设置 2. 参考使用文档，添加小部件2. 增加Google统计之前看别人的博客下面有网站运行时间，很是羡慕，猜测是统计插件的原因，后来按照教程探索增加Google Analytics 统计功能，发现并不是这个原因，不过可以更详致的统计网站信息了，虽然现在只有自己看。。。 3. 增加运行时间网上搜索到教程，然后照搬小改完成，原来只是增加一个script脚本，逐渐理解其原理。 4. 增加转载说明参考别人写的个性化hexo博客里有一部分提到了此节，于是将其迁到了ejs中，发现只不过是一段模板代码，初步理解一小点hexo框架原理 5. 增加点击红心功能也是照搬教程，原来也是一段script脚本而已。 6. 增加canvas以及3D效果这个也可以实现，不过考虑实用，并没有加入 7. 增加live2d看板娘效果此技术看起来还是不错的，由于自己本身并不是二次元爱好者，所以就不考虑加入此功能了。 参考网址： 二次元live2d看板娘效果中的web前端技术 给hexo博客增加live2d看板卡通人物动画","categories":[],"tags":[]},{"title":"统计学重要知识点","slug":"统计学重要知识点","date":"2019-01-06T04:36:57.000Z","updated":"2019-04-03T06:28:58.396Z","comments":true,"path":"articles/统计学重要知识点/","link":"","permalink":"http://www.iamlightsmile.com/articles/统计学重要知识点/","excerpt":"统计学重要知识点1. 概念 概率：概率是一个事件发生、一种情况出现的可能性大小的数量指标，介于0与1之间。 分布：分布包括离散分布和连续分布，用来表述随机变量取值的概率规律。 概率密度函数（probability density function，PDF）： 在数学中，连续型随机变量的概率密度函数（在不至于混淆时可以简称为密度函数）是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。当概率密度函数存在时，累计分布函数式概率密度函数的积分。 累积分布函数（cumulative distribution function，CDF）：又叫分布函数，是概率密度函数的积分，能完整描述一个实随机变量X的概率分布，一般以大写“CDF”标记。","text":"统计学重要知识点1. 概念 概率：概率是一个事件发生、一种情况出现的可能性大小的数量指标，介于0与1之间。 分布：分布包括离散分布和连续分布，用来表述随机变量取值的概率规律。 概率密度函数（probability density function，PDF）： 在数学中，连续型随机变量的概率密度函数（在不至于混淆时可以简称为密度函数）是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。当概率密度函数存在时，累计分布函数式概率密度函数的积分。 累积分布函数（cumulative distribution function，CDF）：又叫分布函数，是概率密度函数的积分，能完整描述一个实随机变量X的概率分布，一般以大写“CDF”标记。 伯努利分布（零一分布、两点分布、0-1分布）伯努利试验成功的次数服从伯努利分布 二项分布： 重复n次独立的伯努利试验 泊松分布: 在二项分布的伯努利试验中，如果试验次数n很大，二项分布的概率p很小，且乘积$\\lambda=np$比较适中，则事件出现的次数的概率可以用泊松分布来逼近。事实上，二项分布可以看做泊松分布在离散时间上的对应物。同样的，泊松分布也可看为二项分布在特殊情况下的极限。 大数定律: 在数学与统计学中，大数定律又称大数法则、大数律，是描述相当多次数重复实验的结果的定律。根据这个定律知道，样本数量越多，则其算术平均值就有越高的概率接近期望值。大数定律很重要，因为它“说明”了一些随机事件的均值的长期稳定性。 正态分布: 正态分布又名高斯分布，是一个非常常见的连续概率分布。正态分布是自然科学与行为科学的定量现象的一个方便模型。各种各样的心理学测试分数和物理现象比如光子计数都被发现近似地服从正态分布，尽管这些现象的根本原因经常是未知的，理论上可以证明如果把许多小作用加起来看做一个变量，那么这个变量服从正态分布。 中心极限定理： 中心极限定理是指概率论中讨论随机变量序列部分和分布渐进于正态分布的一类定理。这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量近似服从正态分布的条件。它是概率论中最重要的一类定理，有广泛的实际应用背景。在自然界与生产中，一些现象受到许多相互独立的随机因素的影响，如果每个因素所产生的影响都很微小时，总的影响可以看作是服从正态分布的。中心极限定理就是从数学上证明了这一现象。 离散分布：离散分布描述离散随机变量的每个值的发生概率，如伯努利分布、二项分布、泊松分布。离散随机变量是指具有可计数的值的随机变量，例如非负整数的列表。在离散概率分布中，离散随机变量的每个可能值可与一个非零概率想关联。因此，离散概率分布通常具有表格形式。 连续分布：连续分布描述连续随机变量的可能值的概率，例如正太分布。连续随机变量是一组无限且不可计数的可能值（称为范围）的随机变量。连续随机变量（X）的概率被定义为其PDF曲线下的面积。因此，只有值范围才能具有非零的概率。连续随机变量等于某个值的概率始终为零。 2. 相关1. 概率分布的种类 概率分布要么是连续概率分布，要么是离散概率分布，这取决于它们是定理连续变量还是离散变量的概率。 2. 大数定理和中心极限定理的联系和区别 大数定律（LLN）和中心极限定理（CLT）的联系与区别在于： 共同点：都是用来描述独立同分布（i.i.d）的随机变量的和的渐近表现（asymptotic behavior） 区别：它们描述的是在不同的收敛速度（convergence rate）之下的表现，其次LLN前提条件弱一点：$E(|X|) &lt; \\infty$，CLT条件强一点:$E(X^2) &lt; \\infty$。 假设有n个i.i.d的随机变量，令它们的和为$S_p = \\sum^{n}_{i=1}X_i$.大数定律（以其中弱大数定律为例）说的是$\\frac{1}{n}S_n - E(X) \\underrightarrow{P} 0$.中心极限定理说的是$\\sqrt{n}(\\frac{1}{n}S_n - E(X)) \\underrightarrow{D} N(0,E)$.作者：Detian Deng；来源：知乎；原文链接 大数定律讨论的是依概率收敛，中心极限定理涉及按分布收敛（按分布收敛比点点收敛弱很多啊）。私以为搞清楚随机变量序列的收敛性就是为了方便在样本量很大情况下计算概率。。。 大数定律是说随机变量序列的算术平均以概率收敛到其均值的算术平均。比较经典的运用就是用频率确定概率，比如估计不合格品率，抽样的不合格品比例就是可以作为总体的不合格品率估计值。 中心极限定理说的是给出随机变量和的分布函数在什么条件下收敛到正太分布。比较熟悉的比如用来估计误差，误差基本上都是由这样那样大大小小微小因素叠加的，这些个因素相加就是总的误差，这个时候就要用到参数估计和假设检验了，在认为误差近似正太分布情况下，给出误差上限下限，置信度，判断质量是否达到要求等等工作就可以做了。作者：yyylll；来源：知乎；原文链接 大数定律揭示了大量随机变量的平均结果，但没有涉及到随即变量的分布的问题。而中心极限定理说明的是在一定条件下，大量随机独立变量的平均数是以正态分布为极限的。 3. 泊松分布的现实意义 马同学的讲解：泊松分布的现实意义是什么，为什么现实生活多数服从于泊松分布？ 先说结论：泊松分布是二项分布n很大而p很小时的一种极限形式二项分布是说，已知某件事情发生的概率是p，那么做n次试验，事情发生的次数就服从于二项分布。泊松分布是指某段连续的时间内某件事情发生的次数，而且“某件事情”发生所用的时间是可以忽略的。例如，在五分钟内，电子元件遭受脉冲的次数，就服从于泊松分布。假如你把“连续的时间”分割成无数小份，那么每个小份之间都是相互独立的。在每个很小的时间区间内，电子元件都有可能“遭受到脉冲”或者“没有遭受到脉冲”，这就可以被认为是一个p很小的二项分布。而因为“连续的时间”被分割成无穷多份，因此n(试验次数)很大。所以，泊松分布可以认为是二项分布的一种极限形式。因为二项分布其实就是一个最最简单的“发生”与“不发生”的分布，它可以描述非常多的随机的自然界现象，因此其极限形式泊松分布自然也是非常有用的。作者：ctian；来源：知乎；原文链接 4. 理解概率分布函数和概率密度函数 产品经理马忠信的讲解： 应该如何理解概率分布函数和概率密度函数？ 3. 参考 连续和离散分布 泊松分布 为什么我觉得大数定理和中心极限定理是矛盾的？ 中心极限定理 中心极限定理 正态分布 二项分布、泊松分布、正态分布的关系","categories":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/categories/数学/"}],"tags":[{"name":"数学","slug":"数学","permalink":"http://www.iamlightsmile.com/tags/数学/"},{"name":"统计学","slug":"统计学","permalink":"http://www.iamlightsmile.com/tags/统计学/"}]},{"title":"概念集锦","slug":"概念集锦","date":"2019-01-05T09:54:10.000Z","updated":"2019-01-07T03:17:15.326Z","comments":true,"path":"articles/概念集锦/","link":"","permalink":"http://www.iamlightsmile.com/articles/概念集锦/","excerpt":"数学 数学是利用符号研究数量、结构、变化以及空间等概念的一门学科，从某种角度看属于形式科学的一种。数学透过抽象化和逻辑推理的使用，由计数、计算、量度和对物体形状及运动的观察而产生。数学家们拓展这些概念，为了公式化新的猜想以及从选定的公理及定理中建立起严谨推导出的定理。","text":"数学 数学是利用符号研究数量、结构、变化以及空间等概念的一门学科，从某种角度看属于形式科学的一种。数学透过抽象化和逻辑推理的使用，由计数、计算、量度和对物体形状及运动的观察而产生。数学家们拓展这些概念，为了公式化新的猜想以及从选定的公理及定理中建立起严谨推导出的定理。 函数 函数，在数学中，为两集合间的一种对应关系：输入值集合中的每项元素皆能对应唯一一项输入值集合中的元素。 决定论 决定论，是一种哲学立场，认为每个事件的发生，包括人类的认知、举止、决定和行动，都有条件决定它发生，而非另外的事件发生。决定论认为，自然界和人类世界中普遍存在一种客观规律和因果关系。一切结果都是由先前的某种原因导致的，或者是可以根据前提条件来预测未来可能出现的结果。其重要观点即是：“有其因必有其果。”或黑格尔的“凡是合乎理性的东西都是现实的，凡是现实的东西都是合乎理性的。”","categories":[],"tags":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/tags/哲学/"},{"name":"概念","slug":"概念","permalink":"http://www.iamlightsmile.com/tags/概念/"}]},{"title":"你好，2019","slug":"你好，2019","date":"2019-01-01T14:41:46.000Z","updated":"2019-01-07T03:14:21.008Z","comments":true,"path":"articles/你好，2019/","link":"","permalink":"http://www.iamlightsmile.com/articles/你好，2019/","excerpt":"再见，2018.","text":"再见，2018. 终于领悟到，自己早就已经成年，已经步入社会，该是个成熟的成年人了，不再是小孩子了。 要学会一个人照顾好自己，经营好自己，不要发脾气，不要因畏惧而胆怯。 要学会先做应该做的事，再做自己想做的事。 独自在外，要有一个人远航的心理准备，没有家和学校这样的避风港，许多事就算不想做也要去做，不要再想当然的随意而为了，要学会对自己负责，称为真正心理成熟的成年人。 凡事要做好规划，目标要明确，要有强大的执行力作为支撑，要有强大的意志力做燃料。 不要再去讲黑白对错了，社会永远都是森林法则，弱肉强食，规律如此，何来对错？学会适应这个法则吧，努力变强，什么都会有的。 不要再执念于过去了，回忆再美好也只是曾经，现在再悔恨也于事无补，把视角向前看吧。 不要再惦记喜欢的人了，现在的你配不上她。 不喜欢的饭就算难以下胃也要吃下去，不要饿肚子。 不要只有想法而不付出行动，想法谁都有，最终还是行动说明一切。 该向游戏和网络小说说再见了，不要再把精力放到没有意义的事情上去了。 过去曾经幼稚的孩子，也该死掉了。 你好，2019.","categories":[],"tags":[]},{"title":"自然语言处理（1）基本概念","slug":"自然语言处理（1）基本概念","date":"2018-05-26T14:23:47.000Z","updated":"2019-01-07T03:19:14.194Z","comments":true,"path":"articles/自然语言处理（1）基本概念/","link":"","permalink":"http://www.iamlightsmile.com/articles/自然语言处理（1）基本概念/","excerpt":"自然语言处理（Natural Language Processing，NLP）：自然语言处理是人工智能和语言学领域的分支学科，主要研究如何让计算机处理和运用自然语言。 自然语言处理广义上分为两大部分，第一部分是自然语言理解，是指让电脑“懂”人类的语言；第二部分为自然语言生成，是指把计算机数据转化为自然语言。","text":"自然语言处理（Natural Language Processing，NLP）：自然语言处理是人工智能和语言学领域的分支学科，主要研究如何让计算机处理和运用自然语言。 自然语言处理广义上分为两大部分，第一部分是自然语言理解，是指让电脑“懂”人类的语言；第二部分为自然语言生成，是指把计算机数据转化为自然语言。 自然语言处理研究的内容： 机器翻译（machine translation，MT）：实现一种语言到另一种语言的自动翻译。 自动文摘（automatic summarizing或automatic abstracting）：将原文档的主要内容和含义自动归纳、提炼出来，形成摘要或缩写。 信息检索（information retrieval）：信息检索也称情报检索，就是利用计算机系统从海量文档中找到符合用户需要的相关文档。面向两种或两种以上语言的信息检索叫做跨语言信息检索（cross-language/trans-lingual information retrieval）。 文档分类（document categorization/classification）：文档分类也称文本分类（text categorization/classification）或信息分类（information categorization/classification），其目的就是利用计算机系统对大量的文档按照一定的分类标准（例如，根据主题或内容划分等）实现自动归类。 问答系统（question-answering system）：通过计算机系统对用户提出的问题的理解，利用自动推理等手段，在有关知识资源中自动求解答案并做出相应的回答。 信息过滤（information filtering）：通过计算机系统自动识别和过滤那些满足特定条件的文档信息。通常指网络有害信息的自动识别和过滤，主要用于信息安全和防护、网络内容管理等。 信息收取（information extraction）：指从文本中收取出特定的事件（event）或事实信息，有时候又称事件抽取（event extraction）。 文本挖掘（text mining）：有时又称数据挖掘（data mining），是指从文本（多指网络文本）中获取高质量信息的过程。 舆情分析（public opinion analysis）：舆情是指在一定的社会空间内，围绕中介性社会事件的发生、发展和变化，民众对社会管理者产生和持有的社会政治态度。 隐喻计算（metaphorical computation）：“隐喻”就是用乙事物或其某种特征来描述甲事物的语言现象。简要的讲，隐喻计算就是研究自然语言语句或篇章中隐喻修辞的理解方法。 文字编辑和自动校对（automatic proofreading）：对文字拼写、用词，甚至语法、文档格式等进行自动检查、校对和编排。 作文自动评分：对作文质量和写作水平进行自动评价和打分。 光读字符识别（optical character recognition，OCR）：通过计算机系统对印刷体或手写体等文字进行自动识别，将其转换成计算机可以处理的电子文本，简称字符识别或文字识别。 语音识别（speech recognition）：将输入计算机的语音信号识别转换成书面语表示。 文语转换（text-to-speech conversion）：将书面文本自动转换成对应的语音表征，又称语音合成（speech synthesis）。 说话人识别/认证/验证（speaker recognition/identification/verification）：对一说话人的言语样本做声学分析，依据推断（确定或验证）说话人的身份。（摘自《统计自然语言处理》（第2版）） 自然语言处理涉及的几个层次：形态学（morphology）、语法学（syntax）、语义学（semantics）、语用学（pragmatics）。","categories":[],"tags":[]},{"title":"腾讯云SDKforJS开发实战","slug":"腾讯云SDKforJS开发实战","date":"2018-04-21T07:31:34.000Z","updated":"2019-01-07T03:19:04.554Z","comments":true,"path":"articles/腾讯云SDKforJS开发实战/","link":"","permalink":"http://www.iamlightsmile.com/articles/腾讯云SDKforJS开发实战/","excerpt":"前面曾经提到过的，我想要把自然语言处理相关的技术接入到我的毕设微信小程序里面。 由于腾讯云未提供JS的SDK，要自己编写HTTP请求来实现，之前觉得比较麻烦，相关说明文档没有整明白，不想尝试，后来觉得既然是自己选择的路，那么无论再苦再累，都要坚持走下去，无论结果是什么，也算对得起当初自己的豪情壮志了。","text":"前面曾经提到过的，我想要把自然语言处理相关的技术接入到我的毕设微信小程序里面。 由于腾讯云未提供JS的SDK，要自己编写HTTP请求来实现，之前觉得比较麻烦，相关说明文档没有整明白，不想尝试，后来觉得既然是自己选择的路，那么无论再苦再累，都要坚持走下去，无论结果是什么，也算对得起当初自己的豪情壮志了。 再贴一下我的参考文档：腾讯云文智自然语言处理API链接 大致了解了签名方法和技术之后，我就在网上搜关于JS的HmacSHA256加密和Base64编码的函数库，发现了Crypto.js，在官网上吧zip文件下下来之后，参照一篇文章细说CryptoJs使用（微信小程序加密解密)把core.js和sha256.js和enc-base64.js的内容放到一个叫crypto.js文件中，最后再加上module.exports = CryptoJS。（话说我之前看截图里是module.export = CryptoJS，我也就照搬下来，试了半天结果总是不行，后来发现export后面还有个s，真是尬） 随后测试了一下这个函数库，经过测试发现加密的结果与API文档中的样例一致，然后就开始下一步了。 关于具体的格式我也不是很清楚，经过大胆尝试之后总是说有问题，后来只能去学习Python SDK的源码了。 通过源码的探索，我了解到了从最初的传参到生成URL的整体过程，同时也注意到了一些文档中并没有提到的细节和注意事项。然而经过测试之后还是提示说鉴权失败，我真是日了狗了。不过最终我还是成功了，下面是几个记录： 我下载的SDK是Python SDK，而我要编写的是JS代码。其中的time获取函数不同，其中Python中为int(time.time()),获取到的是整数时间戳，而JS中的则为Date.now()获取到的是以毫秒为单位的所以要做进一步的转化为Math.floor(Date.now()/1000) 同时要使用SDK必须要带要有版本号如SDK_PYTHON_2.0.13，如果没有则提示错误信息：Exception ‘Version’，而腾讯云中并没有提到这一点，所以我们必须要模拟一个，即加一个这样的参数：RequestClient=SDK_PYTHON_2.0.13 同时最最他妈的坑爹的一点就是腾讯云中明确提到了签名方式用HmacSHA1或者HmacSHA256都可以，只要参数中说明即可，即加上：SignatureMethod: ‘HmacSHA1’即可，而我之前网上搜的大多使用256，所以我也使用256，但是一直报鉴权失败的问题，可是我的签名方式（拿腾讯云提供的demo测试了一下，得到的签名一致）和参数都没有问题，困扰了好久，最终我把Python的SDK源码改了改，如修改当前时间戳Timestamp、修改随机正整数Nonce等，发现了Timestamp不能乱改，和腾讯云的服务器的时间戳间隔不能超过2小时，同时时间戳一定的情况下Nonce不能重复，后来在Python SDK上尝试使用HmacSHA256发现也报错了！我的哥！又改回HmacSHA1发现又没问题了，不会是现在腾讯云后台不支持256吧，然后我把JS的代码相关部分也改成了使用HmacSHA1，皇天不负有心人，终于他妈的成功了！不行你倒是说一声啊，同时具体的一些细节也不交代清楚。 同时还有的是JS实现网络调用的库，因为不同的引擎不同的平台所以实现肯定会有所不同，我最终搜索到了Fly.js这个很好用的库。 简体中文版说明文档链接福利在这里！Flyio帮助文档 不同的环境Fly的入口文件不同。 前面提到了腾讯云没有JS的SDK，而作为读过了Python SDK的程序员，难以控制自己想要编写自己的SDK的冲动，一方面是也算是一个挑战和尝试，一方面是这样也方便了自己的使用，最后好像是莫名的强迫症，如果不做的话实在是受不了。 于是大致样式就开始照搬Python SDK的实现，同时由于自己仅仅是小尝试，简单成功就可以了，至于module的健壮性和架构也就忽略掉了。 我的是在Webstorm上编写调试JS的，引擎是NodeJs。由于报import、from的错，所以只能含泪把es6的语法写成了CommonJS的形式。 最终的成果长成这么个吊样子，文件名为qcloud.js： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/*不同的应用平台fly.js的引入方式不同，同时fly的内部实现也略有不同毕竟不同的平台js的引擎也不同，所以相关网络请求访问的底层实现也会有所不同比如说浏览器环境中是构造XmlHttpRequest执行ajax调用而微信小程序则是使用底层api：wx.request函数node环境依赖http模块及net模块的底层实现详情参见 https://wendux.github.io/dist/#/doc/flyio/readme不同环境下文件引入实例：具体文件自己去找，去下浏览器环境const Fly = require('./fly.umd.min')微信小程序环境const Fly = require('./fly')node环境const Fly=require(\"flyio/src/node\")*///以下是node环境下引入fly的方式const Fly = require(\"flyio/src/node\");const CryptoJS = require('./crypto');function sortKeys(obj) &#123; let newobj = &#123;&#125;; Object.keys(obj).sort().forEach(value =&gt; newobj[value] = obj[value]); return newobj&#125;function codeObj(obj) &#123; let arr = []; for (let k in obj) &#123; arr.push(k + '=' + obj[k]) &#125; return arr.join('&amp;')&#125;class QCloud &#123; constructor()&#123; this.config = &#123; protocol:'https://', path:'/v2/index.php', method:'GET', region:'gz', domain:'.api.qcloud.com', requestClient:'SDK_PYTHON_2.0.13', signatureMethod:'HmacSHA1', secretId:'', secretKey:'' &#125; this.params = &#123;&#125; &#125; init(config)&#123; Object.assign(this.config,config); this.fly = new Fly; this.fly.config.baseURL = this.config.protocol+this.config.module+this.config.domain &#125; getParams() &#123; return this.config.method+this.config.module+this.config.domain+ this.config.path+'?'+codeObj(sortKeys(this.params)) &#125; getUrl(action,params)&#123; this.initParams(); this.params.Action = action; Object.assign(this.params,params); this.sign(); return this.config.protocol+this.config.module+this.config.domain+this.config.path+'?'+this.getParams() &#125; use(action,params,fthen,fcatch)&#123; this.initParams(); this.params.Action = action; Object.assign(this.params,params) this.sign() this.request(fthen,fcatch) &#125; sign()&#123; let pa = this.getParams(); let signnature = CryptoJS.enc.Base64.stringify(CryptoJS.HmacSHA1(pa,this.config.secretKey)); this.params.Signature = signnature &#125; initParams()&#123; this.params = &#123; Region: this.config.region, Nonce: Math.floor(Math.random()*Number.MAX_SAFE_INTEGER), Timestamp: Math.floor(Date.now()/1000), RequestClient: this.config.requestClient, SignatureMethod: this.config.signatureMethod, SecretId: this.config.secretId, &#125; &#125; request(fthen,fcatch)&#123; this.fly.get(this.config.path,this.params).then(fthen).catch(fcatch) &#125;&#125;module.exports = new QCloud(); 这个不写注释的问题好像确实要改。。。 下面是测试代码文件（testMyModule.js）：12345678910111213141516const QCloud = require('./qclooud')QCloud.init(&#123; module:'wenzhi', secretId:'自己的secretId', secretKey:'自己的secretKey'&#125;)let params = &#123; 'content':'人生苦短，please Python。太祖、刘邦、朱元璋哪个更厉害？！'&#125;QCloud.use('TextClassify',params,function (responce) &#123; console.log(responce.data)&#125;,function (error) &#123; console.log(error)&#125;) 下面是输出结果：1&#123;\"code\":0,\"message\":\"\",\"codeDesc\":\"Success\",\"classes\":[&#123;\"class\":\"\\u6587\\u5316\",\"class_num\":61,\"conf\":0.713&#125;,&#123;\"class\":\"\\u5386\\u53f2\",\"class_num\":95,\"conf\":0.221&#125;,&#123;\"class\":\"\\u672a\\u5206\\u7c7b\",\"class_num\":0,\"conf\":0.066&#125;]&#125; 关于上面\\u问题的出现，我们使用eval函数把json string转成json object就可以了，如：1console.log(eval('('+responce.data+')')) 即可得下面的结果：1234567&#123; code: 0, message: '', codeDesc: 'Success', classes: [ &#123; class: '文化', class_num: 61, conf: 0.713 &#125;, &#123; class: '历史', class_num: 95, conf: 0.221 &#125;, &#123; class: '未分类', class_num: 0, conf: 0.066 &#125; ] &#125; 因为JS本身异步调用的特点，使得我们对于事件的业务逻辑处理许多时候都是通过回调函数来解决而并非Python常见的同步处理。下面是相关的Python调用示例：1234567891011121314151617181920212223242526272829303132from QcloudApi.qcloudapi import QcloudApifrom settings import secretId,secretKeyimport jsonfrom pprint import pprintmodule = 'wenzhi'config = &#123; 'secretId': secretId, 'secretKey': secretKey, 'Region': 'gz', 'method': 'POST'&#125;action = 'TextClassify' #文本分类params = &#123; 'content':'人生苦短，please Python。太祖、刘邦、朱元璋哪个更厉害？！'&#125;try: service = QcloudApi(module,config) print(service.generateUrl(action,params)) pprint(json.loads(service.call(action,params)))except Exception as e: print('Exception',e) 看起来我实现的module还是有点样子的嘛哈哈！ 我已经把相关的源码放到了我的GitHub上，欢迎各位有需要的看客们下载使用！","categories":[],"tags":[]},{"title":"科技发展之小感慨","slug":"科技发展之小感慨","date":"2018-04-21T07:23:00.000Z","updated":"2019-01-07T03:17:42.813Z","comments":true,"path":"articles/科技发展之小感慨/","link":"","permalink":"http://www.iamlightsmile.com/articles/科技发展之小感慨/","excerpt":"现在真的是越来越理解“人生而有涯而知也无涯”了。 现在科技发展的速度这么快，技术更迭周期越来越短，我们这些将来要搞编程的到时候真的是活到老学到老啊！ 比如近几年来大火的人工智能、云计算、大数据、机器学习、深度学习等等，还有物联网和机器人技术等，就在不久的将来，一定会极大的改变人们的生产生活方式。","text":"现在真的是越来越理解“人生而有涯而知也无涯”了。 现在科技发展的速度这么快，技术更迭周期越来越短，我们这些将来要搞编程的到时候真的是活到老学到老啊！ 比如近几年来大火的人工智能、云计算、大数据、机器学习、深度学习等等，还有物联网和机器人技术等，就在不久的将来，一定会极大的改变人们的生产生活方式。 现在发现自己真的是一个井底之蛙，所知甚少。最近接触和了解的一些工具、术语和概念简直让我头大，比如说什么Racket语言、Julia语言、Dart语言、Scala框架、Flink框架、Flutter框架、Scrapy框架等等，虽然从理论上讲这些都是术与器的层面，但是这些东西的技术覆盖面和更迭速度还是有些让我措手不及，感觉要学的东西好多啊。 哎，果然人生最重要的是要学会选择。要想得到什么，就必须要做好失去其他的心理准备。 只有与时俱进，终身学习的人才能不被这个时代落后太远。 相对来讲，如果所从事的职业是医学、法律等知识更迭速度比较慢、半衰期很长的领域，那么可能是经验越丰富越值钱，而我们搞编程的不仅是技术，甚至是思想也会变化。如果不注重培养自己的核心竞争力，稍有不慎，可能就被年轻人迎头赶上，丢掉低级码农的饭碗。 相对而言，搞纯工程以后的道路可能是技术总监、项目经理啥的，如果搞学术，可能自己真的没有这个条件了，另外的出路应该是除了计算机领域之外，去学习和掌握一些其他领域的知识智慧，做一个跨学科的交叉复合型人才，这样不仅可以更好的发展和应用该领域的知识学问，还可以使计算机更好的发挥价值吧？","categories":[],"tags":[]},{"title":"自然语言处理与知识图谱相关市场研究","slug":"自然语言处理与知识图谱相关市场研究","date":"2018-04-19T14:15:10.000Z","updated":"2019-01-07T03:20:14.060Z","comments":true,"path":"articles/自然语言处理与知识图谱相关市场研究/","link":"","permalink":"http://www.iamlightsmile.com/articles/自然语言处理与知识图谱相关市场研究/","excerpt":"经过我的探索，发现现在商用也好，学习也罢，目前提供NLP技术服务的大致分三种： 一种是大学院校的教授、助教和研究生等依托团队的学术背景和技术沉淀，通过创办规模较小的公司提供技术支持，以实现技术变现。服务对象主要为科研院所、政府部门、一些没有精力或没有必要自己去做这方面服务的大公司和一些没有条件或没有必要自己去做的中小型公司。 一种是大公司如Google、Facebook、Microsoft、阿里、腾讯、百度、华为、科大讯飞等，目前也都在这方面发力。其中关于自然语言处理方面在BAT三者之间应该是百度做的最早，目前腾讯的比较成熟，而阿里在这方面才刚刚开始，前两天才公测结束，服务正式上线日期官网说是4月23号。科大讯飞的特点应该主要是语音相关的。华为的不了解。这些公司相关领域技术自己是要自己开拓发展的。 第三种则应该是比较专业，主打自然语言处理及其衍生相关服务的了。基本上创始人和主要的技术人员都是来自国内外知名的大学和公司，有学习相关的专业技术，并有丰富的从业经验，后来自主创业，开辟相关市场，想要在自然语言处理服务领域彻底火起来之前做大做强，多吃点蛋糕。国外的有不少，国内的现在也在发展中，估计目前至少有10家公司在做相关的了。服务对象和第一种基本相同。","text":"经过我的探索，发现现在商用也好，学习也罢，目前提供NLP技术服务的大致分三种： 一种是大学院校的教授、助教和研究生等依托团队的学术背景和技术沉淀，通过创办规模较小的公司提供技术支持，以实现技术变现。服务对象主要为科研院所、政府部门、一些没有精力或没有必要自己去做这方面服务的大公司和一些没有条件或没有必要自己去做的中小型公司。 一种是大公司如Google、Facebook、Microsoft、阿里、腾讯、百度、华为、科大讯飞等，目前也都在这方面发力。其中关于自然语言处理方面在BAT三者之间应该是百度做的最早，目前腾讯的比较成熟，而阿里在这方面才刚刚开始，前两天才公测结束，服务正式上线日期官网说是4月23号。科大讯飞的特点应该主要是语音相关的。华为的不了解。这些公司相关领域技术自己是要自己开拓发展的。 第三种则应该是比较专业，主打自然语言处理及其衍生相关服务的了。基本上创始人和主要的技术人员都是来自国内外知名的大学和公司，有学习相关的专业技术，并有丰富的从业经验，后来自主创业，开辟相关市场，想要在自然语言处理服务领域彻底火起来之前做大做强，多吃点蛋糕。国外的有不少，国内的现在也在发展中，估计目前至少有10家公司在做相关的了。服务对象和第一种基本相同。 除此之外，其他的一些技术公司或大公司中则主要使用公司内部相关研发团队或技术部门主要针对与公司业务的技术和成果做一些辅助工作，不提供外部服务，没有想发展壮大的目标计划，同时种种原因也不想借助他人的服务。 在知识图谱相关应用中，有用于企业决策的，也有用于金融分析的，基本情况应该是基本类似于自然语言处理服务领域。","categories":[],"tags":[]},{"title":"腾讯云-云智自然语言处理API小试","slug":"腾讯云-云智自然语言处理API小试","date":"2018-04-19T13:02:27.000Z","updated":"2019-01-07T03:18:51.003Z","comments":true,"path":"articles/腾讯云-云智自然语言处理API小试/","link":"","permalink":"http://www.iamlightsmile.com/articles/腾讯云-云智自然语言处理API小试/","excerpt":"前述（妈耶！这是昨天的文章了，昨天晚上写着写着突然断电，而我的电脑是台式机。。。） 今天在忙毕设的事情，毕设项目是做一个微计划日程管理的小程序，目前已经完成了大部分的功能。 其中包括图表统计、时间轴、四象限、小卡片、数据备份和数据还原等功能。不过今天刚通知了说审核失败，理由是身份为个人的开发者不能做备忘录相关的微信小程序。 昨天想着能不能把自己的兴趣（自然语言处理+知识图谱）和毕设结合起来，打算通过调用一些开放的自然语言处理的Restful API接口来处理一些todo、plan、target相关分析统计工作。","text":"前述（妈耶！这是昨天的文章了，昨天晚上写着写着突然断电，而我的电脑是台式机。。。） 今天在忙毕设的事情，毕设项目是做一个微计划日程管理的小程序，目前已经完成了大部分的功能。 其中包括图表统计、时间轴、四象限、小卡片、数据备份和数据还原等功能。不过今天刚通知了说审核失败，理由是身份为个人的开发者不能做备忘录相关的微信小程序。 昨天想着能不能把自己的兴趣（自然语言处理+知识图谱）和毕设结合起来，打算通过调用一些开放的自然语言处理的Restful API接口来处理一些todo、plan、target相关分析统计工作。 哈工大的ltp之前尝试过，不过现在调用的结果还是说未授权的用户，虽然网页上显示我的可使用流量还有18G之多。 复旦的话有知识工场有提供知识图谱的相关Restful API，尝试了一下感觉还可以，蛮不错的，有时间也做个记录。 以上的都是使用的是http，而小程序的request请求只能是https，所以许多方法也就都不实用了。除非自己搭设一个https的服务器，然后转接请求http请求，就和代理差不多。 一些其他公司的服务经过探索后也都是有的，比如说百度、阿里、腾讯、华为等。估计差不多都大同小异，用起来也都差不多。基本上都是提供相关平台语言SDK来服务的，如果是要自己动手去写出http请求的话，还要自己对签名进行处理等等，比较麻烦，目前并不想尝试实践。 由于先搜到的腾讯云，同时由于小程序的缘故刚注册的腾讯云的微信公共平台的账号，所以就在腾讯云上学习探索了。 发现调用的方式其实挺简单的，相关的文章网上也早已经有了。 比如说：腾讯文智自然语言处理-分词API Python小实验和开发者实验室体验之文智自然语言处理SDK by python等。 腾讯云文智自然语言处理API链接：https://cloud.tencent.com/document/api/271 下面简单的贴一下相关的代码。 虽然从逻辑上讲这些重复性的代码可以通过运用相关设计模式经验封装成模块和函数更为合理，但是这里仅是简单的测试，所以也就没继续搞了。 情感分析123456789101112131415161718192021222324252627282930313233from QcloudApi.qcloudapi import QcloudApifrom settings import secretId,secretKeyimport jsonfrom pprint import pprintmodule = 'wenzhi'config = &#123; 'secretId': secretId, 'secretKey': secretKey, 'Region': 'gz', 'method': 'POST'&#125;action = 'TextSentiment' #情感分析params = &#123; 'content': \"李亚鹏挺王菲：加油！孩他娘。\"&#125;try: service = QcloudApi(module,config) print(service.generateUrl(action,params)) pprint(json.loads(service.call(action,params)))except Exception as e: print('Exception',e) 运行结果：123456https://wenzhi.api.qcloud.com/v2/index.php&#123;'code': 0, 'codeDesc': 'Success', 'message': '', 'negative': 0.0051898001693189, 'positive': 0.99481022357941&#125; 其中的settings模块里面装有自己的secretId和secretKey，就两行代码而已：12secretId = '自己的secretId'secretKey = '自己的secretKey' 很明显无法打印出有效的URL~因为使用的方法为POST，相关的数据在请求体中而非GET方法中的请求头中。 后面的所有代码只需要修改action和params的值即可 文本抓取12345action = 'ContentGrab'# 文本抓取params = &#123; 'url': 'http://www.iamlightsmile.com'&#125; 运行结果：靠，之前还好好的，现在报错了！1234https://wenzhi.api.qcloud.com/v2/index.php&#123;'code': 4000, 'codeDesc': 'InvalidParameter', 'message': '(-100) service timeout.'&#125; 不知为啥，有时会出现如上的错误，尽管代码没有问题，多尝试几次可能就出现正常的结果了，尽管这种不确定性还是挺烦人的。。。正确结果如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879https://wenzhi.api.qcloud.com/v2/index.php&#123;'code': 0, 'codeDesc': 'Success', 'content': 'lightsmile\\n' '1459679436@qq.com\\n' \"lightsmile's Blog\\n\" \"lightsmile's Blog\\n\" ' lightsmile \\n' '2018-04-18\\n' 'GithubPages新尝试\\n' ' 由于某些不便明说的原因，我要再申请一个域名，再申请一个GitHub账号，再搞一个GitHub ' 'Pages。打算用来记录一些不便明说的东西。 ' '这次域名的申请不同于之前的在万网的iamlightsmile.com，这次是在腾讯云上申请的，不过都大同小异了。接着等待大概3天左右的实名认证，通过后域名就可以解析可用了。由于只是使用GitHub ' 'Pages 作为静态网页，不需要另外购置服务器，所以也不... 阅读全文… \\n' 'GitHub\\n' '2018-04-17\\n' '学习Python设计模式\\n' ' 本书主要参阅的书籍是《精通Python设计模式》 ' '本书分为创建型模式、结构型模式、行为型模式三大类，同时又细分为16种模式。具体到每个模式，则通过简单介绍、现实生活中例子、软件应用实例、应用场景、具体代码实现、小结几部分，多个角度加深对某个设计模式的理解。案例贴近生活，代码简单易懂，描述清晰明白，翻译水平上佳，确实算是我认为的好书，同时翻译还将代码上传到GitHub上方便读者下载学习，这里真... ' '阅读全文… \\n' '2018-04-14\\n' '推荐阅读书籍\\n' ' 此博文作为书籍阅读及相关的记录哲学篇 《和谐辩证法》 《智慧之根》 计算机篇 《统计学习理论基础》 《大数据智能》 ' '《统计自然语言处理》（第二版） 《Python自然语言处理》（第2版，没有纸质） 基础篇 《线性代数及其应用》 历史篇小说篇 ' '《天行健》 《英雄志》 《国士无双》 思维篇 《如何系统思考》 阅读全文… \\n' '2018-04-07\\n' '哈工大ltp小试\\n' ' 今天开始探索学习使用哈工大的LTP（Language Technology Platform）。 这里是官网地址 ' '这里是GitHub地址 这里是pyltp的使用文档 ' '平台采用的语言是C++，但是也提供了Python和Java的封装。由于本人目前使用Python作为自然语言处理的工具语言，所以以下的探索流程都是使用本人电脑中的Window8.1操作系统的PyCharm集成开发环境，使用的Pyt... ' '阅读全文… \\n' 'Python\\n' 'ltp\\n' '自然语言处理\\n' '2018-04-06\\n' 'Scrapy爬取知乎数据小试\\n' ' 啊啊啊，没时间写啦，以后有时间再写吧！ 。。。发现今天是周五，不熄灯。。。 ' '前两周一直在忙毕设的事情，由于某些原因毕设选择了相对简单的微信小程序，经过奋战之后一些主要的基本功能已经实现多半。 ' '自然语言处理的一些最基本的概念已经有所了解，下面想要找点实战项目练练手。由于处理的第一步便是要获取语料，想着以后爬虫这东西肯定是要学的，于是从昨天开始学习相关视频、配置相关环境，今天看了部分，照着Dem... ' '阅读全文… \\n' 'Scrapy\\n' '爬虫\\n' '2018-04-03\\n' '随想\\n' ' 其实这世上哪有什么善恶，有的只是不同环境下不同的选择。 我发现人和人相识的过程基本上都是从他的经历中提取特质然后贴上标签的过程。 ' '普天之下又有多少人敢把自己的灵魂放在阳光下炙烤呢？草他妈的！ 阅读全文… \\n' '2018-03-28\\n' '计算机\\n' '微信小程序的component\\n' ' ' '我发现无法直接在样式即wxss里通过color属性设置icon组件的颜色，是无效的，只能通过在wxml里设置它的color属性为js传入的变量值或者是通过变量值来控制具体的颜色值。 ' '我们可以将微信小程序中的components组件视为一个对象，没错，它本来就是一个对象，只是相对而言，它的初始化方法和设置方式不同于在一般的js语言中，它的data属性里是这个对象建立时初始化时的数据，作用域... ' '阅读全文… \\n' '微信小程序\\n' '2018-03-19\\n' '计算机\\n' 'learnNLTKbyWatchVideo\\n' ' The following is learning from the video:NLTK with Python 3 for ' 'Natural Language Processing.You can watch the videos in ' 'YouTube,iliibili and the author’s website: pythonprogramming.net ' 'I use jupyte... 阅读全文… \\n' 'NLTK\\n' 'Python\\n' '自然语言处理\\n' '2018-03-13\\n' '线性代数与微积分浅解\\n' ' ' '以前在大一大二时曾学过高等数学（微积分）与线性代数，不过在当时都是被动的学一学，考个分数而已，同时教授一般也都是照本宣科的围绕理论展开，平淡无味的 ' '阅读全文… \\n' '2018-03-13\\n' '《自然语言处理综论》学习笔记\\n' ' Bill Manaris 关于自然语言处理的定义 阅读全文… \\n' '博客内容遵循 知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议\\n' '扫一扫，分享到微信\\n' '&#123;title&#125;\\n' ' &#123;tags&#125; \\n' '&#123;date&#125;\\n', 'message': '', 'title': \"lightsmile's Blog | lightsmile\"&#125; 内容转码123456action = 'ContentTranscode'# 内容转码params = &#123; 'url': 'www.iamlightsmile.com', 'to_html': 1&#125; 运行结果：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146https://wenzhi.api.qcloud.com/v2/index.php&#123;'code': 0, 'codeDesc': 'Success', 'content': '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE html PUBLIC ' '\"-//WAPFORUM//DTD XHTML Mobile 1.0//EN\" ' '\"http://www.wapforum.org/DTD/xhtml-mobile10.dtd\"&gt;&lt;html ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;head&gt;&lt;meta ' 'http-equiv=\"Content-Type\" content=\"application/xhtml+xml; ' 'charset=UTF-8\"/&gt;&lt;title&gt;lightsmile\\'s Blog | ' 'lightsmile&lt;/title&gt;&lt;style type=\"text/css\"&gt;* ' '&#123;margin:0;padding:0;&#125;body &#123;font-family: ' 'Arial,Helvetica,sans-serif;&#125;a &#123;cursor: pointer;text-decoration: ' 'underline;&#125;body, div, p, a, table, textarea, form, img, ol, ul, ' 'li, h1, h2, h3, h4, h5, h6 &#123;border:0 none;&#125;#tc_content ' '&#123;font-size: 16px;line-height: 25px;word-wrap: break-word;padding: ' '5px 6px;overflow: hidden;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div ' 'id=\"tc_content\"&gt;&lt;div class=\"fold_div\"&gt;&lt;a class=\"fold_a\" ' 'href=\"http://www.iamlightsmile.com##bk=1&amp;pg=1\"&gt;[展开]&amp;#160;1459679436@qq.com&amp;#160;&lt;/a&gt;&lt;/div&gt;lightsmile\\'s&amp;#160;Blog&amp;#160;&lt;br ' \"/&gt;lightsmile's&amp;#160;Blog&amp;#160;&amp;#160;lightsmile&amp;#160;&lt;br \" '/&gt;2018-04-18&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/18/GithubPages%E6%96%B0%E5%B0%9D%E8%AF%95/\" ' 'position=\"6\"&gt;GithubPages新尝试&lt;/a&gt;&amp;#160;由于某些不便明说的原因，我要再申请一个域名，再申请一个GitHub账号，再搞一个GitHub&amp;#160;Pages。打算用来记录一些不便明说的东西。\\n' '这次域名的申请不同于之前的在万网的iamlightsmile.com，这次是在腾讯云上申请的，不过都大同小异了。接着等待大概3天左右的实名认证，通过后域名就可以解析可用了。由于只是使用GitHub&amp;#160;Pages&amp;#160;作为静态网页，不需要另外购置服务器，所以也不...\\n' '&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/18/GithubPages%E6%96%B0%E5%B0%9D%E8%AF%95/\" ' 'position=\"6\"&gt;阅读全文…&lt;/a&gt; &lt;a ' 'href=\"http://www.iamlightsmile.com/tags/GitHub/\" ' 'position=\"6\"&gt;GitHub&lt;/a&gt; &lt;br /&gt;2018-04-17&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/17/%E5%AD%A6%E4%B9%A0Python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/\" ' 'position=\"6\"&gt;学习Python设计模式&lt;/a&gt; &lt;br ' '/&gt;&amp;#160;本书主要参阅的书籍是《精通Python设计模式》\\n' '本书分为创建型模式、结构型模式、行为型模式三大类，同时又细分为16种模式。具体到每个模式，则通过简单介绍、现实生活中例子、软件应用实例、应用场景、具体代码实现、小结几部分，多个角度加深对某个设计模式的理解。案例贴近生活，代码简单易懂，描述清晰明白，翻译水平上佳，确实算是我认为的好书，同时翻译还将代码上传到GitHub上方便读者下载学习，这里真...\\n' '&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/17/%E5%AD%A6%E4%B9%A0Python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/\" ' 'position=\"6\"&gt;阅读全文…&lt;/a&gt; &lt;br /&gt;2018-04-14&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/14/%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB%E4%B9%A6%E7%B1%8D/\" ' 'position=\"6\"&gt;推荐阅读书籍&lt;/a&gt; &lt;br /&gt;&amp;#160;此博文作为书籍阅读及相关的记录哲学篇\\n' '《和谐辩证法》\\n' '《智慧之根》\\n' '计算机篇\\n' '《统计学习理论基础》\\n' '《大数据智能》\\n' '《统计自然语言处理》（第二版）\\n' '《Python自然语言处理》（第2版，没有纸质）\\n' '基础篇\\n' '《线性代数及其应用》\\n' '历史篇小说篇\\n' '《天行健》\\n' '《英雄志》\\n' '《国士无双》\\n' '思维篇\\n' '《如何系统思考》\\n' '&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/14/%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB%E4%B9%A6%E7%B1%8D/\" ' 'position=\"6\"&gt;阅读全文…&lt;/a&gt; &lt;br /&gt;2018-04-07&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/07/%E5%93%88%E5%B7%A5%E5%A4%A7ltp%E5%B0%8F%E8%AF%95/\" ' 'position=\"6\"&gt;哈工大ltp小试&lt;/a&gt; &lt;br ' '/&gt;&amp;#160;今天开始探索学习使用哈工大的LTP（Language&amp;#160;Technology&amp;#160;Platform）。\\n' '这里是官网地址\\n' '这里是GitHub地址\\n' '这里是pyltp的使用文档\\n' '平台采用的语言是C++，但是也提供了Python和Java的封装。由于本人目前使用Python作为自然语言处理的工具语言，所以以下的探索流程都是使用本人电脑中的Window8.1操作系统的PyCharm集成开发环境，使用的Pyt...\\n' '&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/07/%E5%93%88%E5%B7%A5%E5%A4%A7ltp%E5%B0%8F%E8%AF%95/\" ' 'position=\"6\"&gt;阅读全文…&lt;/a&gt; &lt;br /&gt;&lt;a ' 'href=\"http://www.iamlightsmile.com/tags/Python/\" ' 'position=\"6\"&gt;Python&lt;/a&gt; &lt;a ' 'href=\"http://www.iamlightsmile.com/tags/ltp/\" ' 'position=\"6\"&gt;ltp&lt;/a&gt; &lt;a ' 'href=\"http://www.iamlightsmile.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/\" ' 'position=\"6\"&gt;自然语言处理&lt;/a&gt; &lt;br /&gt;2018-04-06&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/06/Scrapy%E7%88%AC%E5%8F%96%E7%9F%A5%E4%B9%8E%E6%95%B0%E6%8D%AE%E5%B0%8F%E8%AF%95/\" ' 'position=\"6\"&gt;Scrapy爬取知乎数据小试&lt;/a&gt; &lt;br /&gt;&amp;#160;啊啊啊，没时间写啦，以后有时间再写吧！\\n' '。。。发现今天是周五，不熄灯。。。\\n' '前两周一直在忙毕设的事情，由于某些原因毕设选择了相对简单的微信小程序，经过奋战之后一些主要的基本功能已经实现多半。\\n' '自然语言处理的一些最基本的概念已经有所了解，下面想要找点实战项目练练手。由于处理的第一步便是要获取语料，想着以后爬虫这东西肯定是要学的，于是从昨天开始学习相关视频、配置相关环境，今天看了部分，照着Dem...\\n' '&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/06/Scrapy%E7%88%AC%E5%8F%96%E7%9F%A5%E4%B9%8E%E6%95%B0%E6%8D%AE%E5%B0%8F%E8%AF%95/\" ' 'position=\"6\"&gt;阅读全文…&lt;/a&gt; &lt;br /&gt;&lt;a ' 'href=\"http://www.iamlightsmile.com/tags/Scrapy/\" ' 'position=\"6\"&gt;Scrapy&lt;/a&gt; &lt;a ' 'href=\"http://www.iamlightsmile.com/tags/%E7%88%AC%E8%99%AB/\" ' 'position=\"6\"&gt;爬虫&lt;/a&gt; &lt;br /&gt;2018-04-03&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/03/%E9%9A%8F%E6%83%B3/\" ' 'position=\"6\"&gt;随想&lt;/a&gt; &lt;br /&gt;&amp;#160;其实这世上哪有什么善恶，有的只是不同环境下不同的选择。\\n' '我发现人和人相识的过程基本上都是从他的经历中提取特质然后贴上标签的过程。\\n' '普天之下又有多少人敢把自己的灵魂放在阳光下炙烤呢？草他妈的！\\n' '&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/04/03/%E9%9A%8F%E6%83%B3/\" ' 'position=\"6\"&gt;阅读全文…&lt;/a&gt; &lt;br /&gt;2018-03-28&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/\" ' 'position=\"6\"&gt;计算机&lt;/a&gt; &lt;br /&gt;&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/03/28/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E6%8E%A2%E7%B4%A2%E9%9A%8F%E7%AC%94/\" ' 'position=\"6\"&gt;微信小程序的component&lt;/a&gt; &lt;br ' '/&gt;&amp;#160;我发现无法直接在样式即wxss里通过color属性设置icon组件的颜色，是无效的，只能通过在wxml里设置它的color属性为js传入的变量值或者是通过变量值来控制具体的颜色值。\\n' '我们可以将微信小程序中的components组件视为一个对象，没错，它本来就是一个对象，只是相对而言，它的初始化方法和设置方式不同于在一般的js语言中，它的data属性里是这个对象建立时初始化时的数据，作用域...\\n' '&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/03/28/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E6%8E%A2%E7%B4%A2%E9%9A%8F%E7%AC%94/\" ' 'position=\"6\"&gt;阅读全文…&lt;/a&gt; &lt;br /&gt;&lt;a ' 'href=\"http://www.iamlightsmile.com/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/\" ' 'position=\"6\"&gt;微信小程序&lt;/a&gt; &lt;br /&gt;2018-03-19&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/\" ' 'position=\"6\"&gt;计算机&lt;/a&gt; &lt;br /&gt;&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/03/19/learnNLTKbyWatchVideo/\" ' 'position=\"6\"&gt;learnNLTKbyWatchVideo&lt;/a&gt; &lt;br ' '/&gt;&amp;#160;The&amp;#160;following&amp;#160;is&amp;#160;learning&amp;#160;from&amp;#160;the&amp;#160;video:NLTK&amp;#160;with&amp;#160;Python&amp;#160;3&amp;#160;for&amp;#160;Natural&amp;#160;Language&amp;#160;Processing.You&amp;#160;can&amp;#160;watch&amp;#160;the&amp;#160;videos&amp;#160;in&amp;#160;YouTube,iliibili&amp;#160;and&amp;#160;the&amp;#160;author’s&amp;#160;website:&amp;#160;pythonprogramming.net\\n' 'I&amp;#160;use&amp;#160;jupyte...\\n' '&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/03/19/learnNLTKbyWatchVideo/\" ' 'position=\"6\"&gt;阅读全文…&lt;/a&gt; &lt;br /&gt;&lt;a ' 'href=\"http://www.iamlightsmile.com/tags/NLTK/\" ' 'position=\"6\"&gt;NLTK&lt;/a&gt; &lt;a ' 'href=\"http://www.iamlightsmile.com/tags/Python/\" ' 'position=\"6\"&gt;Python&lt;/a&gt; &lt;a ' 'href=\"http://www.iamlightsmile.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/\" ' 'position=\"6\"&gt;自然语言处理&lt;/a&gt; &lt;br /&gt;2018-03-13&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/03/13/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86%E6%B5%85%E8%A7%A3/\" ' 'position=\"6\"&gt;线性代数与微积分浅解&lt;/a&gt; &lt;br ' '/&gt;&amp;#160;以前在大一大二时曾学过高等数学（微积分）与线性代数，不过在当时都是被动的学一学，考个分数而已，同时教授一般也都是照本宣科的围绕理论展开，平淡无味的\\n' '&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/03/13/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86%E6%B5%85%E8%A7%A3/\" ' 'position=\"6\"&gt;阅读全文…&lt;/a&gt; &lt;br /&gt;2018-03-13&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/03/13/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%BB%BC%E8%AE%BA%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/\" ' 'position=\"6\"&gt;《自然语言处理综论》学习笔记&lt;/a&gt; &lt;br ' '/&gt;&amp;#160;Bill&amp;#160;Manaris&amp;#160;关于自然语言处理的定义\\n' '&lt;a ' 'href=\"http://www.iamlightsmile.com/2018/03/13/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%BB%BC%E8%AE%BA%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/\" ' 'position=\"6\"&gt;阅读全文…&lt;/a&gt; &lt;br /&gt;1&amp;#160;&lt;a ' 'href=\"http://www.iamlightsmile.com/page/2/\" ' 'position=\"786432\"&gt;2&lt;/a&gt; &lt;a ' 'href=\"http://www.iamlightsmile.com/page/3/\" ' 'position=\"786432\"&gt;3&lt;/a&gt; &lt;a ' 'href=\"http://www.iamlightsmile.com/page/4/\" ' 'position=\"786432\"&gt;4&lt;/a&gt; &lt;a ' 'href=\"http://www.iamlightsmile.com/page/2/\" ' 'position=\"786432\"&gt;下一页&lt;/a&gt; &lt;br /&gt;博客内容遵循&amp;#160;&lt;a ' 'href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh\" ' 'position=\"6\"&gt;知识共享&amp;#160;署名&amp;#160;-&amp;#160;非商业性&amp;#160;-&amp;#160;相同方式共享&amp;#160;4.0&amp;#160;国际协议&lt;/a&gt; ' '&lt;br /&gt;lightsmile&amp;#160;&amp;amp;copy;&amp;#160;2015&amp;#160;-&amp;#160;2018\\n' 'Power&amp;#160;by&amp;#160;&lt;a href=\"http://hexo.io/\" ' 'position=\"6\"&gt;Hexo&lt;/a&gt;&amp;#160;Theme&amp;#160;&lt;a ' 'href=\"https://github.com/yscoder/hexo-theme-indigo\" ' 'position=\"6\"&gt;indigo&lt;/a&gt; &lt;br /&gt;扫一扫，分享到微信&amp;#160;&lt;br /&gt;&lt;a ' 'href=\"http://www.iamlightsmile.com/%7Bpath%7D\" ' 'position=\"6\"&gt;&#123;title&#125;&#123;tags&#125;&#123;date&#125;&lt;/a&gt; &lt;br /&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;', 'message': ''&#125; 分词、词性标注、命名实体识别1234567action = 'LexicalAnalysis'# 分词、词性标注、命名实体识别params = &#123; 'text': \"我爱洗澡\", 'code': 0x00200000, #0x00200000表示utf-8 'type': 0 #取值 0 或 1，默认为 0。 0 为基础粒度版分词，倾向于将句子切分的更细，在搜索场景使用为佳。 1 为混合粒度版分词，倾向于保留更多基本短语不被切分开。&#125; 1234567891011121314151617181920https://wenzhi.api.qcloud.com/v2/index.php&#123;'code': 0, 'codeDesc': 'Success', 'combtokens': [&#123;'cls': '短语', 'pos': 0, 'wlen': '8', 'word': '我爱洗澡'&#125;], 'message': '', 'tokens': [&#123;'pos': 0, 'wlen': '2', 'word': '我', 'wtype': '代词', 'wtype_pos': 27&#125;, &#123;'pos': 2, 'wlen': '2', 'word': '爱', 'wtype': '动词', 'wtype_pos': 31&#125;, &#123;'pos': 4, 'wlen': '4', 'word': '洗澡', 'wtype': '动词', 'wtype_pos': 31&#125;]&#125; 文本纠错12345action = 'LexicalCheck' #文本纠错params = &#123; 'text': '人生苦短，我用Python！哼哼哈嘿！巴啦巴啦小魔仙！'&#125; 1234567https://wenzhi.api.qcloud.com/v2/index.php&#123;'code': 0, 'codeDesc': 'Success', 'conf': 1, 'message': '', 'text': '人生苦短，我用Python！哼哼哈嘿！巴啦巴啦小魔仙！', 'text_annotate': '人生苦短，我用Python！哼哼哈嘿！巴啦巴啦小魔仙！'&#125; 同义词12345action = 'LexicalSynonym'# 同义词params = &#123; 'text': '人生苦短，我用Python。我爱自然语言处理和知识图谱！'&#125; 12345678910111213https://wenzhi.api.qcloud.com/v2/index.php&#123;'code': 0, 'codeDesc': 'Success', 'message': '', 'query': None, 'syns': [&#123;'word_ori': &#123;'idx_beg': 7, 'idx_end': 8, 'text': '爱'&#125;, 'word_syns': [&#123;'conf': 0.32546776533127, 'text': '最爱'&#125;]&#125;, &#123;'word_ori': &#123;'idx_beg': 8, 'idx_end': 9, 'text': '自然'&#125;, 'word_syns': [&#123;'conf': 0.36934259533882, 'text': '大自然'&#125;]&#125;, &#123;'word_ori': &#123;'idx_beg': 11, 'idx_end': 12, 'text': '和'&#125;, 'word_syns': [&#123;'conf': 0.60000002384186, 'text': '与'&#125;]&#125;, &#123;'word_ori': &#123;'idx_beg': 13, 'idx_end': 14, 'text': '图谱'&#125;, 'word_syns': [&#123;'conf': 0.37899446487427, 'text': '图片'&#125;]&#125;]&#125; 文本分类12345action = 'TextClassify' #文本分类params = &#123; 'content':'人生苦短，please Python。太祖、刘邦、朱元璋哪个更厉害？！'&#125; 1234567https://wenzhi.api.qcloud.com/v2/index.php&#123;'classes': [&#123;'class': '文化', 'class_num': 61, 'conf': 0.713&#125;, &#123;'class': '历史', 'class_num': 95, 'conf': 0.221&#125;, &#123;'class': '未分类', 'class_num': 0, 'conf': 0.066&#125;], 'code': 0, 'codeDesc': 'Success', 'message': ''&#125; 句法分析12345action = 'TextDependency'# 句法分析params = &#123; 'content': '我爱自然语言处理'&#125; 123456789101112131415161718192021222324https://wenzhi.api.qcloud.com/v2/index.php&#123;'code': 0, 'codeDesc': 'Success', 'keywords': [[&#123;'dep_rel': 'SBV', 'father_id': 2, 'id': 1, 'postag': 'r', 'word': '我'&#125;, &#123;'dep_rel': 'HED', 'father_id': 0, 'id': 2, 'postag': 'v', 'word': '爱'&#125;, &#123;'dep_rel': 'VOB', 'father_id': 2, 'id': 3, 'postag': 'n', 'word': '自然语言'&#125;, &#123;'dep_rel': 'COO', 'father_id': 2, 'id': 4, 'postag': 'v', 'word': '处理'&#125;]], 'message': ''&#125; 关键词提取12345678action = 'TextKeywords' #关键词提取params = &#123; 'title': '自然语言处理', 'content': '''自然语言处理（英语：natural language processing，缩写作 NLP）是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。自然语言生成系统把计算机数据转化为自然语言。自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。'''&#125; 1234567https://wenzhi.api.qcloud.com/v2/index.php&#123;'code': 0, 'codeDesc': 'Success', 'keywords': [&#123;'keyword': '自然语言', 'score': 0.57486760616302, 'type': 'keyword'&#125;], 'message': ''&#125; 敏感信息识别123456action = 'TextSensitivity' #敏感信息识别params = &#123; 'content': \"中共统治！压迫，人民！续一秒！\", 'type': 2&#125; 123456https://wenzhi.api.qcloud.com/v2/index.php&#123;'code': 0, 'codeDesc': 'Success', 'message': '', 'nonsensitive': 0.37754066879815, 'sensitive': 0.62245933120185&#125;","categories":[],"tags":[]},{"title":"GithubPages新尝试","slug":"GithubPages新尝试","date":"2018-04-18T08:01:53.000Z","updated":"2019-01-07T03:12:43.689Z","comments":true,"path":"articles/GithubPages新尝试/","link":"","permalink":"http://www.iamlightsmile.com/articles/GithubPages新尝试/","excerpt":"由于某些不便明说的原因，我要再申请一个域名，再申请一个GitHub账号，再搞一个GitHub Pages。打算用来记录一些不便明说的东西。 这次域名的申请不同于之前的在万网的iamlightsmile.com，这次是在腾讯云上申请的，不过都大同小异了。接着等待大概3天左右的实名认证，通过后域名就可以解析可用了。由于只是使用GitHub Pages 作为静态网页，不需要另外购置服务器，所以也不需要备案的了，备案真是好屌麻烦！","text":"由于某些不便明说的原因，我要再申请一个域名，再申请一个GitHub账号，再搞一个GitHub Pages。打算用来记录一些不便明说的东西。 这次域名的申请不同于之前的在万网的iamlightsmile.com，这次是在腾讯云上申请的，不过都大同小异了。接着等待大概3天左右的实名认证，通过后域名就可以解析可用了。由于只是使用GitHub Pages 作为静态网页，不需要另外购置服务器，所以也不需要备案的了，备案真是好屌麻烦！ 刚开始还挺顺利的，输入网址后顺利输出了相关内容，由于这次同样想使用Hexo搭建博客平台，所以也将先搭起环境然后设置相关github.io的项目，不过到了设置ssh这里出现了问题，经过测试发现可以访问，但是用hexo deploy时则提示权限受限，后来在网上连找了好久关于ssh的相关内容，不过尝试后发现并不能解决问题，后来在GitHub的settings页面中，在SSH keys下面发现一段比较小的段落，有一个common SSH Problems的超链接，点进去之后研磨了好久终于发现问题所在。 原来是由于我用的自己原来的GitHub账号来更新新创建的GitHub账号仓库的内容，默认的权限不够，只能通过将smilelight添加到该项目的协作者或同一个组织下才行。邀请了smilelight之后发现没有反应，账号下没有通知，也没有邮件提醒，还以为是GitHub的问题，后来把那个邀请链接复制下来之后，发现是一个新的页面，需要点进去点击接受才可以，经过几番磨练之后终于提交成功。 至于为什么Hexo还是用我原来的GitHub账号我就不清楚了，虽然我在这个文件的.git的config中修改了自己的用户名和邮箱，但是身份用的还是smilelight，估计可能Hexo默认或者只能使用全局的Git账号信息吧。","categories":[],"tags":[{"name":"GitHub","slug":"GitHub","permalink":"http://www.iamlightsmile.com/tags/GitHub/"}]},{"title":"学习Python设计模式","slug":"学习Python设计模式","date":"2018-04-17T02:16:59.000Z","updated":"2019-01-07T03:15:16.594Z","comments":true,"path":"articles/学习Python设计模式/","link":"","permalink":"http://www.iamlightsmile.com/articles/学习Python设计模式/","excerpt":"本书主要参阅的书籍是《精通Python设计模式》 本书分为创建型模式、结构型模式、行为型模式三大类，同时又细分为16种模式。具体到每个模式，则通过简单介绍、现实生活中例子、软件应用实例、应用场景、具体代码实现、小结几部分，多个角度加深对某个设计模式的理解。案例贴近生活，代码简单易懂，描述清晰明白，翻译水平上佳，确实算是我认为的好书，同时翻译还将代码上传到GitHub上方便读者下载学习，这里真应该点个赞了！","text":"本书主要参阅的书籍是《精通Python设计模式》 本书分为创建型模式、结构型模式、行为型模式三大类，同时又细分为16种模式。具体到每个模式，则通过简单介绍、现实生活中例子、软件应用实例、应用场景、具体代码实现、小结几部分，多个角度加深对某个设计模式的理解。案例贴近生活，代码简单易懂，描述清晰明白，翻译水平上佳，确实算是我认为的好书，同时翻译还将代码上传到GitHub上方便读者下载学习，这里真应该点个赞了！ 我们在学习设计模式的时候不应当仅仅立足于软件开发这一角度，同时应该立足于实际，或者以更加抽象的角度来看待这些设计模式背后的思想。比如针对某个设计模式，我们要明白想通过它实现怎么样的功能，这样设计的好处在哪里，我要提供什么输入，我将得到什么输出，即通过函数或黑盒子的视角从外面看待这个设计模式。同时也要将视角放到该设计模式的内部，可以把具体的某个方法视为车间、某个对象视为工人，将之与现实世界某个应用场景映射起来，分析通过怎么样的一个调度实现了业务的功能，其中的结构优势何在，节省了空间还是时间上的资源等。 笔记摘抄 设计模式的本质是在已有的方案之上发现更好的方案（而不是全新发明）。 设计模式并非是某种高大上或者神秘的东西，而是一些常见的软件工程设计问题的最佳实践方案。 个人认为软件开发技术的学习都应该以实践为前提，只有理解实践过程中遇到的种种问题，才能明白那些技术的本质和目的是什么，因为每种新技术都是因某个/某些问题而出现的。 设计模式一般是描述如何组织代码和使用最佳实践来解决常见的设计问题。 书籍结构创建型模式创建型设计模式处理对象创建相关的问题，目标是当直接创建对象不太方便时，提供更好的方式。 工厂模式工厂背后的思想是简化对象的创建 通过将创建对象的代码和使用对象的代码解耦，工场能够降低应用维护的复杂度。 工厂通常有两种形式：一种是工厂方法，它是一个方法（或以地道的Python数据来说，是一个函数），对不同的输入参数返回不同的对象；第二种是抽象工厂，它是一组用于创建一系列相关事物对象的工厂方法。 工厂方法可以在必要时创建新的对象，从而提高性能和内存使用率。 一个抽象工厂是（逻辑上的）一组工厂方法，其中的每个工厂方法负责产生不同种类的对象。 抽象工厂模式是工厂方法模式的一种泛化。 通常一开始时使用工厂方法，因为它更简单。 工厂方法和抽象工厂设计模式可适用于以下场景： 想要追踪对象莱恩创建时 想要将对象的创建与使用解耦时 想要优化应用的性能和资源占用时 建造者模式建造者模式将一个复杂对象的构造过程与其表现分离，这样，同一个构造过程可用于创建多个不同的表现。 如果我们知道一个对象必须经过多个步骤来创建，并且要求同一个构造过程可以产生不同的表现，就可以使用建造者模式。 建造者模式可适用于以下场景： 想要创建一个复杂对象（对象由多个部分构成，且对象的创建要经过多个不同的步骤，这些步骤也许还需遵从特定的顺序） 要求一个对象能有不同的表现，并希望将对象的构造与表现解耦 想要在某个时间点创建对象，但在稍后的时间点再访问 原型模式原型设计模式用于创建对象的完全副本。 一般创建副本的两种方式： 当创建一个浅副本时，副本依赖引用 当创建一个深副本时，副本复制所有东西 第一种情况下，我们希望提升应用性能和优化内存使用，在对象之间引入数据共享，但需要小心地修改数据，因为所有变更对所有副本都是可见的。 第二种情况下，我们希望能够对一个副本进行更改而不会影响其他对象。 结构型模式结构型设计模式处理一个系统中不同实体（比如，类和对象）之间的关系，关注的是提供一种简单的对象组合方式来创建新功能。 适配器模式适配器模式帮助我们实现两个不兼容接口之间的兼容 开放/封闭原则（open/close principle）是面向对象设计的基本原则之一，声明一个软件实体应该对拓展是开放的，对修改则是封闭的。本质上这意味着我们应该无需修改一个软件实体的源码就能拓展其行为。适配器模式遵从开放/封闭原则。 适配器让一件产品在制造出来之后需要应对新需求之时还能工作。 修饰器模式修饰器模式通常用于拓展一个对象的功能。 一般来说，应用中有些部件是通用的，可应用于其他部件，这样的部件被看作横切关注点。 修饰器模式是实现横切关注点的绝佳方案，因为横切关注点通用但不太适合使用面向对象编程范式来实现。 Python进一步拓展了修饰器的概念，允许我们无需使用继承或组合就能拓展任意可调用对象（函数、方法或类）的行为。 外观模式外观模式有助于隐藏系统所的内部复杂性，并通过一个简化的接口向客户端暴露必要的部分。本质上，外观（Facade）是在已有复杂系统之上实现的一个抽象层。 使用外观模式的最常见理由是为一个复杂系统提供单个简单的入口点。 不把系统的内部功能暴露给客户端代码有一个额外的好处：我们可以改变系统内部代码，但客户端代码不用关心这个改变，也不会受到这个改变的影响。客户端代码不需要进行任何改变。 如果你的系统包含多层，外观模式也能派上用场。你可以为每一层引入一个外观入口点，并让所有层级通过它们的外观相关通信。这提高了层级之间的松耦合性，尽可能保持层级独立。 外观模式是一种隐藏系统复杂性的优雅方式，因为多数情况下客户端代码并不应该关心系统的这些细节。 享元模式作为软件工程师，我们应该编写更好的软件来解决软件问题，而不是要求客户购买更多更好的硬件。 享元模式通过为相似对象引入数据共享来最小化内存使用，提升性能。 一个享元（Flyweight）就是一个包含状态独立的不可变（又称固有的）数据的共享对象。 若想要享元模式有效，需要满足GoF的《设计模式》一书罗列的以下几个条件。 应用需要使用大量的对象。 对象太多，存储/渲染它们的代价太大。 对象ID对于应用不重要。 一般来说，在应用需要创建大量的计算代价大但共享许多属性的对象时，可以使用享元。 模型-视图-控制器模式关注点分离（Separation of Concerns，SoC）原则是软件工程相关的设计原则之一。 SoC原则背后的思想是将一个应用切分成不同的部分，每个部分解决一个单独的关注点。 模型-视图-控制器（Model-View-Controller，MVC）模式是应用到面向对象编程的SOC原则。 其中，模型是核心的部分，代表着应用的信息本源，包含和管理（业务）逻辑、数据、状态以及应用的规则。视图是模型的可视化表现。视图只是展示数据，并不处理数据。控制器是模型与视图之间的链接/粘附。模型与视图之间的所有通信都通过控制器进行。 为了实现模型与其表现之间的解耦，每个视图通常都需要属于它的控制器。（我认为作者想表达的是没有控制器也可以，但是由于视图并不具备处理数据的功能，并且通常每个页面所要处理的业务都会不同，因此这样的模型是高度定制的模型，适用性很差） 在从头开始实现MVC时，请确保创建的模型很智能，控制器很瘦，视图很傻瓜。 可以将具有以下功能的模型视为智能模型： 包含所有的校验/业务规则/逻辑 处理应用的状态 访问应用数据（数据库、云或其他） 不依赖UI可以将符合以下条件的控制器视为瘦控制器： 在用户与视图交互时，更新模型 在模型改变时，更新视图 如果需要，在数据传递给模型/视图之前进行处理 不展示数据 不直接访问应用数据 不包含校验/业务规则/逻辑可以将符合以下条件的视图视为傻瓜视图： 展示数据 允许用户与其交互 仅做最小的数据处理，通常由一种模板语言提供处理能力（例如，使用简单的变量和循环控制） 不存储任何数据 不直接访问应用数据 不包含校验/业务规则/逻辑 使用MVC时，请确保创建智能的模型（核心功能）、瘦控制器（实现视图与模型之间通信的能力）以及傻瓜式的视图（外在表现，最小化逻辑处理） 代理模式四种不同的知名代理类型： 远程代理：实际存在于不同地址空间（例如，某个网络服务器）的对象在本地的代理者。 虚拟代理：用于懒初始化，将一个大计算量对象的创建延迟到真正需要的时候进行。 保护/防护代理：控制对敏感对象的访问。 智能（引用）代理：在对象被访问时执行额外的动作。此类代理的例子包括引用计数和线程安全检查。 现实中，，永远不要执行以下操作： 在源代码中存储密码 以明文形式存储密码 使用一种弱（例如，MD5）或自定义加密形式 行为型模式行为型模式处理对象互联和算法的问题 责任链模式责任链（Chain of Responsibility）模式用于让多个对象来处理单个请求时，或者用于预先不知道应该由哪个对象（来自某个对象链）来处理某个特定请求时。其原则如下所示： 存在一个对象链（链表、树或任何其他便捷的数据结构）。 我们一开始将请求发送给链中的第一个对象。 对象决定其是否要处理该请求。 对象将请求转发给下一个对象。 重复该过程，直到到达链尾。 通过使用责任链模式，我们能让许多对象来处理一个特定请求。在我们预先不知道应该由哪个对象来处理某个请求时，这是有用的。另一个责任链可以派上用场的场景是，在我们知道可能会有多个对象都需要对同一个请求进行处理之时。 这一模式的价值在于解耦。 客户端与所有处理程序（一个处理程序与所有其他处理程序之间也是如此）之间不再是多对多关系，客户端仅需要知道如何与链的起始节点（标头）进行通信。 在无法预先知道处理程序的数量和类型时，该模式有助于对请求/处理事件进行建模。适合使用责任链模式的系统例子包括基于事件的系统、采购系统和运输系统。 命令模式命令设计模式帮助我们将一个操作（撤销、重做、复制、粘贴）封装成一个对象。简而言之，这意味着创建一个类，包含实现该操作所需要的所有逻辑和方法。 这样做的优势如下所述参考： 我们并不需要直接执行一个命令。命令可以按照希望执行。 调用命令的对象与知道如何执行命令的对象解耦。调用者无需知道命令的任何实现细节。 如果有意义，可以把多个命令组织起来，这样调用者能够按顺序执行它们。例如，在实现一个多层撤销命令时，这是很有用的。 解释器模式解释器（Interpreter）模式仅能引起应用的高级用户的兴趣。这是因为解释器模式背后的贮存思想是让非初级用户和领域专家使用一门简单的语言来表达想法。 领域特定语言（Domain Specific Language，DSL）是一种针对一个特定领域的有限表达能力的计算机语言。DSL分为内部DSL和外部DSL。 内部DSL构建在一个宿主编程语言之上。 优势：我们不必担心创建、编译及解析语法，因为这些已经被宿主语言解决掉了。 劣势：会受限于宿主语言的特性。如果宿主语言不具备这些特性，构建一种表达能力强、简洁而且优美的内部DSL是富有挑战性的。 外部DSL不依赖某种宿主语言。DSL的创建者可以决定语言的方方面面（语法、句法等），但也要负责为其创建一个解析器和编译器。为一种新语言创建解析器和编译器是一个非常复杂、长期而又痛苦的过程。 解释器模式仅与内部DSL相关。 观察者模式观察者模式描述单个对象（发布者，又称为主持者或可观察者）与一个或多个对象（订阅者，又称为观察者）之间的发布-订阅关系。 观察者模式背后的思想等同于MVC和关注点分离原则背后的思想，即降低发布者与订阅者之间的耦合度，从而易于在运行时添加/删除订阅者。此外，发布者不关心它的订阅者是谁。它只是将通知发送给所有订阅者。 当我们希望在一个对象（主持者/发布者/可观察者）发生变化时通知/更新另一个或多个对象的时候，通常会使用观察者模式。观察者的数量以及谁是观察者可能会有所不同，也可以（在运行时）动态地改变。 状态模式在很多问题中，有限状态机（通常名为有限状态机）是一个非常方便的状态转换建模（并在必要时以数学方式形式化）工具。 状态机是一个抽象机器，有两个关键部分，状态和转换。状态是指系统的当前（激活）状况。一个状态机在任意时间点只会有一个激活状态。转换是指从一个状态切换到另一个状态，因某个事件或条件的触发而开始。在一个转换发生之前或之后通常会执行一个或多个动作。 状态机带一个不错的特性是可以用图来表现（称为状态图），其中每个状态都是一个节点，每个转换都是两个节点之间的边。 状态模式就是应用到一个特定软件工程问题的状态机。 使用状态模式本质上相当于实现一个状态机来解决特定领域的一个软件问题。 状态设计模式解决的是一定上下文中无限数量状态的完全封装，从而实现更好的可维护性和灵活性。 策略模式策略模式通常用在我们希望对同一个问题透明地使用多种方案时。如果并不存在针对所有输入数据和所有情况的完美算法，那么我们可以使用策略模式，动态地决定在每种情况下应使用哪种算法。 模板模式编写优秀代码的一个要素是避免冗余。 模板模式关注的是消除代码冗余，其思想是我们应该无需改变算法结构就能重新定义一个算法的某个部分。 设计模式是被发现，而不是被发明出来的。","categories":[],"tags":[]},{"title":"哈工大ltp小试","slug":"哈工大ltp小试","date":"2018-04-07T15:28:35.000Z","updated":"2019-01-07T03:14:54.552Z","comments":true,"path":"articles/哈工大ltp小试/","link":"","permalink":"http://www.iamlightsmile.com/articles/哈工大ltp小试/","excerpt":"今天开始探索学习使用哈工大的LTP（Language Technology Platform）。 这里是官网地址 这里是GitHub地址 这里是pyltp的使用文档 平台采用的语言是C++，但是也提供了Python和Java的封装。由于本人目前使用Python作为自然语言处理的工具语言，所以以下的探索流程都是使用本人电脑中的Window8.1操作系统的PyCharm集成开发环境，使用的Python版本是3.6。","text":"今天开始探索学习使用哈工大的LTP（Language Technology Platform）。 这里是官网地址 这里是GitHub地址 这里是pyltp的使用文档 平台采用的语言是C++，但是也提供了Python和Java的封装。由于本人目前使用Python作为自然语言处理的工具语言，所以以下的探索流程都是使用本人电脑中的Window8.1操作系统的PyCharm集成开发环境，使用的Python版本是3.6。 使用流程很简单： 下载最新版本的模型（目前是3.4） 安装pyltp，在命令行输入指令：pip install pyltp。不过我的一直安装失败，总是不成功，后来从网上找了pyltp 3.6的whl文件，然后通过pip install pyltp-0.2.1-cp36-cp36m-win_amd64.whl成功了。 这里有一篇博客专门讲安装的，发现他的界面有点黑客风，挺炫酷的哈~ 通过PyCharm新建Python项目后，整个工程长成这个样子： 不过其中的data文件夹、source文件夹和test文件夹是自己新创建的，其中data文件夹内放置下载的模型，source文件夹放置那个其实也没啥东西的txt文件，test中放置编写的Python代码。 照着使用文档敲的testltp.py内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import pyltp# 分句from pyltp import SentenceSplittersents = SentenceSplitter.split('元芳你怎么看？我就趴窗口上看呗！')print('\\n'.join(sents))# 分词from pyltp import Segmentorsegmentor = Segmentor()segmentor.load('../data/cws.model')words = segmentor.segment(\"元芳你怎么看\")segmentor.release()print(list(words))# 词性标注from pyltp import Postaggerpostagger = Postagger()postagger.load('../data/pos.model')posts = postagger.postag(list(words))postagger.release()print(list(zip(list(words),list(posts))))#命名实体识别from pyltp import NamedEntityRecognizerrecognizer = NamedEntityRecognizer()recognizer.load('../data/ner.model')nettags = recognizer.recognize(list(words),list(posts))recognizer.release()print(list(zip(list(words),list(nettags))))#依存语法分析from pyltp import Parserparer = Parser()parer.load('../data/parser.model')arcs = parer.parse(list(words),list(posts))parer.release()print(list(zip(list(words),[(arc.head,arc.relation) for arc in arcs])))#语义角色标注from pyltp import SementicRoleLabellerlabeller = SementicRoleLabeller()labeller.load('../data/pisrl_win.model')roles = labeller.label(list(words),list(posts),arcs)labeller.release()for role in roles: print(role.index, \"\".join( [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments])) 其中我的pyltp不知道怎么安装的，竟然成了build-in里的东西了，而不是在site-packages里面。 执行以上代码的结果为： 其中要注意的是这里在语义角色标注中使用的并非pisrl.model而是pisrl_win.model，前者会报错的。 同时由于文档中提到： 于是我又去申请了哈工大的语言云个人账号,经过邮箱激活之后，按照人家的使用文档就要探索一下，不过没想到出现了问题。。。 由于官网提供的Python示例代码使用的是2.7版本，所以这里我使用的是另外的一个网络请求模块：requests，最终的代码testltpCloud.py长成这样： 1234567891011121314import requestsfrom settings import APIKEYurl_get_base = \"http://api.ltp-cloud.com/analysis/\"args = &#123; 'api_key': APIKEY, 'text': '我是中国人。', 'pattern': 'dp', 'format': 'plain'&#125;# result = urllib.request.urlopen(url_get_base, urllib.parse.urlencode(args)) # POST methodresult = requests.post(url_get_base,args)print(result.text) 在同路径下的settings.py的内容为： 其中APIKEY的内容是邮件发给你的api_key字符串。 不过运行的结果为： 显示未授权用户，但是我的账户类型是免费的，刚注册的啊，为啥这样，搜了搜没搜出个啥，就先这样吧。 又向目标迈进了一步，嘿嘿，加油！","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://www.iamlightsmile.com/tags/Python/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://www.iamlightsmile.com/tags/自然语言处理/"},{"name":"ltp","slug":"ltp","permalink":"http://www.iamlightsmile.com/tags/ltp/"}]},{"title":"Scrapy爬取知乎数据小试","slug":"Scrapy爬取知乎数据小试","date":"2018-04-06T15:06:27.000Z","updated":"2019-01-07T03:14:07.928Z","comments":true,"path":"articles/Scrapy爬取知乎数据小试/","link":"","permalink":"http://www.iamlightsmile.com/articles/Scrapy爬取知乎数据小试/","excerpt":"啊啊啊，没时间写啦，以后有时间再写吧！ 。。。发现今天是周五，不熄灯。。。 前两周一直在忙毕设的事情，由于某些原因毕设选择了相对简单的微信小程序，经过奋战之后一些主要的基本功能已经实现多半。 自然语言处理的一些最基本的概念已经有所了解，下面想要找点实战项目练练手。由于处理的第一步便是要获取语料，想着以后爬虫这东西肯定是要学的，于是从昨天开始学习相关视频、配置相关环境，今天看了部分，照着Demo练了练小手。","text":"啊啊啊，没时间写啦，以后有时间再写吧！ 。。。发现今天是周五，不熄灯。。。 前两周一直在忙毕设的事情，由于某些原因毕设选择了相对简单的微信小程序，经过奋战之后一些主要的基本功能已经实现多半。 自然语言处理的一些最基本的概念已经有所了解，下面想要找点实战项目练练手。由于处理的第一步便是要获取语料，想着以后爬虫这东西肯定是要学的，于是从昨天开始学习相关视频、配置相关环境，今天看了部分，照着Demo练了练小手。 B站真是个好地方，上面有不少免费的好的视频可以看，虽然版权这方面%&amp;@#￥！@￥……@#￥%……#@￥%##￥@ 这是学习爬虫的视频链接 作者是拿轮子哥vczh作为start_user的，当时还愣了一下，可以的，会玩，想当年自己也关注过轮子哥一段时间，不过看他经常给美女们点赞、抖机灵，后来便取关了。 废话少说，言归正传： 爬虫：请求网站并提取数据的自动化程序。 爬虫的基本流程： 发起请求：通过HTTP库向目标站点发起请求,即发送一个Request,情况请可以包含额外的headers等信息,等待服务器响应。 获取响应内容：如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，内容可能有HTML，Json字符串，二进制数据（如图片视频）等类型。 解析内容：得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析，可呢是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。 保存数据：保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。 项目所实现的是从首个著名知乎用户（本项目中为vczh）个人信息及其所有关注人、所有粉丝相关信息爬取开始、一直延伸整个关注网，并将结果数据集保存在MongoDB中。 具体来说就是先爬轮子哥的个人信息数据，然后依次爬取他的所有关注人的个人信息以及他的所有粉丝们的个人信息，这样的策略应用到每一个爬虫经过的用户上，从而实现数据的遍历抽取。单纯从Python代码的角度上讲，类似于会重复的深度优先遍历，然而具体的Scrapy引擎内部会如何调度这些Request队列就是人家内部的算法了。 视频的上传时间是18年1月11日，当时从网页中获取到的用户信息相对比较简单、集中、丰富，今天我试了又试，发现可获取到的直接信息变少了。由于只是初步尝试，所以也就按部就班的照样执行，没有做得不偿失的优化了。 Scrapy引擎的框架大致如下： 我们可以看到整个引擎主要是由四部分组成： Scheduler（调度器） Downloader（下载器） Spiders（爬虫） Item Pipeline（项目 管道） 其中Spider中定义了具体的爬虫逻辑，比如我们要怎么爬，爬什么，后面跟着的s表明这通常不是一个Spider，而是通常有多个Spider，我们可以根据不同的具体需求编写对应的Spider。 Spiders之上的是一些Spider Middlewares（即爬虫中间件），有点类似于Python中函数的修饰器，可以对函数进行一些增强和拓展，同样的，我们可以通过这些Spider中间件来丰富和拓展我们编写的小Spider，让它们表现的更给力一些，同时这也可以简化我们的编写逻辑，因为我们只需要在之上套些中间件就可以了，套什么中间件视具体需求而定，比如说冬天穿大衣、夏天穿衬衫等。 当Spider生成好之后，引擎会开始执行这个Spider的内部逻辑，如start_requests方法，和parse方法等等，具体的它会将HTTP的Request请求交给Downloader完成，由Downloader完成从Internet上下载资源数据的具体任务，而Downloader也可以有自己的中间件也就是Downloader Middlewares，可以对Downloader进行改装，增强。 Downloader完成下载后，Scrapy引擎会将Downloader的成果Responses交给之前的Spider，执行它的解析方法。之后视具体情况，Spider可能会爬取更多的数据，相应的会产生更多的Request请求，或者将Response中的数据进行处理，处理为Item，然后转交给Item Pipeline做最后的数据处理工作。 因为爬虫一般不是一个一个的爬，而是通常成百上千乃至上万的爬，Scheduler的主要作用类似于CPU的处理器对这些请求做一个规划调度，先做哪个，后做哪个等等。 Item Pipline 中，Item可以视为一个数据对象的容器，而Pipline则类似Unix系统中的管道，或者更通俗点，就像流水线的的工人，从网上获取原材料之后，Spider这个工人进行加工处理，之后这些Pipline们做做类似贴标签的工作，最终提交正式的产品。 因为我们爬虫的编写都是具有针对性的，知己知彼百战不殆嘛。所以首先要分析知乎相关的数据流通状况： 以上是轮子哥的知乎页面。 首先必须要按F12打开开发者工具，查看Network页面。 对于某个用户的具体信息，如关注人或粉丝列表中的，我们只需要将鼠标请放在某个人的图像上，知乎就会通过Ajax去请求这个人的数据，以Json对象的格式返回给浏览器，同样的，当我们查看关注人列表和粉丝列表时，知乎也是通过Ajax请求返回这些数据对象的，具体的如以下几张图片： 这是轮子哥关注的某个人的信息，观看图右侧我们发现这个人相关信息就在这个Json对象中。而如果要获取到这个Response体中的Json对象，我们只要执行最上面的网络请求就可以了： 还有类似的关注人列表和粉丝列表也都是大概类似的情况，不过情况的url中的参数和内容稍有不同罢了。 关注人列表： 相关请求： 粉丝列表： 相关请求： 其实爬虫这个东西的基本原理很简单，就是执行HTTP请求，处理响应的数据，将这个过程重复化自动化而已。 而基于前面我们所提到的爬取信息的相关策略，我们要做的就是爬轮子哥的数据，然后请求两个列表中其他人的数据，并拓展开来。 整个项目的结构如下： 这里我们需要编写的是zhihu.py、items.py、piplines.py、settings.py 在settings.py中我们进行了请求头、Item Pipline中间件和MongoDB相关的配置。 如下： 1234567891011121314DEFAULT_REQUEST_HEADERS = &#123; 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; W…) Gecko/20100101 Firefox/57.0', 'authorization':' oauth c3cef7c66a1843f8b3a9e6a1e3160e20'&#125;ITEM_PIPELINES = &#123; 'zhihuUser.pipelines.MongoPipeline': 300,&#125;MONGO_URI = 'localhost'MONGO_DATABASE = 'zhihu' 我们针对Json对象的内容编写的ZhihuUserItem对象： 123456789101112131415161718192021222324252627from scrapy import Item,Fieldclass ZhihuUserItem(Item): # define the fields for your item here like: # name = scrapy.Field() id = Field() is_followed = Field() avatar_url_template = Field() user_type = Field() answer_count = Field() is_following = Field() url = Field() url_token = Field() allow_message = Field() articles_count = Field() is_blocking = Field() name = Field() headline = Field() badge = Field() is_advertiser = Field() avatar_url = Field() is_org = Field() gender = Field() follower_count = Field() employments = Field() type = Field() 因为要将数据存储到MongoDB中，所以要进行MongoDB的Item Pipline中间件的编写： 12345678910111213141516171819202122232425262728import pymongoclass MongoPipeline(object): collection_name = 'user_info' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].insert_one(dict(item)) # self.db[self.collection_name].update(&#123;'url_token': item['url_token']&#125;,&#123;'$set': item&#125;,True) 最后是zhihu.py 中 ZhihuSpider的编写： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import jsonimport scrapyfrom scrapy import Requestfrom zhihuUser.items import ZhihuUserItemclass ZhihuSpider(scrapy.Spider): name = 'zhihu' allowed_domains = ['www.zhihu.com'] start_urls = ['http://www.zhihu.com/'] start_user = 'excited-vczh' user_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;' user_query = 'allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics' followees_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;' followees_query = 'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics' followers_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followers?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;' followers_query = 'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics' def start_requests(self): yield Request(self.user_url.format(user=self.start_user,include=self.user_query),callback=self.parse_user) # yield Request(self.followees_url.format(user=self.start_user,include=self.followees_query,offset=0,limit=20),callback=self.parse_followees) def parse_user(self, response): result = json.loads(response.text) item = ZhihuUserItem() for field in item.fields: if field in result.keys(): item[field] = result.get(field) yield item yield Request(self.followees_url.format(user=result.get('url_token'), include=self.followees_query, offset=0, limit=20), callback=self.parse_followees) yield Request( self.followers_url.format(user=result.get('url_token'), include=self.followers_query, offset=0, limit=20), callback=self.parse_followees) # print(json.loads(response.text)) def parse_followees(self, response): result = json.loads(response.text) if 'data' in result.keys(): for result in result.get('data'): yield Request(self.user_url.format(user=result.get('url_token'),include=self.user_query),callback=self.parse_user) if 'paging' in result.keys() and result.get('paging').get('is_end') == False: next_page = result.get('paging').get('next') yield Request(next_page,callback=self.parse_followees) def parse_followers(self, response): result = json.loads(response.text) if 'data' in result.keys(): for result in result.get('data'): yield Request(self.user_url.format(user=result.get('url_token'),include=self.user_query),callback=self.parse_user) if 'paging' in result.keys() and result.get('paging').get('is_end') == False: next_page = result.get('paging').get('next') yield Request(next_page,callback=self.parse_followers) def parse(self, response): pass 在终端下执行scrapy crawl zhihu后，爬虫便会开始启动，由于这个工程一直爬一直爬，所以让它爬一会做个样子就行了，通过Ctrl+C停止当前任务，随后我们可以通过可视化工具查看到存入MongoDB中的数据： 大功告成！虽然超级简单。。。","categories":[],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://www.iamlightsmile.com/tags/爬虫/"},{"name":"Scrapy","slug":"Scrapy","permalink":"http://www.iamlightsmile.com/tags/Scrapy/"}]},{"title":"微信小程序的component","slug":"微信小程序探索随笔","date":"2018-03-28T09:14:22.000Z","updated":"2019-01-07T03:15:46.273Z","comments":true,"path":"articles/微信小程序探索随笔/","link":"","permalink":"http://www.iamlightsmile.com/articles/微信小程序探索随笔/","excerpt":"我发现无法直接在样式即wxss里通过color属性设置icon组件的颜色，是无效的，只能通过在wxml里设置它的color属性为js传入的变量值或者是通过变量值来控制具体的颜色值。","text":"我发现无法直接在样式即wxss里通过color属性设置icon组件的颜色，是无效的，只能通过在wxml里设置它的color属性为js传入的变量值或者是通过变量值来控制具体的颜色值。 我们可以将微信小程序中的components组件视为一个对象，没错，它本来就是一个对象，只是相对而言，它的初始化方法和设置方式不同于在一般的js语言中，它的data属性里是这个对象建立时初始化时的数据，作用域和生命周期伴随着component对象实例，而properties属性效果类似，均可以在component对象内部的函数和方法中使用this.data获取到，只是相对而言，data的数据是组件内部的数据，它是属于组件本身的属性，从设计上讲不取决于外部的应用场景；而properties属性则是暴露在组件外部的属性，它的作用相当于一般的编程语言中我们在new一个对象时做的初始化工作如new People(name=”lightsmile”,sex = false)，也就是说组件的一些业务属性是要通过这些属性接口来实现的，它是根据场景所制订的，具体实例体现在如： 1&lt;order-by bindtap=\"handOrderTap\" id=\"fuck\" test=\"&#123;&#123;5&#125;&#125;\" data-tes=\"&#123;&#123;test&#125;&#125;\" data-fuck=\"fuck\" data-order=\"&#123;&#123;orders&#125;&#125;\" data-&gt;&lt;/order-by&gt; 中id是每个标签都具有的值，而以“data-”开头的数据都是这个页面中，暴露给触发事件的值，在事件处理函数中，可以通过如 1234567handOrderTap(e) &#123; console.log(e) console.log(e.currentTarget.id) this.fuck = this.selectComponent(\"#fuck\") console.log(this.fuck.data) console.log(this.fuck.dataset)&#125; 获取到e（event）事件的具体信息，这里的handOrderTag只是自定义的函数，e.target和e.currentTarget分别代表不同的对象。 其中target是指事件的原触发对象，而currentTarget是指当前事件的触发对象，这是与事件的冒泡捕获机制相关的。 而不以“data-”开头，也不是如class、id、style等其他的属性如在上例中属性名为test的是页面传递给组件对象的信息，这里的test对应着组件对象之前设定的test属性，即在2中提到的暴露的属性名 而“=”号后面的“”和“”则均是属于当前页面的逻辑层Page对象内部的数据，即this.data.test和this.tata.orders（其中的this指代的是当前对象），也就是说，在与Component组件或页面Page对象对应的wxml中，其中的永远都是指的当前组件或页面对象，无法外指和内值，即在Component对应wxml中无法引用外部Page的对象（这个很合理，因为组件本来就是要被复用的，不应该出现还可以引用外部的数据的情况），Page对应的wxml中也无法引用内部Component对象的内部属性，而如果要使用内部的值，一种方法是内部定义触发的方法然后再使用如this.triggerEvent(‘change’, this)触发外部的change事件，这样组件外部就可以使用bindchange=”方法名”进行handle处理了。如下几个阶段： 1. 在Page的wxml中使用组件order-by,绑定了自定义事件MyEvent，这样在MyEvent事件被触发时，Page对象的handleMyEvent方法就会被执行。 2. handleMyEvent方法的内容如下： 3.Component的wxml中的text组件绑定tap事件到Component对象的myEvent方法 4.myEvent方法的内容如下： 因此整个的流程应当为：当我点击text文本的时候，会触发tap事件，这样它绑定的myEvent方法会被执行，然后方法内部又会主动引发MyEvent事件，这样在Page页面对其绑定的handleMyEvent方法会被执行，而该方法定义在Page对象内部。（注意不要搞混事件和方法，虽然我这里的名字比较混乱）控制台输出的结果为： 即先触发tap事件，再触发MyEvent自定义事件，下面我们来看一看内部传递的东西： 从中我们可以发现第一个事件e的currentTarget属性的dataset属性是一个空对象，对应着我们并没有在wxml的text组件内部填写“data-xx”属性，而第二个事件e的currentTarget属性的dataset属性是一个包含了fuck、order、tes三个属性的对象，尽管其中order和tes的内容为空（因为对应的数据在Page对象中没有，我错误的写成了Component对象内部的了，发现获取不到，这才有了这篇文章）。 因此一般情况下，我们可以通过这种事件的方式来实现数据的传递工作，并且 this.triggerEvent() 方法接收自定义事件名称外，还接收两个对象，eventDetail 和 eventOptions。这也就是说，我们完全可以不传递this，而传递任意自己定义的对象数据，比如在myEvent中我可以不传this，接着传e，我也可以定义只与业务相关的数据对象来处理，由于方法定义在页面或组件对象内部，可以访问内部数据，而通过事件传递后可以通过参数访问数据，以此就实现了组件向页面的数据传递工作。 当然，这样的一个特点是事件绑定是放在视图view层，而事件处理传递是放在逻辑js层，可能在做一些其他业务时还是需要相应的业务转化工作才可以，不够直接和方便，因为页面还是无法做到直接访问组件数据。 后来发现果然页面提供了这么一个方法：this.selectComponent(“#fuck”)可以在页面js中使用selectComponent选择某个component组件对象的实例，在此之上可以继续访问到它的data属性、dataset属性和properties属性，分别对应的是组件的properties属性、data属性和properties属性，如下图所示： 这里我们将组件的id设为fuck，handleOrderTap方法的内容如下： 打印的内容为： 我们可以发现它的id是自己定义的fuck,而is是项目的绝对路径，还有上面提到的data、properties、dataset属性（显而易见，properties和data属性指向同一个属性对象）（然而，经过测试发现虽然内容相同，不过并非指向同一个对象，如下图所示：真是奇了怪了。。。）。 如果打开proto属性，会发现更多的东西，比如说方法： 通过this.fuck.loghaha()即可完成对方法的调用，当然了，这里获取完全可以写成let fuck = this.selectComponent(“#fuck”)，然后通过fuck.loghaha()来调用，这里简单沿袭了网上搜的文章的实例。以上，也算自己学习的小经历总结吧。","categories":[{"name":"计算机","slug":"计算机","permalink":"http://www.iamlightsmile.com/categories/计算机/"}],"tags":[{"name":"微信小程序","slug":"微信小程序","permalink":"http://www.iamlightsmile.com/tags/微信小程序/"}]},{"title":"learnNLTKbyWatchVideo","slug":"learnNLTKbyWatchVideo","date":"2018-03-19T09:28:40.000Z","updated":"2019-01-07T03:13:26.417Z","comments":true,"path":"articles/learnNLTKbyWatchVideo/","link":"","permalink":"http://www.iamlightsmile.com/articles/learnNLTKbyWatchVideo/","excerpt":"The following is learning from the video:NLTK with Python 3 for Natural Language Processing.You can watch the videos in YouTube,iliibili and the author’s website: pythonprogramming.net I use jupyter notebook to write and run the python code,the python version is 3.4.4. Frist,we need to import the nltk module to use it","text":"The following is learning from the video:NLTK with Python 3 for Natural Language Processing.You can watch the videos in YouTube,iliibili and the author’s website: pythonprogramming.net I use jupyter notebook to write and run the python code,the python version is 3.4.4. Frist,we need to import the nltk module to use it 1234import nltkfrom nltk.tokenize import sent_tokenize,word_tokenizetext = r\"hello,how are you! I am lightsmile. My github link is www.github.com/smilelight. My persoanl website is www.iamlightsmile.com\" 1. Tokenizing words and entences(分词和分句)use the sent_tokenize method to tokenize the texts to sentenses(分句) 1sent_tokenize(text) [&#39;hello,how are you!&#39;, &#39;I am lightsmile.&#39;, &#39;My github link is www.github.com/smilelight.&#39;, &#39;My persoanl website is www.iamlightsmile.com&#39;] use the word_tokenize method to tokenize the texts to words(分词) 1word_tokenize(text) [&#39;hello&#39;, &#39;,&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;!&#39;, &#39;I&#39;, &#39;am&#39;, &#39;lightsmile&#39;, &#39;.&#39;, &#39;My&#39;, &#39;github&#39;, &#39;link&#39;, &#39;is&#39;, &#39;www.github.com/smilelight&#39;, &#39;.&#39;, &#39;My&#39;, &#39;persoanl&#39;, &#39;website&#39;, &#39;is&#39;, &#39;www.iamlightsmile.com&#39;] 2. Stop Words(停用词)Then,import the stopwords(停用词) from the nltk.corpus module The stopwords are the words which are used commonly in the daliy life but usefulless for we to analyze the texts,so we need to remove them from the texts before we do the next steps. 1from nltk.corpus import stopwords 1example_sentense = \"This is an example showing off stop word filtration\" 1filter_sentense = [w for w in word_tokenize(example_sentense) if w not in stopwords.words('english')] 1filter_sentense [&#39;This&#39;, &#39;example&#39;, &#39;showing&#39;, &#39;stop&#39;, &#39;word&#39;, &#39;filtration&#39;] 3. Stemming(提取词干)Use PorterStemmer() to get the stems of words(提取词干) In some situations there are different expressions which have the same meanings.For example,the words:good,better,well have the similar meanings in the most situations.So,on the purpose to simplify the texts,we can get the stems of words in the texts. 1from nltk.stem import PorterStemmer 1234ps = PorterStemmer()example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]for w in example_words: print(ps.stem(w)) python python python python pythonli 123new_text = \"It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"for w in word_tokenize(new_text): print(ps.stem(w)) It is veri import to be pythonli while you are python with python . all python have python poorli at least onc . 4. Part of speech tagging(词性标注)Use pos_tag method to do part of speech tagging(词性标注) 12tagged = nltk.pos_tag(word_tokenize(new_text))print(tagged) [(&#39;It&#39;, &#39;PRP&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;very&#39;, &#39;RB&#39;), (&#39;important&#39;, &#39;JJ&#39;), (&#39;to&#39;, &#39;TO&#39;), (&#39;be&#39;, &#39;VB&#39;), (&#39;pythonly&#39;, &#39;RB&#39;), (&#39;while&#39;, &#39;IN&#39;), (&#39;you&#39;, &#39;PRP&#39;), (&#39;are&#39;, &#39;VBP&#39;), (&#39;pythoning&#39;, &#39;VBG&#39;), (&#39;with&#39;, &#39;IN&#39;), (&#39;python&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;), (&#39;All&#39;, &#39;DT&#39;), (&#39;pythoners&#39;, &#39;NNS&#39;), (&#39;have&#39;, &#39;VBP&#39;), (&#39;pythoned&#39;, &#39;VBN&#39;), (&#39;poorly&#39;, &#39;RB&#39;), (&#39;at&#39;, &#39;IN&#39;), (&#39;least&#39;, &#39;JJS&#39;), (&#39;once&#39;, &#39;RB&#39;), (&#39;.&#39;, &#39;.&#39;)] 1[w for w,t in tagged if t == 'RB' ] [&#39;very&#39;, &#39;pythonly&#39;, &#39;poorly&#39;, &#39;once&#39;] 5. Chunking(短语识别)1234chunkGram = r\"\"\"Chunk: &#123;&lt;RB.?&gt;*&lt;VB.?&gt;*&lt;NNP&gt;*&lt;NN&gt;&#125;\"\"\"chunkParser = nltk.RegexpParser(chunkGram)chunked = chunkParser.parse(tagged)print(chunked) (S It/PRP is/VBZ very/RB important/JJ to/TO be/VB pythonly/RB while/IN you/PRP are/VBP pythoning/VBG with/IN (Chunk python/NN) ./. All/DT pythoners/NNS have/VBP pythoned/VBN poorly/RB at/IN least/JJS once/RB ./.) 1chunked.draw() 6. Chinking(短语排除)The chinking is used to chunk something expect the chinking things.It’s effect is to remove something. 12345chinkGram = r\"\"\"Chunk: &#123;&lt;.*&gt;&#125; Chink: &#125;&lt;NN&gt;&#123;\"\"\"chinkParser = nltk.RegexpParser(chinkGram)chinked = chinkParser.parse(tagged)print(chinked) (S (Chunk It/PRP) (Chunk is/VBZ) (Chunk very/RB) (Chunk important/JJ) (Chunk to/TO) (Chunk be/VB) (Chunk pythonly/RB) (Chunk while/IN) (Chunk you/PRP) (Chunk are/VBP) (Chunk pythoning/VBG) (Chunk with/IN) (Chunk python/NN) (Chunk ./.) (Chunk All/DT) (Chunk pythoners/NNS) (Chunk have/VBP) (Chunk pythoned/VBN) (Chunk poorly/RB) (Chunk at/IN) (Chunk least/JJS) (Chunk once/RB) (Chunk ./.)) 1chinked.draw() 7. Named Entity Recognition(命名实体识别)1234new_text2 = \"The Obama,president of the United States,is walking by the Danube with his families.They'll go back home at 7:00 a.m..\"tagged2 = nltk.pos_tag(word_tokenize(new_text2))nameEnt = nltk.ne_chunk(tagged2)nameEnt.draw() 8. Lemmatizing(词形还原)12345from nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer()entities = [\"cats\",\"body\",\"shoes\",\"python\",\"shit\",\"park\"]for entity in entities: print(lemmatizer.lemmatize(entity)) cat body shoe python shit park 1nltk.__file__ &#39;C:\\\\Program Files\\\\Anaconda3\\\\lib\\\\site-packages\\\\nltk\\\\__init__.py&#39; 9. NLTK Corpora(语料库)12345from nltk.corpus import gutenbergfrom nltk.tokenize import sent_tokenizesample = gutenberg.raw('bible-kjv.txt')tok = sent_tokenize(sample)tok[:5] [&#39;[The King James Bible]\\n\\nThe Old Testament of the King James Bible\\n\\nThe First Book of Moses: Called Genesis\\n\\n\\n1:1 In the beginning God created the heaven and the earth.&#39;, &#39;1:2 And the earth was without form, and void; and darkness was upon\\nthe face of the deep.&#39;, &#39;And the Spirit of God moved upon the face of the\\nwaters.&#39;, &#39;1:3 And God said, Let there be light: and there was light.&#39;, &#39;1:4 And God saw the light, that it was good: and God divided the light\\nfrom the darkness.&#39;] 10. WordNet(一个英语词汇数据库)123from nltk.corpus import wordnetsyns = wordnet.synsets(\"program\")syns [Synset(&#39;plan.n.01&#39;), Synset(&#39;program.n.02&#39;), Synset(&#39;broadcast.n.02&#39;), Synset(&#39;platform.n.02&#39;), Synset(&#39;program.n.05&#39;), Synset(&#39;course_of_study.n.01&#39;), Synset(&#39;program.n.07&#39;), Synset(&#39;program.n.08&#39;), Synset(&#39;program.v.01&#39;), Synset(&#39;program.v.02&#39;)] 1234567891011word = wordnet.synsets('boy')synonyms =[]antonyms = []for w in word: for l in w.lemmas(): synonyms.append(l.name()) if l.antonyms(): for a in l.antonyms(): antonyms.append(a.name())print(set(synonyms))print(set(antonyms)) {&#39;male_child&#39;, &#39;son&#39;, &#39;boy&#39;} {&#39;female_child&#39;, &#39;daughter&#39;, &#39;girl&#39;} Use list comprehension(列表推导式) 1234synonyms2 = set([l.name() for w in word for l in w.lemmas()])antonyms2 = set([a.name() for w in word for l in w.lemmas() for a in l.antonyms()])print(synonyms2)print(antonyms2) {&#39;male_child&#39;, &#39;son&#39;, &#39;boy&#39;} {&#39;female_child&#39;, &#39;daughter&#39;, &#39;girl&#39;} 1word[0][\"boy\"].antosyns() --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-77-93678c6743d6&gt; in &lt;module&gt;() ----&gt; 1 word[0][&quot;boy&quot;].antosyns() TypeError: &#39;Synset&#39; object is not subscriptable 123cat = wordnet.synset(\"cat.n.01\")dog = wordnet.synset(\"dog.n.01\")dog.wup_similarity(cat) 0.8571428571428571 11. Text Classfication(文本分类)12import randomfrom nltk.corpus import movie_reviews 123456documents = [(list(movie_reviews.words(fileid)),category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]random.shuffle(documents)documents[1] ([&#39;did&#39;, &#39;you&#39;, &#39;ever&#39;, &#39;wonder&#39;, &#39;if&#39;, &#39;dennis&#39;, &#39;rodman&#39;, &#39;was&#39;, &#39;actually&#39;, &#39;from&#39;, &#39;this&#39;, &#39;planet&#39;, &#39;?&#39;, &#39;or&#39;, &#39;if&#39;, &#39;sylvester&#39;, &#39;stallone&#39;, &#39;was&#39;, &#39;some&#39;, &#39;kind&#39;, &#39;of&#39;, &#39;weird&#39;, &#39;extra&#39;, &#39;-&#39;, &#39;terrestrial&#39;, &#39;?&#39;, &#39;i&#39;, &#39;used&#39;, &#39;to&#39;, &#39;think&#39;, &#39;that&#39;, &#39;about&#39;, &#39;my&#39;, &#39;7th&#39;, &#39;grade&#39;, &#39;english&#39;, &#39;teacher&#39;, &#39;,&#39;, &#39;ms&#39;, &#39;.&#39;, &#39;carey&#39;, &#39;.&#39;, &#39;but&#39;, &#39;after&#39;, &#39;seeing&#39;, &#39;this&#39;, &#39;movie&#39;, &#39;,&#39;, &#39;they&#39;, &#39;may&#39;, &#39;have&#39;, &#39;confirmed&#39;, &#39;my&#39;, &#39;suspicions&#39;, &#39;.&#39;, &#39;as&#39;, &#39;the&#39;, &#39;story&#39;, &#39;goes&#39;, &#39;,&#39;, &#39;at&#39;, &#39;any&#39;, &#39;time&#39;, &#39;,&#39;, &#39;there&#39;, &#39;are&#39;, &#39;over&#39;, &#39;a&#39;, &#39;thousand&#39;, &#39;aliens&#39;, &#39;living&#39;, &#39;among&#39;, &#39;us&#39;, &#39;here&#39;, &#39;on&#39;, &#39;earth&#39;, &#39;.&#39;, &#39;the&#39;, &#39;men&#39;, &#39;in&#39;, &#39;black&#39;, &#39;(&#39;, &#39;mib&#39;, &#39;)&#39;, &#39;are&#39;, &#39;the&#39;, &#39;watchdogs&#39;, &#39;that&#39;, &#39;oversee&#39;, &#39;the&#39;, &#39;cosmic&#39;, &#39;citizens&#39;, &#39;,&#39;, &#39;guardians&#39;, &#39;of&#39;, &#39;our&#39;, &#39;beloved&#39;, &#39;planet&#39;, &#39;from&#39;, &#39;nasty&#39;, &#39;-&#39;, &#39;tempered&#39;, &#39;aliens&#39;, &#39;,&#39;, &#39;and&#39;, &#39;secret&#39;, &#39;service&#39;, &#39;to&#39;, &#39;the&#39;, &#39;stars&#39;, &#39;.&#39;, &#39;based&#39;, &#39;in&#39;, &#39;new&#39;, &#39;york&#39;, &#39;city&#39;, &#39;(&#39;, &#39;where&#39;, &#39;weird&#39;, &#39;is&#39;, &#39;the&#39;, &#39;norm&#39;, &#39;)&#39;, &#39;,&#39;, &#39;the&#39;, &#39;mib&#39;, &#39;organization&#39;, &#39;gives&#39;, &#39;human&#39;, &#39;form&#39;, &#39;to&#39;, &#39;our&#39;, &#39;space&#39;, &#39;-&#39;, &#39;faring&#39;, &#39;emigrants&#39;, &#39;so&#39;, &#39;that&#39;, &#39;they&#39;, &#39;may&#39;, &#39;walk&#39;, &#39;and&#39;, &#39;live&#39;, &#39;among&#39;, &#39;us&#39;, &#39;unnoticed&#39;, &#39;.&#39;, &#39;but&#39;, &#39;to&#39;, &#39;enforce&#39;, &#39;the&#39;, &#39;laws&#39;, &#39;of&#39;, &#39;earth&#39;, &#39;,&#39;, &#39;the&#39;, &#39;mib&#39;, &#39;carry&#39;, &#39;weapons&#39;, &#39;that&#39;, &#39;are&#39;, &#39;powerful&#39;, &#39;enough&#39;, &#39;to&#39;, &#39;meet&#39;, &#39;or&#39;, &#39;exceed&#39;, &#39;destruction&#39;, &#39;quotas&#39;, &#39;in&#39;, &#39;one&#39;, &#39;single&#39;, &#39;blast&#39;, &#39;.&#39;, &#39;they&#39;, &#39;carry&#39;, &#39;other&#39;, &#39;-&#39;, &#39;worldly&#39;, &#39;technology&#39;, &#39;to&#39;, &#39;erase&#39;, &#39;people&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;short&#39;, &#39;-&#39;, &#39;term&#39;, &#39;memory&#39;, &#39;when&#39;, &#39;common&#39;, &#39;folk&#39;, &#39;see&#39;, &#39;the&#39;, &#39;mib&#39;, &#39;in&#39;, &#39;action&#39;, &#39;.&#39;, &#39;and&#39;, &#39;their&#39;, &#39;best&#39;, &#39;leads&#39;, &#39;on&#39;, &#39;cosmic&#39;, &#39;things&#39;, &#39;-&#39;, &#39;gone&#39;, &#39;-&#39;, &#39;awry&#39;, &#39;are&#39;, &#39;the&#39;, &#39;supermarket&#39;, &#39;tabloids&#39;, &#39;.&#39;, &#39;little&#39;, &#39;do&#39;, &#39;we&#39;, &#39;know&#39;, &#39;that&#39;, &#39;there&#39;, &#39;are&#39;, &#39;much&#39;, &#39;stronger&#39;, &#39;battles&#39;, &#39;of&#39;, &#39;good&#39;, &#39;v&#39;, &#39;.&#39;, &#39;evil&#39;, &#39;going&#39;, &#39;on&#39;, &#39;in&#39;, &#39;the&#39;, &#39;depths&#39;, &#39;of&#39;, &#39;space&#39;, &#39;.&#39;, &#39;one&#39;, &#39;of&#39;, &#39;the&#39;, &#39;aliens&#39;, &#39;-&#39;, &#39;as&#39;, &#39;-&#39;, &#39;human&#39;, &#39;on&#39;, &#39;this&#39;, &#39;planet&#39;, &#39;is&#39;, &#39;an&#39;, &#39;important&#39;, &#39;diplomat&#39;, &#39;that&#39;, &#39;is&#39;, &#39;carrying&#39;, &#39;something&#39;, &#39;very&#39;, &#39;precious&#39;, &#39;.&#39;, &#39;it&#39;, &#39;holds&#39;, &#39;the&#39;, &quot;&#39;&quot;, &#39;key&#39;, &quot;&#39;&quot;, &#39;,&#39;, &#39;literally&#39;, &#39;,&#39;, &#39;to&#39;, &#39;universal&#39;, &#39;peace&#39;, &#39;.&#39;, &#39;a&#39;, &#39;giant&#39;, &#39;cockroach&#39;, &#39;-&#39;, &#39;like&#39;, &#39;alien&#39;, &#39;soon&#39;, &#39;arrives&#39;, &#39;on&#39;, &#39;the&#39;, &#39;planet&#39;, &#39;and&#39;, &#39;steals&#39;, &#39;this&#39;, &quot;&#39;&quot;, &#39;key&#39;, &quot;&#39;&quot;, &#39;.&#39;, &#39;in&#39;, &#39;the&#39;, &#39;wrong&#39;, &#39;alien&#39;, &#39;hands&#39;, &#39;(&#39;, &#39;flippers&#39;, &#39;?&#39;, &#39;mandibles&#39;, &#39;?&#39;, &#39;tentacles&#39;, &#39;?&#39;, &#39;)&#39;, &#39;,&#39;, &#39;it&#39;, &#39;can&#39;, &#39;be&#39;, &#39;used&#39;, &#39;as&#39;, &#39;a&#39;, &#39;weapon&#39;, &#39;.&#39;, &#39;therefore&#39;, &#39;,&#39;, &#39;it&#39;, &#39;must&#39;, &#39;be&#39;, &#39;recovered&#39;, &#39;and&#39;, &#39;returned&#39;, &#39;to&#39;, &#39;it&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;rightful&#39;, &#39;owners&#39;, &#39;.&#39;, &#39;otherwise&#39;, &#39;,&#39;, &#39;to&#39;, &#39;ensure&#39;, &#39;universal&#39;, &#39;safety&#39;, &#39;,&#39;, &#39;earth&#39;, &#39;will&#39;, &#39;be&#39;, &#39;destroyed&#39;, &#39;,&#39;, &#39;along&#39;, &#39;with&#39;, &#39;the&#39;, &quot;&#39;&quot;, &#39;key&#39;, &quot;&#39;&quot;, &#39;.&#39;, &#39;now&#39;, &#39;,&#39;, &#39;it&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;the&#39;, &#39;mib&#39;, &#39;who&#39;, &#39;must&#39;, &#39;prevent&#39;, &#39;this&#39;, &#39;catastrophe&#39;, &#39;.&#39;, &#39;the&#39;, &#39;mib&#39;, &#39;agents&#39;, &#39;on&#39;, &#39;the&#39;, &#39;case&#39;, &#39;are&#39;, &#39;&quot;&#39;, &#39;k&#39;, &#39;&quot;&#39;, &#39;,&#39;, &#39;played&#39;, &#39;by&#39;, &#39;tommy&#39;, &#39;lee&#39;, &#39;jones&#39;, &#39;.&#39;, &#39;he&#39;, &#39;is&#39;, &#39;crustier&#39;, &#39;than&#39;, &#39;burnt&#39;, &#39;toast&#39;, &#39;and&#39;, &#39;even&#39;, &#39;more&#39;, &#39;serious&#39;, &#39;than&#39;, &#39;al&#39;, &#39;gore&#39;, &#39;.&#39;, &#39;the&#39;, &#39;stars&#39;, &#39;in&#39;, &#39;the&#39;, &#39;sky&#39;, &#39;no&#39;, &#39;longer&#39;, &#39;spark&#39;, &#39;wonder&#39;, &#39;in&#39;, &#39;his&#39;, &#39;eyes&#39;, &#39;.&#39;, &#39;he&#39;, &#39;is&#39;, &#39;accompanied&#39;, &#39;by&#39;, &#39;a&#39;, &#39;flippant&#39;, &#39;rookie&#39;, &#39;,&#39;, &#39;&quot;&#39;, &#39;j&#39;, &#39;&quot;&#39;, &#39;,&#39;, &#39;played&#39;, &#39;by&#39;, &#39;will&#39;, &#39;smith&#39;, &#39;.&#39;, &#39;but&#39;, &#39;,&#39;, &#39;despite&#39;, &#39;this&#39;, &#39;shoot&#39;, &#39;-&#39;, &#39;em&#39;, &#39;-&#39;, &#39;up&#39;, &#39;,&#39;, &#39;protect&#39;, &#39;-&#39;, &#39;earth&#39;, &#39;-&#39;, &#39;from&#39;, &#39;-&#39;, &#39;destruction&#39;, &#39;premise&#39;, &#39;,&#39;, &#39;this&#39;, &#39;is&#39;, &#39;nothing&#39;, &#39;at&#39;, &#39;all&#39;, &#39;like&#39;, &#39;a&#39;, &#39;typical&#39;, &#39;summer&#39;, &#39;action&#39;, &#39;movie&#39;, &#39;.&#39;, &#39;and&#39;, &#39;,&#39;, &#39;this&#39;, &#39;isn&#39;, &quot;&#39;&quot;, &#39;t&#39;, &#39;an&#39;, &#39;independence&#39;, &#39;day&#39;, &#39;knockoff&#39;, &#39;.&#39;, &#39;rather&#39;, &#39;,&#39;, &#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;stylishly&#39;, &#39;offbeat&#39;, &#39;sci&#39;, &#39;-&#39;, &#39;fi&#39;, &#39;comedy&#39;, &#39;that&#39;, &#39;pokes&#39;, &#39;fun&#39;, &#39;at&#39;, &#39;what&#39;, &#39;the&#39;, &#39;government&#39;, &#39;always&#39;, &#39;denies&#39;, &#39;?&#39;, &#39;that&#39;, &#39;there&#39;, &#39;are&#39;, &#39;real&#39;, &#39;aliens&#39;, &#39;that&#39;, &#39;live&#39;, &#39;here&#39;, &#39;,&#39;, &#39;and&#39;, &#39;that&#39;, &#39;the&#39;, &#39;government&#39;, &#39;does&#39;, &#39;its&#39;, &#39;darndest&#39;, &#39;to&#39;, &#39;cover&#39;, &#39;them&#39;, &#39;up&#39;, &#39;.&#39;, &#39;but&#39;, &#39;to&#39;, &#39;give&#39;, &#39;it&#39;, &#39;some&#39;, &#39;sense&#39;, &#39;of&#39;, &#39;excitement&#39;, &#39;and&#39;, &#39;to&#39;, &#39;keep&#39;, &#39;it&#39;, &#39;within&#39;, &#39;the&#39;, &#39;parameters&#39;, &#39;of&#39;, &#39;the&#39;, &#39;summer&#39;, &#39;movie&#39;, &#39;recipe&#39;, &#39;,&#39;, &#39;there&#39;, &#39;must&#39;, &#39;be&#39;, &#39;some&#39;, &#39;kind&#39;, &#39;of&#39;, &#39;earth&#39;, &#39;-&#39;, &#39;hangs&#39;, &#39;-&#39;, &#39;in&#39;, &#39;-&#39;, &#39;the&#39;, &#39;-&#39;, &#39;balance&#39;, &#39;scenario&#39;, &#39;.&#39;, &#39;yet&#39;, &#39;,&#39;, &#39;this&#39;, &#39;movie&#39;, &#39;is&#39;, &#39;very&#39;, &#39;appealing&#39;, &#39;.&#39;, &#39;the&#39;, &#39;abundance&#39;, &#39;of&#39;, &#39;wierdness&#39;, &#39;(&#39;, &#39;talking&#39;, &#39;aliens&#39;, &#39;,&#39;, &#39;pee&#39;, &#39;-&#39;, &#39;wee&#39;, &#39;atomizers&#39;, &#39;,&#39;, &#39;a&#39;, &#39;mortician&#39;, &#39;who&#39;, &quot;&#39;&quot;, &#39;lives&#39;, &quot;&#39;&quot;, &#39;for&#39;, &#39;her&#39;, &#39;work&#39;, &#39;,&#39;, &#39;and&#39;, &#39;lots&#39;, &#39;of&#39;, &#39;yucky&#39;, &#39;bugs&#39;, &#39;and&#39;, &#39;slime&#39;, &#39;-&#39;, &#39;splattering&#39;, &#39;galore&#39;, &#39;)&#39;, &#39;,&#39;, &#39;is&#39;, &#39;played&#39;, &#39;straight&#39;, &#39;,&#39;, &#39;like&#39;, &#39;as&#39;, &#39;if&#39;, &#39;this&#39;, &#39;were&#39;, &#39;normal&#39;, &#39;(&#39;, &#39;of&#39;, &#39;course&#39;, &#39;,&#39;, &#39;we&#39;, &#39;are&#39;, &#39;in&#39;, &#39;nyc&#39;, &#39;)&#39;, &#39;.&#39;, &#39;it&#39;, &#39;gives&#39;, &#39;it&#39;, &#39;a&#39;, &#39;deadpan&#39;, &#39;feel&#39;, &#39;,&#39;, &#39;which&#39;, &#39;makes&#39;, &#39;it&#39;, &#39;all&#39;, &#39;the&#39;, &#39;more&#39;, &#39;funnier&#39;, &#39;and&#39;, &#39;odder&#39;, &#39;.&#39;, &#39;jones&#39;, &#39;plays&#39;, &#39;the&#39;, &#39;venerable&#39;, &#39;seen&#39;, &#39;-&#39;, &#39;it&#39;, &#39;-&#39;, &#39;all&#39;, &#39;agent&#39;, &#39;with&#39;, &#39;seriousness&#39;, &#39;and&#39;, &#39;maturity&#39;, &#39;.&#39;, &#39;smith&#39;, &#39;is&#39;, &#39;likeable&#39;, &#39;and&#39;, &#39;makes&#39;, &#39;a&#39;, &#39;great&#39;, &#39;comic&#39;, &#39;partner&#39;, &#39;to&#39;, &#39;jones&#39;, &quot;&#39;&quot;, &#39;straight&#39;, &#39;man&#39;, &#39;routine&#39;, &#39;.&#39;, &#39;they&#39;, &#39;click&#39;, &#39;like&#39;, &#39;dorothy&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;ruby&#39;, &#39;red&#39;, &#39;shoes&#39;, &#39;.&#39;, &#39;the&#39;, &#39;look&#39;, &#39;and&#39;, &#39;feel&#39;, &#39;of&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;is&#39;, &#39;made&#39;, &#39;even&#39;, &#39;better&#39;, &#39;with&#39;, &#39;direction&#39;, &#39;from&#39;, &#39;barry&#39;, &#39;sonnenfeld&#39;, &#39;(&#39;, &#39;the&#39;, &#39;addam&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;family&#39;, &#39;)&#39;, &#39;.&#39;, &#39;this&#39;, &#39;guy&#39;, &#39;has&#39;, &#39;a&#39;, &#39;knack&#39;, &#39;for&#39;, &quot;&#39;&quot;, &#39;gothic&#39;, &quot;&#39;&quot;, &#39;comedy&#39;, &#39;,&#39;, &#39;and&#39;, &#39;successfully&#39;, &#39;transfers&#39;, &#39;his&#39;, &#39;macabre&#39;, &#39;sense&#39;, &#39;of&#39;, &#39;humor&#39;, &#39;onto&#39;, &#39;the&#39;, &#39;screen&#39;, &#39;.&#39;, &#39;and&#39;, &#39;,&#39;, &#39;an&#39;, &#39;appropriate&#39;, &#39;dose&#39;, &#39;of&#39;, &#39;special&#39;, &#39;effects&#39;, &#39;helps&#39;, &#39;to&#39;, &#39;bolster&#39;, &#39;the&#39;, &#39;oddness&#39;, &#39;of&#39;, &#39;their&#39;, &#39;task&#39;, &#39;without&#39;, &#39;diverting&#39;, &#39;attention&#39;, &#39;from&#39;, &#39;the&#39;, &#39;human&#39;, &#39;actors&#39;, &#39;.&#39;, &#39;the&#39;, &#39;story&#39;, &#39;moves&#39;, &#39;well&#39;, &#39;,&#39;, &#39;and&#39;, &#39;before&#39;, &#39;you&#39;, &#39;know&#39;, &#39;it&#39;, &#39;,&#39;, &#39;the&#39;, &#39;end&#39;, &#39;credits&#39;, &#39;are&#39;, &#39;already&#39;, &#39;rolling&#39;, &#39;!&#39;, &#39;the&#39;, &#39;result&#39;, &#39;is&#39;, &#39;100&#39;, &#39;minutes&#39;, &#39;worth&#39;, &#39;of&#39;, &#39;fun&#39;, &#39;in&#39;, &#39;the&#39;, &#39;form&#39;, &#39;of&#39;, &#39;ewwwws&#39;, &#39;and&#39;, &#39;blechhhs&#39;, &#39;,&#39;, &#39;aaaahhhs&#39;, &#39;and&#39;, &#39;wows&#39;, &#39;.&#39;, &#39;let&#39;, &#39;the&#39;, &#39;men&#39;, &#39;in&#39;, &#39;black&#39;, &#39;protect&#39;, &#39;and&#39;, &#39;color&#39;, &#39;your&#39;, &#39;world&#39;, &#39;.&#39;], &#39;pos&#39;) 1all_words = [w.lower() for w in movie_reviews.words()] 1all_words [&#39;plot&#39;, &#39;:&#39;, &#39;two&#39;, &#39;teen&#39;, &#39;couples&#39;, &#39;go&#39;, &#39;to&#39;, &#39;a&#39;, &#39;church&#39;, &#39;party&#39;, &#39;,&#39;, &#39;drink&#39;, &#39;and&#39;, &#39;then&#39;, &#39;drive&#39;, &#39;.&#39;, &#39;they&#39;, &#39;get&#39;, &#39;into&#39;, &#39;an&#39;, &#39;accident&#39;, &#39;.&#39;, &#39;one&#39;, &#39;of&#39;, &#39;the&#39;, &#39;guys&#39;, &#39;dies&#39;, &#39;,&#39;, &#39;but&#39;, &#39;his&#39;, &#39;girlfriend&#39;, &#39;continues&#39;, &#39;to&#39;, &#39;see&#39;, &#39;him&#39;, &#39;in&#39;, &#39;her&#39;, &#39;life&#39;, &#39;,&#39;, &#39;and&#39;, &#39;has&#39;, &#39;nightmares&#39;, &#39;.&#39;, &#39;what&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;the&#39;, &#39;deal&#39;, &#39;?&#39;, &#39;watch&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;and&#39;, &#39;&quot;&#39;, &#39;sorta&#39;, &#39;&quot;&#39;, &#39;find&#39;, &#39;out&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;critique&#39;, &#39;:&#39;, &#39;a&#39;, &#39;mind&#39;, &#39;-&#39;, &#39;fuck&#39;, &#39;movie&#39;, &#39;for&#39;, &#39;the&#39;, &#39;teen&#39;, &#39;generation&#39;, &#39;that&#39;, &#39;touches&#39;, &#39;on&#39;, &#39;a&#39;, &#39;very&#39;, &#39;cool&#39;, &#39;idea&#39;, &#39;,&#39;, &#39;but&#39;, &#39;presents&#39;, &#39;it&#39;, &#39;in&#39;, &#39;a&#39;, &#39;very&#39;, &#39;bad&#39;, &#39;package&#39;, &#39;.&#39;, &#39;which&#39;, &#39;is&#39;, &#39;what&#39;, &#39;makes&#39;, &#39;this&#39;, &#39;review&#39;, &#39;an&#39;, &#39;even&#39;, &#39;harder&#39;, &#39;one&#39;, &#39;to&#39;, &#39;write&#39;, &#39;,&#39;, &#39;since&#39;, &#39;i&#39;, &#39;generally&#39;, &#39;applaud&#39;, &#39;films&#39;, &#39;which&#39;, &#39;attempt&#39;, &#39;to&#39;, &#39;break&#39;, &#39;the&#39;, &#39;mold&#39;, &#39;,&#39;, &#39;mess&#39;, &#39;with&#39;, &#39;your&#39;, &#39;head&#39;, &#39;and&#39;, &#39;such&#39;, &#39;(&#39;, &#39;lost&#39;, &#39;highway&#39;, &#39;&amp;&#39;, &#39;memento&#39;, &#39;)&#39;, &#39;,&#39;, &#39;but&#39;, &#39;there&#39;, &#39;are&#39;, &#39;good&#39;, &#39;and&#39;, &#39;bad&#39;, &#39;ways&#39;, &#39;of&#39;, &#39;making&#39;, &#39;all&#39;, &#39;types&#39;, &#39;of&#39;, &#39;films&#39;, &#39;,&#39;, &#39;and&#39;, &#39;these&#39;, &#39;folks&#39;, &#39;just&#39;, &#39;didn&#39;, &quot;&#39;&quot;, &#39;t&#39;, &#39;snag&#39;, &#39;this&#39;, &#39;one&#39;, &#39;correctly&#39;, &#39;.&#39;, &#39;they&#39;, &#39;seem&#39;, &#39;to&#39;, &#39;have&#39;, &#39;taken&#39;, &#39;this&#39;, &#39;pretty&#39;, &#39;neat&#39;, &#39;concept&#39;, &#39;,&#39;, &#39;but&#39;, &#39;executed&#39;, &#39;it&#39;, &#39;terribly&#39;, &#39;.&#39;, &#39;so&#39;, &#39;what&#39;, &#39;are&#39;, &#39;the&#39;, &#39;problems&#39;, &#39;with&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;?&#39;, &#39;well&#39;, &#39;,&#39;, &#39;its&#39;, &#39;main&#39;, &#39;problem&#39;, &#39;is&#39;, &#39;that&#39;, &#39;it&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;simply&#39;, &#39;too&#39;, &#39;jumbled&#39;, &#39;.&#39;, &#39;it&#39;, &#39;starts&#39;, &#39;off&#39;, &#39;&quot;&#39;, &#39;normal&#39;, &#39;&quot;&#39;, &#39;but&#39;, &#39;then&#39;, &#39;downshifts&#39;, &#39;into&#39;, &#39;this&#39;, &#39;&quot;&#39;, &#39;fantasy&#39;, &#39;&quot;&#39;, &#39;world&#39;, &#39;in&#39;, &#39;which&#39;, &#39;you&#39;, &#39;,&#39;, &#39;as&#39;, &#39;an&#39;, &#39;audience&#39;, &#39;member&#39;, &#39;,&#39;, &#39;have&#39;, &#39;no&#39;, &#39;idea&#39;, &#39;what&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;going&#39;, &#39;on&#39;, &#39;.&#39;, &#39;there&#39;, &#39;are&#39;, &#39;dreams&#39;, &#39;,&#39;, &#39;there&#39;, &#39;are&#39;, &#39;characters&#39;, &#39;coming&#39;, &#39;back&#39;, &#39;from&#39;, &#39;the&#39;, &#39;dead&#39;, &#39;,&#39;, &#39;there&#39;, &#39;are&#39;, &#39;others&#39;, &#39;who&#39;, &#39;look&#39;, &#39;like&#39;, &#39;the&#39;, &#39;dead&#39;, &#39;,&#39;, &#39;there&#39;, &#39;are&#39;, &#39;strange&#39;, &#39;apparitions&#39;, &#39;,&#39;, &#39;there&#39;, &#39;are&#39;, &#39;disappearances&#39;, &#39;,&#39;, &#39;there&#39;, &#39;are&#39;, &#39;a&#39;, &#39;looooot&#39;, &#39;of&#39;, &#39;chase&#39;, &#39;scenes&#39;, &#39;,&#39;, &#39;there&#39;, &#39;are&#39;, &#39;tons&#39;, &#39;of&#39;, &#39;weird&#39;, &#39;things&#39;, &#39;that&#39;, &#39;happen&#39;, &#39;,&#39;, &#39;and&#39;, &#39;most&#39;, &#39;of&#39;, &#39;it&#39;, &#39;is&#39;, &#39;simply&#39;, &#39;not&#39;, &#39;explained&#39;, &#39;.&#39;, &#39;now&#39;, &#39;i&#39;, &#39;personally&#39;, &#39;don&#39;, &quot;&#39;&quot;, &#39;t&#39;, &#39;mind&#39;, &#39;trying&#39;, &#39;to&#39;, &#39;unravel&#39;, &#39;a&#39;, &#39;film&#39;, &#39;every&#39;, &#39;now&#39;, &#39;and&#39;, &#39;then&#39;, &#39;,&#39;, &#39;but&#39;, &#39;when&#39;, &#39;all&#39;, &#39;it&#39;, &#39;does&#39;, &#39;is&#39;, &#39;give&#39;, &#39;me&#39;, &#39;the&#39;, &#39;same&#39;, &#39;clue&#39;, &#39;over&#39;, &#39;and&#39;, &#39;over&#39;, &#39;again&#39;, &#39;,&#39;, &#39;i&#39;, &#39;get&#39;, &#39;kind&#39;, &#39;of&#39;, &#39;fed&#39;, &#39;up&#39;, &#39;after&#39;, &#39;a&#39;, &#39;while&#39;, &#39;,&#39;, &#39;which&#39;, &#39;is&#39;, &#39;this&#39;, &#39;film&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;biggest&#39;, &#39;problem&#39;, &#39;.&#39;, &#39;it&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;obviously&#39;, &#39;got&#39;, &#39;this&#39;, &#39;big&#39;, &#39;secret&#39;, &#39;to&#39;, &#39;hide&#39;, &#39;,&#39;, &#39;but&#39;, &#39;it&#39;, &#39;seems&#39;, &#39;to&#39;, &#39;want&#39;, &#39;to&#39;, &#39;hide&#39;, &#39;it&#39;, &#39;completely&#39;, &#39;until&#39;, &#39;its&#39;, &#39;final&#39;, &#39;five&#39;, &#39;minutes&#39;, &#39;.&#39;, &#39;and&#39;, &#39;do&#39;, &#39;they&#39;, &#39;make&#39;, &#39;things&#39;, &#39;entertaining&#39;, &#39;,&#39;, &#39;thrilling&#39;, &#39;or&#39;, &#39;even&#39;, &#39;engaging&#39;, &#39;,&#39;, &#39;in&#39;, &#39;the&#39;, &#39;meantime&#39;, &#39;?&#39;, &#39;not&#39;, &#39;really&#39;, &#39;.&#39;, &#39;the&#39;, &#39;sad&#39;, &#39;part&#39;, &#39;is&#39;, &#39;that&#39;, &#39;the&#39;, &#39;arrow&#39;, &#39;and&#39;, &#39;i&#39;, &#39;both&#39;, &#39;dig&#39;, &#39;on&#39;, &#39;flicks&#39;, &#39;like&#39;, &#39;this&#39;, &#39;,&#39;, &#39;so&#39;, &#39;we&#39;, &#39;actually&#39;, &#39;figured&#39;, &#39;most&#39;, &#39;of&#39;, &#39;it&#39;, &#39;out&#39;, &#39;by&#39;, &#39;the&#39;, &#39;half&#39;, &#39;-&#39;, &#39;way&#39;, &#39;point&#39;, &#39;,&#39;, &#39;so&#39;, &#39;all&#39;, &#39;of&#39;, &#39;the&#39;, &#39;strangeness&#39;, &#39;after&#39;, &#39;that&#39;, &#39;did&#39;, &#39;start&#39;, &#39;to&#39;, &#39;make&#39;, &#39;a&#39;, &#39;little&#39;, &#39;bit&#39;, &#39;of&#39;, &#39;sense&#39;, &#39;,&#39;, &#39;but&#39;, &#39;it&#39;, &#39;still&#39;, &#39;didn&#39;, &quot;&#39;&quot;, &#39;t&#39;, &#39;the&#39;, &#39;make&#39;, &#39;the&#39;, &#39;film&#39;, &#39;all&#39;, &#39;that&#39;, &#39;more&#39;, &#39;entertaining&#39;, &#39;.&#39;, &#39;i&#39;, &#39;guess&#39;, &#39;the&#39;, &#39;bottom&#39;, &#39;line&#39;, &#39;with&#39;, &#39;movies&#39;, &#39;like&#39;, &#39;this&#39;, &#39;is&#39;, &#39;that&#39;, &#39;you&#39;, &#39;should&#39;, &#39;always&#39;, &#39;make&#39;, &#39;sure&#39;, &#39;that&#39;, &#39;the&#39;, &#39;audience&#39;, &#39;is&#39;, &#39;&quot;&#39;, &#39;into&#39;, &#39;it&#39;, &#39;&quot;&#39;, &#39;even&#39;, &#39;before&#39;, &#39;they&#39;, &#39;are&#39;, &#39;given&#39;, &#39;the&#39;, &#39;secret&#39;, &#39;password&#39;, &#39;to&#39;, &#39;enter&#39;, &#39;your&#39;, &#39;world&#39;, &#39;of&#39;, &#39;understanding&#39;, &#39;.&#39;, &#39;i&#39;, &#39;mean&#39;, &#39;,&#39;, &#39;showing&#39;, &#39;melissa&#39;, &#39;sagemiller&#39;, &#39;running&#39;, &#39;away&#39;, &#39;from&#39;, &#39;visions&#39;, &#39;for&#39;, &#39;about&#39;, &#39;20&#39;, &#39;minutes&#39;, &#39;throughout&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;is&#39;, &#39;just&#39;, &#39;plain&#39;, &#39;lazy&#39;, &#39;!&#39;, &#39;!&#39;, &#39;okay&#39;, &#39;,&#39;, &#39;we&#39;, &#39;get&#39;, &#39;it&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;there&#39;, &#39;are&#39;, &#39;people&#39;, &#39;chasing&#39;, &#39;her&#39;, &#39;and&#39;, &#39;we&#39;, &#39;don&#39;, &quot;&#39;&quot;, &#39;t&#39;, &#39;know&#39;, &#39;who&#39;, &#39;they&#39;, &#39;are&#39;, &#39;.&#39;, &#39;do&#39;, &#39;we&#39;, &#39;really&#39;, &#39;need&#39;, &#39;to&#39;, &#39;see&#39;, &#39;it&#39;, &#39;over&#39;, &#39;and&#39;, &#39;over&#39;, &#39;again&#39;, &#39;?&#39;, &#39;how&#39;, &#39;about&#39;, &#39;giving&#39;, &#39;us&#39;, &#39;different&#39;, &#39;scenes&#39;, &#39;offering&#39;, &#39;further&#39;, &#39;insight&#39;, &#39;into&#39;, &#39;all&#39;, &#39;of&#39;, &#39;the&#39;, &#39;strangeness&#39;, &#39;going&#39;, &#39;down&#39;, &#39;in&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;?&#39;, &#39;apparently&#39;, &#39;,&#39;, &#39;the&#39;, &#39;studio&#39;, &#39;took&#39;, &#39;this&#39;, &#39;film&#39;, &#39;away&#39;, &#39;from&#39;, &#39;its&#39;, &#39;director&#39;, &#39;and&#39;, &#39;chopped&#39;, &#39;it&#39;, &#39;up&#39;, &#39;themselves&#39;, &#39;,&#39;, &#39;and&#39;, &#39;it&#39;, &#39;shows&#39;, &#39;.&#39;, &#39;there&#39;, &#39;might&#39;, &quot;&#39;&quot;, &#39;ve&#39;, &#39;been&#39;, &#39;a&#39;, &#39;pretty&#39;, &#39;decent&#39;, &#39;teen&#39;, &#39;mind&#39;, &#39;-&#39;, &#39;fuck&#39;, &#39;movie&#39;, &#39;in&#39;, &#39;here&#39;, &#39;somewhere&#39;, &#39;,&#39;, &#39;but&#39;, &#39;i&#39;, &#39;guess&#39;, &#39;&quot;&#39;, &#39;the&#39;, &#39;suits&#39;, &#39;&quot;&#39;, &#39;decided&#39;, &#39;that&#39;, &#39;turning&#39;, &#39;it&#39;, &#39;into&#39;, &#39;a&#39;, &#39;music&#39;, &#39;video&#39;, &#39;with&#39;, &#39;little&#39;, &#39;edge&#39;, &#39;,&#39;, &#39;would&#39;, &#39;make&#39;, &#39;more&#39;, &#39;sense&#39;, &#39;.&#39;, &#39;the&#39;, &#39;actors&#39;, &#39;are&#39;, &#39;pretty&#39;, &#39;good&#39;, &#39;for&#39;, &#39;the&#39;, &#39;most&#39;, &#39;part&#39;, &#39;,&#39;, &#39;although&#39;, &#39;wes&#39;, &#39;bentley&#39;, &#39;just&#39;, &#39;seemed&#39;, &#39;to&#39;, &#39;be&#39;, &#39;playing&#39;, &#39;the&#39;, &#39;exact&#39;, &#39;same&#39;, &#39;character&#39;, &#39;that&#39;, &#39;he&#39;, &#39;did&#39;, &#39;in&#39;, &#39;american&#39;, &#39;beauty&#39;, &#39;,&#39;, &#39;only&#39;, &#39;in&#39;, &#39;a&#39;, &#39;new&#39;, &#39;neighborhood&#39;, &#39;.&#39;, &#39;but&#39;, &#39;my&#39;, &#39;biggest&#39;, &#39;kudos&#39;, &#39;go&#39;, &#39;out&#39;, &#39;to&#39;, &#39;sagemiller&#39;, &#39;,&#39;, &#39;who&#39;, &#39;holds&#39;, &#39;her&#39;, &#39;own&#39;, &#39;throughout&#39;, &#39;the&#39;, &#39;entire&#39;, &#39;film&#39;, &#39;,&#39;, &#39;and&#39;, &#39;actually&#39;, &#39;has&#39;, &#39;you&#39;, &#39;feeling&#39;, &#39;her&#39;, &#39;character&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;unraveling&#39;, &#39;.&#39;, &#39;overall&#39;, &#39;,&#39;, &#39;the&#39;, &#39;film&#39;, &#39;doesn&#39;, &quot;&#39;&quot;, &#39;t&#39;, &#39;stick&#39;, &#39;because&#39;, &#39;it&#39;, &#39;doesn&#39;, &quot;&#39;&quot;, &#39;t&#39;, &#39;entertain&#39;, &#39;,&#39;, &#39;it&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;confusing&#39;, &#39;,&#39;, &#39;it&#39;, &#39;rarely&#39;, &#39;excites&#39;, &#39;and&#39;, &#39;it&#39;, &#39;feels&#39;, &#39;pretty&#39;, &#39;redundant&#39;, &#39;for&#39;, &#39;most&#39;, &#39;of&#39;, &#39;its&#39;, &#39;runtime&#39;, &#39;,&#39;, &#39;despite&#39;, &#39;a&#39;, &#39;pretty&#39;, &#39;cool&#39;, &#39;ending&#39;, &#39;and&#39;, &#39;explanation&#39;, &#39;to&#39;, &#39;all&#39;, &#39;of&#39;, &#39;the&#39;, &#39;craziness&#39;, &#39;that&#39;, &#39;came&#39;, &#39;before&#39;, &#39;it&#39;, &#39;.&#39;, &#39;oh&#39;, &#39;,&#39;, &#39;and&#39;, &#39;by&#39;, &#39;the&#39;, &#39;way&#39;, &#39;,&#39;, &#39;this&#39;, &#39;is&#39;, &#39;not&#39;, &#39;a&#39;, &#39;horror&#39;, &#39;or&#39;, &#39;teen&#39;, &#39;slasher&#39;, &#39;flick&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;it&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;just&#39;, &#39;packaged&#39;, &#39;to&#39;, &#39;look&#39;, &#39;that&#39;, &#39;way&#39;, &#39;because&#39;, &#39;someone&#39;, &#39;is&#39;, &#39;apparently&#39;, &#39;assuming&#39;, &#39;that&#39;, &#39;the&#39;, &#39;genre&#39;, &#39;is&#39;, &#39;still&#39;, &#39;hot&#39;, &#39;with&#39;, &#39;the&#39;, &#39;kids&#39;, &#39;.&#39;, &#39;it&#39;, &#39;also&#39;, &#39;wrapped&#39;, &#39;production&#39;, &#39;two&#39;, &#39;years&#39;, &#39;ago&#39;, &#39;and&#39;, &#39;has&#39;, &#39;been&#39;, &#39;sitting&#39;, &#39;on&#39;, &#39;the&#39;, &#39;shelves&#39;, &#39;ever&#39;, &#39;since&#39;, &#39;.&#39;, &#39;whatever&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;skip&#39;, &#39;it&#39;, &#39;!&#39;, &#39;where&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;joblo&#39;, &#39;coming&#39;, &#39;from&#39;, &#39;?&#39;, &#39;a&#39;, &#39;nightmare&#39;, &#39;of&#39;, &#39;elm&#39;, &#39;street&#39;, &#39;3&#39;, &#39;(&#39;, &#39;7&#39;, &#39;/&#39;, &#39;10&#39;, &#39;)&#39;, &#39;-&#39;, &#39;blair&#39;, &#39;witch&#39;, &#39;2&#39;, &#39;(&#39;, &#39;7&#39;, &#39;/&#39;, &#39;10&#39;, &#39;)&#39;, &#39;-&#39;, &#39;the&#39;, &#39;crow&#39;, &#39;(&#39;, &#39;9&#39;, &#39;/&#39;, &#39;10&#39;, &#39;)&#39;, &#39;-&#39;, &#39;the&#39;, &#39;crow&#39;, &#39;:&#39;, &#39;salvation&#39;, &#39;(&#39;, &#39;4&#39;, &#39;/&#39;, &#39;10&#39;, &#39;)&#39;, &#39;-&#39;, &#39;lost&#39;, &#39;highway&#39;, &#39;(&#39;, &#39;10&#39;, &#39;/&#39;, &#39;10&#39;, &#39;)&#39;, &#39;-&#39;, &#39;memento&#39;, &#39;(&#39;, &#39;10&#39;, &#39;/&#39;, &#39;10&#39;, &#39;)&#39;, &#39;-&#39;, &#39;the&#39;, &#39;others&#39;, &#39;(&#39;, &#39;9&#39;, &#39;/&#39;, &#39;10&#39;, &#39;)&#39;, &#39;-&#39;, &#39;stir&#39;, &#39;of&#39;, &#39;echoes&#39;, &#39;(&#39;, &#39;8&#39;, &#39;/&#39;, &#39;10&#39;, &#39;)&#39;, &#39;the&#39;, &#39;happy&#39;, &#39;bastard&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;quick&#39;, &#39;movie&#39;, &#39;review&#39;, &#39;damn&#39;, &#39;that&#39;, &#39;y2k&#39;, &#39;bug&#39;, &#39;.&#39;, &#39;it&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;got&#39;, &#39;a&#39;, &#39;head&#39;, &#39;start&#39;, &#39;in&#39;, &#39;this&#39;, &#39;movie&#39;, &#39;starring&#39;, &#39;jamie&#39;, &#39;lee&#39;, &#39;curtis&#39;, &#39;and&#39;, &#39;another&#39;, &#39;baldwin&#39;, &#39;brother&#39;, &#39;(&#39;, &#39;william&#39;, &#39;this&#39;, &#39;time&#39;, &#39;)&#39;, &#39;in&#39;, &#39;a&#39;, &#39;story&#39;, &#39;regarding&#39;, &#39;a&#39;, &#39;crew&#39;, &#39;of&#39;, &#39;a&#39;, &#39;tugboat&#39;, &#39;that&#39;, &#39;comes&#39;, &#39;across&#39;, &#39;a&#39;, &#39;deserted&#39;, &#39;russian&#39;, &#39;tech&#39;, &#39;ship&#39;, &#39;that&#39;, &#39;has&#39;, &#39;a&#39;, &#39;strangeness&#39;, &#39;to&#39;, &#39;it&#39;, &#39;when&#39;, &#39;they&#39;, &#39;kick&#39;, &#39;the&#39;, &#39;power&#39;, &#39;back&#39;, &#39;on&#39;, &#39;.&#39;, &#39;little&#39;, &#39;do&#39;, &#39;they&#39;, &#39;know&#39;, &#39;the&#39;, &#39;power&#39;, &#39;within&#39;, &#39;.&#39;, &#39;.&#39;, &#39;.&#39;, &#39;going&#39;, &#39;for&#39;, &#39;the&#39;, &#39;gore&#39;, &#39;and&#39;, &#39;bringing&#39;, &#39;on&#39;, &#39;a&#39;, &#39;few&#39;, &#39;action&#39;, &#39;sequences&#39;, &#39;here&#39;, &#39;and&#39;, &#39;there&#39;, &#39;,&#39;, &#39;virus&#39;, &#39;still&#39;, &#39;feels&#39;, &#39;very&#39;, &#39;empty&#39;, &#39;,&#39;, &#39;like&#39;, &#39;a&#39;, &#39;movie&#39;, &#39;going&#39;, &#39;for&#39;, &#39;all&#39;, &#39;flash&#39;, &#39;and&#39;, &#39;no&#39;, &#39;substance&#39;, &#39;.&#39;, &#39;we&#39;, &#39;don&#39;, &quot;&#39;&quot;, &#39;t&#39;, &#39;know&#39;, &#39;why&#39;, &#39;the&#39;, &#39;crew&#39;, &#39;was&#39;, &#39;really&#39;, &#39;out&#39;, &#39;in&#39;, ...] Warning:At the begining, I just writed the follow codes like this:new_all_words = [w for w in all_words if w not in nltk.corpus.stopwords.words(&#39;english&#39;),however the code couldn’t complite successfully even I had been waiting for several minites. Finally, I found that the I/O operations can be 1583820 times, and the operation system read data from the hark disk again and again without saying any thing,it was so stupid.So when we programming,we should set the I/O resources as a variable if it will be used for several times. 1stopwords = nltk.corpus.stopwords.words('english') 1new_all_words = [w for w in all_words if w not in stopwords] 1new_all_words = [w for w in new_all_words if w.isalpha()] 1len(all_words) 1583820 1len(nltk.corpus.stopwords.words('english')) 179 1nltk.corpus.stopwords.words('english') [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;, &#39;ourselves&#39;, &#39;you&#39;, &quot;you&#39;re&quot;, &quot;you&#39;ve&quot;, &quot;you&#39;ll&quot;, &quot;you&#39;d&quot;, &#39;your&#39;, &#39;yours&#39;, &#39;yourself&#39;, &#39;yourselves&#39;, &#39;he&#39;, &#39;him&#39;, &#39;his&#39;, &#39;himself&#39;, &#39;she&#39;, &quot;she&#39;s&quot;, &#39;her&#39;, &#39;hers&#39;, &#39;herself&#39;, &#39;it&#39;, &quot;it&#39;s&quot;, &#39;its&#39;, &#39;itself&#39;, &#39;they&#39;, &#39;them&#39;, &#39;their&#39;, &#39;theirs&#39;, &#39;themselves&#39;, &#39;what&#39;, &#39;which&#39;, &#39;who&#39;, &#39;whom&#39;, &#39;this&#39;, &#39;that&#39;, &quot;that&#39;ll&quot;, &#39;these&#39;, &#39;those&#39;, &#39;am&#39;, &#39;is&#39;, &#39;are&#39;, &#39;was&#39;, &#39;were&#39;, &#39;be&#39;, &#39;been&#39;, &#39;being&#39;, &#39;have&#39;, &#39;has&#39;, &#39;had&#39;, &#39;having&#39;, &#39;do&#39;, &#39;does&#39;, &#39;did&#39;, &#39;doing&#39;, &#39;a&#39;, &#39;an&#39;, &#39;the&#39;, &#39;and&#39;, &#39;but&#39;, &#39;if&#39;, &#39;or&#39;, &#39;because&#39;, &#39;as&#39;, &#39;until&#39;, &#39;while&#39;, &#39;of&#39;, &#39;at&#39;, &#39;by&#39;, &#39;for&#39;, &#39;with&#39;, &#39;about&#39;, &#39;against&#39;, &#39;between&#39;, &#39;into&#39;, &#39;through&#39;, &#39;during&#39;, &#39;before&#39;, &#39;after&#39;, &#39;above&#39;, &#39;below&#39;, &#39;to&#39;, &#39;from&#39;, &#39;up&#39;, &#39;down&#39;, &#39;in&#39;, &#39;out&#39;, &#39;on&#39;, &#39;off&#39;, &#39;over&#39;, &#39;under&#39;, &#39;again&#39;, &#39;further&#39;, &#39;then&#39;, &#39;once&#39;, &#39;here&#39;, &#39;there&#39;, &#39;when&#39;, &#39;where&#39;, &#39;why&#39;, &#39;how&#39;, &#39;all&#39;, &#39;any&#39;, &#39;both&#39;, &#39;each&#39;, &#39;few&#39;, &#39;more&#39;, &#39;most&#39;, &#39;other&#39;, &#39;some&#39;, &#39;such&#39;, &#39;no&#39;, &#39;nor&#39;, &#39;not&#39;, &#39;only&#39;, &#39;own&#39;, &#39;same&#39;, &#39;so&#39;, &#39;than&#39;, &#39;too&#39;, &#39;very&#39;, &#39;s&#39;, &#39;t&#39;, &#39;can&#39;, &#39;will&#39;, &#39;just&#39;, &#39;don&#39;, &quot;don&#39;t&quot;, &#39;should&#39;, &quot;should&#39;ve&quot;, &#39;now&#39;, &#39;d&#39;, &#39;ll&#39;, &#39;m&#39;, &#39;o&#39;, &#39;re&#39;, &#39;ve&#39;, &#39;y&#39;, &#39;ain&#39;, &#39;aren&#39;, &quot;aren&#39;t&quot;, &#39;couldn&#39;, &quot;couldn&#39;t&quot;, &#39;didn&#39;, &quot;didn&#39;t&quot;, &#39;doesn&#39;, &quot;doesn&#39;t&quot;, &#39;hadn&#39;, &quot;hadn&#39;t&quot;, &#39;hasn&#39;, &quot;hasn&#39;t&quot;, &#39;haven&#39;, &quot;haven&#39;t&quot;, &#39;isn&#39;, &quot;isn&#39;t&quot;, &#39;ma&#39;, &#39;mightn&#39;, &quot;mightn&#39;t&quot;, &#39;mustn&#39;, &quot;mustn&#39;t&quot;, &#39;needn&#39;, &quot;needn&#39;t&quot;, &#39;shan&#39;, &quot;shan&#39;t&quot;, &#39;shouldn&#39;, &quot;shouldn&#39;t&quot;, &#39;wasn&#39;, &quot;wasn&#39;t&quot;, &#39;weren&#39;, &quot;weren&#39;t&quot;, &#39;won&#39;, &quot;won&#39;t&quot;, &#39;wouldn&#39;, &quot;wouldn&#39;t&quot;] 12words_freqlist = nltk.FreqDist(new_all_words)print(words_freqlist.most_common(10)) [(&#39;film&#39;, 9517), (&#39;one&#39;, 5852), (&#39;movie&#39;, 5771), (&#39;like&#39;, 3690), (&#39;even&#39;, 2565), (&#39;good&#39;, 2411), (&#39;time&#39;, 2411), (&#39;story&#39;, 2169), (&#39;would&#39;, 2109), (&#39;much&#39;, 2049)] 12. Words as Features for Learning(用来学习的特征词汇)1word_features = list(words_freqlist.keys())[:3000] 12345678def find_features(document): words = set(document) features = &#123;&#125; for w in word_features: features[w] = (w in words) return featuresfeaturesets = [(find_features(rev), category) for (rev, category) in documents] 13. Naive Bayes(朴素贝叶斯)12training_set = featuresets[:1900]testing_set = featuresets[1900:] 12classifier = nltk.NaiveBayesClassifier.train(training_set)print(\"Naive Bayes Algo accuracy:\",(nltk.classify.accuracy(classifier,testing_set))*100) Naive Bayes Algo accuracy: 84.0 1classifier.show_most_informative_features(15) Most Informative Features sucks = True neg : pos = 8.7 : 1.0 annual = True pos : neg = 8.2 : 1.0 frances = True pos : neg = 8.2 : 1.0 unimaginative = True neg : pos = 7.8 : 1.0 idiotic = True neg : pos = 7.3 : 1.0 schumacher = True neg : pos = 7.1 : 1.0 mena = True neg : pos = 7.1 : 1.0 atrocious = True neg : pos = 7.1 : 1.0 silverstone = True neg : pos = 7.1 : 1.0 suvari = True neg : pos = 7.1 : 1.0 turkey = True neg : pos = 6.7 : 1.0 regard = True pos : neg = 6.5 : 1.0 kidding = True neg : pos = 6.4 : 1.0 crappy = True neg : pos = 6.4 : 1.0 shoddy = True neg : pos = 6.4 : 1.0 14. Save Classifier with Pickle(使用Pickle保存分类器)1import pickle 123save_classifier = open('naivebayes.pickle',\"wb\")pickle.dump(classifier,save_classifier)save_classifier.close() 123classifier_f = open('naivebayes.pickle',\"rb\")classifier = pickle.load(classifier_f)classifier_f.close() 12print(\"Naive Bayes Algo accuracy:\",(nltk.classify.accuracy(classifier,testing_set))*100)classifier.show_most_informative_features(15) Naive Bayes Algo accuracy: 84.0 Most Informative Features sucks = True neg : pos = 8.7 : 1.0 annual = True pos : neg = 8.2 : 1.0 frances = True pos : neg = 8.2 : 1.0 unimaginative = True neg : pos = 7.8 : 1.0 idiotic = True neg : pos = 7.3 : 1.0 schumacher = True neg : pos = 7.1 : 1.0 mena = True neg : pos = 7.1 : 1.0 atrocious = True neg : pos = 7.1 : 1.0 silverstone = True neg : pos = 7.1 : 1.0 suvari = True neg : pos = 7.1 : 1.0 turkey = True neg : pos = 6.7 : 1.0 regard = True pos : neg = 6.5 : 1.0 kidding = True neg : pos = 6.4 : 1.0 crappy = True neg : pos = 6.4 : 1.0 shoddy = True neg : pos = 6.4 : 1.0 training again 123456789random.shuffle(documents)all_words = [w.lower() for w in movie_reviews.words()]new_all_words = [w for w in all_words if w not in stopwords]new_all_words = [w for w in new_all_words if w.isalpha()]words_freqlist = nltk.FreqDist(new_all_words)word_features = list(words_freqlist.keys())[:3000]training_set = featuresets[:1900]testing_set = featuresets[1900:]classifier = nltk.NaiveBayesClassifier.train(training_set) 1print(\"Naive Bayes Algo accuracy:\",(nltk.classify.accuracy(classifier,testing_set))*100) Naive Bayes Algo accuracy: 84.0 123classifier_f = open('naivebayes.pickle',\"rb\")classifier = pickle.load(classifier_f)classifier_f.close() 1print(\"Naive Bayes Algo accuracy:\",(nltk.classify.accuracy(classifier,testing_set))*100) Naive Bayes Algo accuracy: 84.0 1print(\"Naive Bayes Algo accuracy:\",(nltk.classify.accuracy(classifier,testing_set))*100) Naive Bayes Algo accuracy: 84.0 15. Scikit-Learn incorporation()1from nltk.classify.scikitlearn import SklearnClassifier 1from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB 123MNB_classifier = SklearnClassifier(MultinomialNB())MNB_classifier.train(training_set)print(\"MNB_classifier accuracy percent:\",(nltk.classify.accuracy(MNB_classifier,testing_set))*100) MNB_classifier accuracy percent: 82.0 1234# 这段代码有问题，不可以运行。GNB_classifier = SklearnClassifier(GaussianNB())GNB_classifier.train(training_set)print(\"GNB_classifier accuracy percent:\",(nltk.classify.accuracy(GNB_classifier,testing_set))*100) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-149-dbf69e211330&gt; in &lt;module&gt;() 1 GNB_classifier = SklearnClassifier(GaussianNB()) ----&gt; 2 GNB_classifier.train(training_set) 3 print(&quot;GNB_classifier accuracy percent:&quot;,(nltk.classify.accuracy(GNB_classifier,testing_set))*100) C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\classify\\scikitlearn.py in train(self, labeled_featuresets) 117 X = self._vectorizer.fit_transform(X) 118 y = self._encoder.fit_transform(y) --&gt; 119 self._clf.fit(X, y) 120 121 return self C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py in fit(self, X, y, sample_weight) 180 Returns self. 181 &quot;&quot;&quot; --&gt; 182 X, y = check_X_y(X, y) 183 return self._partial_fit(X, y, np.unique(y), _refit=True, 184 sample_weight=sample_weight) C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator) 519 X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite, 520 ensure_2d, allow_nd, ensure_min_samples, --&gt; 521 ensure_min_features, warn_on_dtype, estimator) 522 if multi_output: 523 y = check_array(y, &#39;csr&#39;, force_all_finite=True, ensure_2d=False, C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator) 378 if sp.issparse(array): 379 array = _ensure_sparse_format(array, accept_sparse, dtype, copy, --&gt; 380 force_all_finite) 381 else: 382 array = np.array(array, dtype=dtype, order=order, copy=copy) C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py in _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite) 241 &quot;&quot;&quot; 242 if accept_sparse in [None, False]: --&gt; 243 raise TypeError(&#39;A sparse matrix was passed, but dense &#39; 244 &#39;data is required. Use X.toarray() to &#39; 245 &#39;convert to a dense numpy array.&#39;) TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array. 12 123BNB_classifier = SklearnClassifier(BernoulliNB())BNB_classifier.train(training_set)print(\"BNB_classifier accuracy percent:\",(nltk.classify.accuracy(BNB_classifier,testing_set))*100) BNB_classifier accuracy percent: 84.0 12from sklearn.linear_model import LogisticRegression, SGDClassifierfrom sklearn.svm import SVC, LinearSVC, NuSVC 123LogisticRegression_classifier = SklearnClassifier(LogisticRegression())LogisticRegression_classifier.train(training_set)print(\"LogisticRegression_classifier accuracy percent:\",(nltk.classify.accuracy(LogisticRegression_classifier,testing_set))*100) LogisticRegression_classifier accuracy percent: 82.0 123SGDClassifier_classifier = SklearnClassifier(SGDClassifier())SGDClassifier_classifier.train(training_set)print(\"SGDClassifier_classifier accuracy percent:\",(nltk.classify.accuracy(SGDClassifier_classifier,testing_set))*100) SGDClassifier_classifier accuracy percent: 82.0 123SVC_classifier = SklearnClassifier(SVC())SVC_classifier.train(training_set)print(\"SVC_classifier accuracy percent:\",(nltk.classify.accuracy(SVC_classifier,testing_set))*100) SVC_classifier accuracy percent: 82.0 123LinearSVC_classifier = SklearnClassifier(LinearSVC())LinearSVC_classifier.train(training_set)print(\"LinearSVC_classifier accuracy percent:\",(nltk.classify.accuracy(LinearSVC_classifier,testing_set))*100) LinearSVC_classifier accuracy percent: 80.0 123NuSVC_classifier = SklearnClassifier(NuSVC())NuSVC_classifier.train(training_set)print(\"NuSVC_classifier accuracy percent:\",(nltk.classify.accuracy(NuSVC_classifier,testing_set))*100) NuSVC_classifier accuracy percent: 82.0 16. Combining Algos with a Vote12from nltk.classify import ClassifierIfrom statistics import mode 12345678910111213141516171819class VoteClassifier(ClassifierI): def __init__(self, *classifiers): self._classifiers = classifiers def classify(self, features): votes = [] for c in self._classifiers: v = c.classify(features) votes.append(v) return mode(votes) def confidence(self,features): votes = [] for c in self._classifiers: v = c.classify(features) votes.append(v) choice_votes = votes.count(mode(votes)) conf = choice_votes / len(votes) return conf 12345678910voted_classifier = VoteClassifier(classifier, MNB_classifier, BNB_classifier, LogisticRegression_classifier, SGDClassifier_classifier, #SVC_classifier,视频中没有这个，况且如果不注释掉就会报统计错误，说有两个相同的值。 #如： http://blog.csdn.net/dongfuguo/article/details/50163757 中 mode错误一般 LinearSVC_classifier, NuSVC_classifier)print(\"voted_classifier accuracy percent:\",(nltk.classify.accuracy(voted_classifier,testing_set))*100) voted_classifier accuracy percent: 81.0 1print(\"Classification:\",voted_classifier.classify(testing_set[0][0]),\"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100) Classification: neg Confidence %: 100.0 1print(\"Classification:\",voted_classifier.classify(testing_set[1][0]),\"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100) Classification: pos Confidence %: 100.0 1print(\"Classification:\",voted_classifier.classify(testing_set[2][0]),\"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100) Classification: pos Confidence %: 100.0 1print(\"Classification:\",voted_classifier.classify(testing_set[3][0]),\"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100) Classification: neg Confidence %: 87.5 1print(\"Classification:\",voted_classifier.classify(testing_set[4][0]),\"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100) Classification: pos Confidence %: 100.0 1print(\"Classification:\",voted_classifier.classify(testing_set[5][0]),\"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100) Classification: neg Confidence %: 75.0 17. Investigating Bias()18. Better training data()12short_pos = open(\"short_reviews/positive.txt\",\"r\",encoding=\"unicode-escape\").read()short_neg = open(\"short_reviews/negative.txt\",\"r\",encoding=\"unicode-escape\").read() 1short_pos[:300] &#39;the rock is destined to be the 21st century\\&#39;s new &quot; conan &quot; and that he\\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \\nthe gorgeously elaborate continuation of &quot; the lord of the rings &quot; trilogy is so huge that a column of words cannot adequ&#39; 1documents = [] 12# map(lambda r : document.append(r,'pos'), [r for r in short_pos.split('\\n')])# 本来想通过类似foreach实现类似的功能，不过好像并不能成功，目前原因还不清楚。 12documents.extend([(r,\"pos\") for r in short_pos.split('\\n')])# 和下面语句的作用是一样的，不过不知道哪个效率更高一些 12for r in short_pos.split('\\n'): documents.append((r,'pos')) 1documents[0] (&#39;the rock is destined to be the 21st century\\&#39;s new &quot; conan &quot; and that he\\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . &#39;, &#39;pos&#39;) 12documents.extend([(r,\"neg\") for r in short_neg.split('\\n')])# 和下面语句的作用是一样的，不过不知道哪个效率更高一些 12for w in short_neg.split('\\n'): documents.append(w.lower()) 12import nltk# 这里之所以再次导入，仅仅是因为我是几次使用这个notebook，懒得运行前面的cell了。 123all_words = []short_pos_words = nltk.word_tokenize(short_pos)short_neg_words = nltk.word_tokenize(short_neg) 12all_words.extend([w.lower() for w in short_pos_words])all_words.extend([w.lower() for w in short_neg_words]) 123456789# 以上代码应该也可以写成：all_words = [w.lower() for w in short_pos_words] + [w.lower() for w in short_pos_words]#甚至是这样：all_words = [w.lower() for w in short_pos_words+short_neg_words]# 不过如果先：all_words = short_pos_words + short_neg_words# 再：all_words = [w.lower() for w in all_words]# 可能效率更高一些吧？ 1stopwords = nltk.corpus.stopwords.words('english') 12all_words = [w for w in all_words if w not in stopwords]# 我自己添加的去除停用词等无关信息，以使得特征提取和训练的效率更高 12all_words = nltk.FreqDist(all_words)word_features = list(all_words.keys())[:5000] 12345678def find_features(document): words = nltk.word_tokenize(document) features = &#123;&#125; for w in word_features: features[w] = (w in words) return featuresfeaturesets = [(find_features(rev), category) for (rev, category) in documents] 12import randomrandom.shuffle(featuresets) 12training_set = featuresets[:10000]testing_set = featuresets[10000:] 12classifier = nltk.NaiveBayesClassifier.train(training_set)print(\"Naive Bayes Algo accuracy:\",(nltk.classify.accuracy(classifier,testing_set))*100) Naive Bayes Algo accuracy: 68.82530120481928 1from nltk.classify.scikitlearn import SklearnClassifier 1from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB 123MNB_classifier = SklearnClassifier(MultinomialNB())MNB_classifier.train(training_set)print(\"MNB_classifier accuracy percent:\",(nltk.classify.accuracy(MNB_classifier,testing_set))*100) MNB_classifier accuracy percent: 67.46987951807229 1234# 这段代码有问题，不可以运行GNB_classifier = SklearnClassifier(GaussianNB())GNB_classifier.train(training_set)print(\"GNB_classifier accuracy percent:\",(nltk.classify.accuracy(GNB_classifier,testing_set))*100) 123BNB_classifier = SklearnClassifier(BernoulliNB())BNB_classifier.train(training_set)print(\"BNB_classifier accuracy percent:\",(nltk.classify.accuracy(BNB_classifier,testing_set))*100) BNB_classifier accuracy percent: 68.97590361445783 12from sklearn.linear_model import LogisticRegression, SGDClassifierfrom sklearn.svm import SVC, LinearSVC, NuSVC 123LogisticRegression_classifier = SklearnClassifier(LogisticRegression())LogisticRegression_classifier.train(training_set)print(\"LogisticRegression_classifier accuracy percent:\",(nltk.classify.accuracy(LogisticRegression_classifier,testing_set))*100) LogisticRegression_classifier accuracy percent: 70.78313253012048 123SGDClassifier_classifier = SklearnClassifier(SGDClassifier())SGDClassifier_classifier.train(training_set)print(\"SGDClassifier_classifier accuracy percent:\",(nltk.classify.accuracy(SGDClassifier_classifier,testing_set))*100) SGDClassifier_classifier accuracy percent: 66.1144578313253 123SVC_classifier = SklearnClassifier(SVC())SVC_classifier.train(training_set)print(\"SVC_classifier accuracy percent:\",(nltk.classify.accuracy(SVC_classifier,testing_set))*100) SVC_classifier accuracy percent: 49.096385542168676 123LinearSVC_classifier = SklearnClassifier(LinearSVC())LinearSVC_classifier.train(training_set)print(\"LinearSVC_classifier accuracy percent:\",(nltk.classify.accuracy(LinearSVC_classifier,testing_set))*100) LinearSVC_classifier accuracy percent: 70.48192771084338 123NuSVC_classifier = SklearnClassifier(NuSVC())NuSVC_classifier.train(training_set)print(\"NuSVC_classifier accuracy percent:\",(nltk.classify.accuracy(NuSVC_classifier,testing_set))*100) NuSVC_classifier accuracy percent: 69.7289156626506 12from nltk.classify import ClassifierIfrom statistics import mode 12345678910111213141516171819class VoteClassifier(ClassifierI): def __init__(self, *classifiers): self._classifiers = classifiers def classify(self, features): votes = [] for c in self._classifiers: v = c.classify(features) votes.append(v) return mode(votes) def confidence(self,features): votes = [] for c in self._classifiers: v = c.classify(features) votes.append(v) choice_votes = votes.count(mode(votes)) conf = choice_votes / len(votes) return conf 12345678910voted_classifier = VoteClassifier(classifier, MNB_classifier, BNB_classifier, LogisticRegression_classifier, SGDClassifier_classifier, #SVC_classifier,视频中没有这个，况且如果不注释掉就会报统计错误，说有两个相同的值。 #如： http://blog.csdn.net/dongfuguo/article/details/50163757 中 mode错误一般 LinearSVC_classifier, NuSVC_classifier)print(\"voted_classifier accuracy percent:\",(nltk.classify.accuracy(voted_classifier,testing_set))*100) voted_classifier accuracy percent: 69.42771084337349 1print(\"Classification:\",voted_classifier.classify(testing_set[0][0]),\"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100) Classification: pos Confidence %: 100.0 19. Sentiment Analysis Module()12all_words = []documents = [] 1allowed_word_types = [\"J\"] 123456789101112131415for p in short_pos.split('\\n'): documents.append((p,\"pos\")) words = nltk.word_tokenize(p) pos = nltk.pos_tag(words) for w in pos: if w[1][0] in allowed_word_types: all_words.append(w[0].lower())for p in short_neg.split('\\n'): documents.append((p,\"neg\")) words = nltk.word_tokenize(p) neg = nltk.pos_tag(words) for w in neg: if w[1][0] in allowed_word_types: all_words.append(w[0].lower()) 1import pickle 提醒一下：直接运行以下的cell 会报错，应该先创建一个pickled_algos文件夹，然后再运行cell 保存文档 123save_documents = open('pickled_algos/documents.pickle',\"wb\")pickle.dump(documents, save_documents)save_documents.close() 保存文本特征 12all_words = nltk.FreqDist(all_words)word_features = list(all_words.keys())[:5000] 123save_word_features = open('pickled_algos/word_features5k.pickle',\"wb\")pickle.dump(word_features,save_word_features)save_word_features.close() 保存朴素贝叶斯算法 123save_classifier = open(\"pickled_algos/originalnaivebayes5k.pickle\",\"wb\")pickle.dump(classifier,save_classifier)save_classifier.close() 保存MultinomialNB算法 123save_classifier = open(\"pickled_algos/MNB_classifier5k.pickle\",\"wb\")pickle.dump(MNB_classifier,save_classifier)save_classifier.close() 保存BernoulliNB算法 123save_classifier = open(\"pickled_algos/BNB_classifier5k.pickle\",\"wb\")pickle.dump(BNB_classifier,save_classifier)save_classifier.close() 保存LogisticRegression算法 123save_classifier = open(\"pickled_algos/LogisticRegression_classifier5k.pickle\",\"wb\")pickle.dump(LogisticRegression_classifier,save_classifier)save_classifier.close() 保存LinearSVC算法 123save_classifier = open(\"pickled_algos/LinearSVC_classifier5k.pickle\",\"wb\")pickle.dump(LinearSVC_classifier,save_classifier)save_classifier.close() 保存SGDC算法 123save_classifier = open(\"pickled_algos/SGDClassifier_classifier5k.pickle\",\"wb\")pickle.dump(SGDClassifier_classifier,save_classifier)save_classifier.close() 12345voted_classifier = VoteClassifier(classifier, LinearSVC_classifier, MNB_classifier, BNB_classifier, LogisticRegression_classifier) 1234def sentiment(text): feats = find_features(text) return voted_classifier.classify(feats) 最终我们编写的模块长成这个样子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#File: sentiment_mod.py 只是一个文件名而已，可以按照自己的想法取，但应做到见名知意import nltkimport randomfrom nltk.classify.scikitlearn import SklearnClassifierimport picklefrom sklearn.naive_bayes import MultinomialNB,BernoulliNBfrom sklearn.linear_model import LogisticRegression,SGDClassifierfrom sklearn.svm import SVC, LinearSVC,NuSVCfrom nltk.classify import ClassifierIfrom statistics import modefrom nltk.tokenize import word_tokenize# 以上许多类模块虽然在代码中看似并没有用到，可是在用pickle还原为相关实例在被外部调用执行的时候还是需要的。# 这里由于我们之前已经训练好了几个分类器，并且已经将文档内容和文本特征等通过pickle持久化保存起来了，所以在此模块中直接用pickle还原就可以直接拿来用了，而不是再次训练。# 并且该模块仅当同一路径下的pickled_algos文件夹及里面的各pickle文件同时存在时才可以正常使用，当然，项目中也要导入本模块需要使用的一些基础模块，如nltk等等。class VoteClassifier(ClassifierI): def __init__(self, *classifiers): self._classifiers = classifiers def classify(self, features): votes = [] for c in self._classifiers: v = c.classify(features) votes.append(v) return mode(votes) def confidence(self,features): votes = [] for c in self._classifiers: v = c.classify(features) votes.append(v) choice_votes = votes.count(mode(votes)) conf = choice_votes / len(votes) return conf documents_f = open('pickled_algos/documents.pickle',\"rb\")documents = pickle.load(documents_f)documents_f.close()word_features5k_f = open('pickled_algos/word_features5k.pickle',\"rb\")word_features = pickle.load(word_features5k_f)word_features5k_f.close()def find_features(document): words = nltk.word_tokenize(document) features = &#123;&#125; for w in word_features: features[w] = (w in words) return featuresopen_file = open(\"pickled_algos/originalnaivebayes5k.pickle\",\"rb\")classifier = pickle.load(open_file)open_file.close()open_file = open(\"pickled_algos/MNB_classifier5k.pickle\",\"rb\")MNB_classifier = pickle.load(open_file)open_file.close()open_file = open(\"pickled_algos/BNB_classifier5k.pickle\",\"rb\")BNB_classifier = pickle.load(open_file)open_file.close()open_file = open(\"pickled_algos/LogisticRegression_classifier5k.pickle\",\"rb\")LogisticRegression_classifier = pickle.load(open_file)open_file.close()open_file = open(\"pickled_algos/LinearSVC_classifier5k.pickle\",\"rb\")LinearSVC_classifier = pickle.load(open_file)open_file.close()open_file = open(\"pickled_algos/SGDClassifier_classifier5k.pickle\",\"rb\")SGDClassifier_classifier = pickle.load(open_file)open_file.close()voted_classifier = VoteClassifier( classifier, LinearSVC_classifier, MNB_classifier, BNB_classifier, LogisticRegression_classifier)def sentiment(text): feats = find_features(text) return voted_classifier.classify(feats),voted_classifier.confidence(feats)# save me as sentiment_mod.py 下面来使用一下： 12345import sentiment_mod as sprint(s.sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"))print(s.sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\")) (&#39;pos&#39;, 1.0) (&#39;neg&#39;, 1.0) 好吧，接下来的实践要使用Twitter 创建APP，可能还要使用个人网站，有点麻烦，所以接下来我只是看了看并没有照着实践。总之，在这一系列的跟着敲代码的过程中，自己初步建立起了很浅的自然语言处理的概念~ 20. Twitter Sentiment Analysis()21. Graphing Live Twitter Sentiment()12","categories":[{"name":"计算机","slug":"计算机","permalink":"http://www.iamlightsmile.com/categories/计算机/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://www.iamlightsmile.com/tags/Python/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://www.iamlightsmile.com/tags/自然语言处理/"},{"name":"NLTK","slug":"NLTK","permalink":"http://www.iamlightsmile.com/tags/NLTK/"}]},{"title":"英文表达与写作练习宣告","slug":"英文表达与写作练习宣告","date":"2018-03-06T10:50:11.000Z","updated":"2019-01-07T03:20:27.237Z","comments":true,"path":"articles/英文表达与写作练习宣告/","link":"","permalink":"http://www.iamlightsmile.com/articles/英文表达与写作练习宣告/","excerpt":"汉语版:为了更好的提高自己的英文表达和写作能力，我决定在以后的技术博客中尽可能的使用英文。仅在此声明！ 我的英文版:In order to improve my English express and writing ablility,I decide that I’ll use English to write my tecnoligy blogs as possible.The statesment is above. 有道翻译英语版:In order to improve my English expression and writing ability, I decided to use English as much as possible in future technical blogs.Only in this statement!","text":"汉语版:为了更好的提高自己的英文表达和写作能力，我决定在以后的技术博客中尽可能的使用英文。仅在此声明！ 我的英文版:In order to improve my English express and writing ablility,I decide that I’ll use English to write my tecnoligy blogs as possible.The statesment is above. 有道翻译英语版:In order to improve my English expression and writing ability, I decided to use English as much as possible in future technical blogs.Only in this statement! 学习笔记: expression :n,表达 alility:n,能力 decided to do something:决定做某事 as much as possible:尽可能多的 technical:adj,技术的","categories":[],"tags":[]},{"title":"常见10种自然语言处理技术（转载）","slug":"常见10种自然语言处理技术（转载）","date":"2018-03-05T07:28:41.000Z","updated":"2019-01-07T03:15:31.431Z","comments":true,"path":"articles/常见10种自然语言处理技术（转载）/","link":"","permalink":"http://www.iamlightsmile.com/articles/常见10种自然语言处理技术（转载）/","excerpt":"原文 该作者也是翻译的外文，英文原文链接 引言自然语言处理（NLP）是一种艺术与科学的结合，旨在从文本数据中提取信息。在它的帮助下，我们从文本中提炼出适用于计算机算法的信息。从自动翻译、文本分类到情绪分析，自然语言处理成为所有数据科学家的必备技能之一。 常见的10个NLP任务如下： 词干提取 词形还原 词向量化 词性标注 命名实体消岐 命名实体识别 情感分析 文本语义相似分析 语种辨识 文本总结","text":"原文 该作者也是翻译的外文，英文原文链接 引言自然语言处理（NLP）是一种艺术与科学的结合，旨在从文本数据中提取信息。在它的帮助下，我们从文本中提炼出适用于计算机算法的信息。从自动翻译、文本分类到情绪分析，自然语言处理成为所有数据科学家的必备技能之一。 常见的10个NLP任务如下： 词干提取 词形还原 词向量化 词性标注 命名实体消岐 命名实体识别 情感分析 文本语义相似分析 语种辨识 文本总结 以下将详细展开：1.词干提取什么是词干提取？词干提取是将词语去除变化或衍生形式，转换为词干或原型形式的过程。词干提取的目标是将相关词语还原为同样的词干，哪怕词干并非词典的词目。例如，英文中: beautiful和beautifully的词干同为beauti Good,better和best 的词干分别为good,better和best。 相关论文：Martin Porter的波特词干算法原文 相关算法：Porter2词干算法的Python实现 程序实现：Porter2算法做词干提取的代码： 123#!pip install stemmingfrom stemming.porter2 import stemstem(\"casually\") 2. 词形还原什么是词形还原？ 词形还原是将一组词语还原为词源或词典的词目形式的过程。还原过程考虑到了POS问题，即词语在句中的语义，词语对相邻语句的语义等。例如，英语中： beautiful和beautifully被分别还原为beautiful和beautifully。 good, better和best被分别还原为good, good和good 相关论文1: 这篇文章详细讨论了词形还原的不同方法。想要了解传统词形还原的工作原理必读。 相关论文2: 这篇论文非常出色，讨论了运用深度学习对变化丰富的语种做词形还原时会遇到的问题。 数据集: 这里是Treebank-3数据集的链接，你可以使用它创建一个自己的词形还原工具。 程序实现：下面给出了在spacy上的英语词形还原代码 1234567#!pip install spacy#python -m spacy download enimport spacynlp=spacy.load(\"en\")doc=\"good better best\"for token in nlp(doc): print(token,token.lemma_) 3. 词向量化什么是词向量化？词向量化是用一组实数构成的向量代表自然语言的叫法。这种技术非常实用，因为电脑无法处理自然语言。词向量化可以捕捉到自然语言和实数间的本质关系。通过词向量化，一个词语或者一段短语可以用一个定维的向量表示，例如向量的长度可以为100。 例如：Man这个词语可以用一个五维向量表示。 这里的每个数字代表了词语在某个特定方向上的量级。 相关博文：这篇文章详细解释了词向量化 相关论文：这篇论文解释了词向量化的细节。深入理解词向量化必读。 相关工具：这是个基于浏览器的词向量可视化工具。 预训练词向量：这里有一份facebook的预训练词向量列表，包含294种语言。 这里可以下载google news的预训练词向量。 1234#!pip install gensimfrom gensim.models.keyedvectors import KeyedVectorsword_vectors=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)word_vectors['human'] 程序实现：这段代码可以用gensim训练你自己的词向量 12sentence=[['first','sentence'],['second','sentence']]model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4) 4.词性标注什么事词性标注？简单来说，词性标注是对句子中的词语标注为名字、动词、形容词、副词等的过程。例如，对句子“Ashok killed the snake with a stick”，词性标注会识别： Ashok 代词 killed 动词 the 限定词 snake 名词 with 连词 a 限定词 stick 名词 . 标点 论文1：choi aptly的这篇《The Last Gist to theState-of-the-Art 》介绍了一种叫动态特征归纳的新方法。这是目前词性标注最先进的方法。 论文2：这篇文章介绍了通过隐马尔科夫模型做无监督词性标注学习的方法。 程序实现：这段代码可以在spacy上做词性标注 123456#!pip install spacy#!python -m spacy download en nlp=spacy.load('en')sentence=\"Ashok killed the snake with a stick\"for token in nlp(sentence): print(token,token.pos_) 5. 命名实体消歧什么是命名实体消岐？命名实体消岐是对句子中的提到的实体识别的过程。例如，对句子“Apple earned a revenue of 200 Billion USD in 2016”，命名实体消岐会推断出句子中的Apple是苹果公司而不是指一种水果。一般来说，命名实体要求有一个实体知识库，能够将句子中提到的实体和知识库联系起来。 论文1：Huang的这篇论文运用了基于深度神经网络和知识库的深层语义关联模型，在命名实体消岐上达到了领先水平。 论文2：Ganea and Hofmann的这篇文章运用了局部神经关注模型和词向量化，没有人为设置特征。 6. 命名实体识别体识别是识别一个句子中有特定意义的实体并将其区分为人名，机构名，日期，地名，时间等类别的任务。例如，一个NER会将一个这样的句子： “Ram of Apple Inc. travelled to Sydney on 5th October 2017” 返回如下的结果： RamofApple ORGInc. ORGtravelledtoSydney GPEon5th DATEOctober DATE2017 DATE 这里，ORG代表机构组织名，GPE代表地名。 然而，当NER被用在不同于该NER被训练的数据领域时，即使是最先进的NER也往往表现不佳。 论文：这篇优秀的论文使用双向LSTM（长短期记忆网络）神经网络结合监督学习和非监督学习方法，在4种语言领域实现了命名实体识别的最新成果。 程序实现：以下使用spacy执行命名实体识别。 1234import spacynlp=spacy.load('en')sentence=\"Ram of Apple Inc. travelled to Sydney on 5th October 2017\"for token in nlp(sentence): print(token, token.ent_type_) 7. 情感分析什么是情感分析？情感分析是一种广泛的主观分析，它使用自然语言处理技术来识别客户评论的语义情感，语句表达的情绪正负面以及通过语音分析或书面文字判断其表达的情感等等。例如： “我不喜欢巧克力冰淇淋”—是对该冰淇淋的负面评价。 “我并不讨厌巧克力冰激凌”—可以被认为是一种中性的评价。 从使用LSTMs和Word嵌入来计算一个句子中的正负词数开始，有很多方法都可以用来进行情感分析。 博文1：本文重点对电影推文进行情感分析。 博文2：本文重点对印度金奈洪水期间的推文进行情感分析。 论文1：本文采用朴素贝叶斯的监督学习方法对IMDB评论进行分类。 论文2：本文利用LDA的无监督学习方法来识别用户生成评论的观点和情感。本文在解决注释评论短缺的问题上表现突出。 资料库：这是一个很好的包含相关研究论文和各种语言情感分析程序实现的资料库。 数据集1：多域情感数据集版本2.0 数据集2：Twitter情感分析数据集 竞赛：一个非常好的比赛，你可以检查你的模型在烂番茄电影评论的情感分析任务中的表现。 8. 语义文本相似度什么是语义文本相似度分析？语义文本相似度分析是对两段文本的意义和本质之间的相似度进行分析的过程。注意，相似性与相关性是不同的。 例如： 汽车和公共汽车是相似的，但是汽车和燃料是相关的。 论文1：本文详细介绍了文本相似度测量的不同方法。是一篇可以一站式了解目前所有方法的必读文章。 论文2：本文介绍了用CNN神经网络去比对两个短文本。 论文3：本文利用Tree-LSTMs方法得到了文本的语义相关和语义分类的最新成果。 9. 语言识别什么是语言识别？语言识别指的是将不同语言的文本区分出来。其利用语言的统计和语法属性来执行此任务。语言识别也可以被认为是文本分类的特殊情况。 博文：在这篇由fastText撰写的博文中介绍了一种新的工具，其可以在1MB的内存使用情况下识别170种语言。 论文1：本文讨论了285种语言的7种语言识别方法。 论文2：本文描述了如何使用深度神经网络来实现自动语言识别的最新成果。 10. 文本摘要什么是文本摘要？文本摘要是通过识别文本的重点并使用这些要点创建摘要来缩短文本的过程。文本摘要的目的是在不改变文本含义的前提下最大限度地缩短文本。 论文1：本文描述了基于神经注意模型的抽象语句梗概方法。 论文2：本文描述了使用序列到序列的RNN在文本摘要中达到的最新结果。 资料库：Google Brain团队的这个资料库拥有使用为文本摘要定制的序列到序列模型的代码。该模型在Gigaword数据集上进行训练。 应用程序：Reddit的autotldr机器人使用文本摘要来梗概从文章到帖子的各种评论。这个功能在Reddit用户中非常有名。 程序实现：以下是如何用gensim包快速实现文本摘要。 123fromgensim.summarization import summarizesentence=\"Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the information of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization.\"summarize(sentence) 结束语以上所有是最流行的NLP任务以及相关的博客、研究论文、资料库、应用等资源。 祝你学习愉快！","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://www.iamlightsmile.com/categories/自然语言处理/"}],"tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://www.iamlightsmile.com/tags/自然语言处理/"}]},{"title":"朕的感情史-编程语言篇！","slug":"朕的感情史-编程语言篇！","date":"2018-03-05T01:16:32.000Z","updated":"2019-01-07T03:17:00.492Z","comments":true,"path":"articles/朕的感情史-编程语言篇！/","link":"","permalink":"http://www.iamlightsmile.com/articles/朕的感情史-编程语言篇！/","excerpt":"对于每一个有理想有追求的程序猿而言，他们可以没有对象，但是不能没有自己的喜欢的语言。 古言道：兄弟诚可贵，老婆价更高。若为编程故（指死亡之die~），躲也躲不掉。 好吧，接下来我要正式介绍一下朕的后宫！","text":"对于每一个有理想有追求的程序猿而言，他们可以没有对象，但是不能没有自己的喜欢的语言。 古言道：兄弟诚可贵，老婆价更高。若为编程故（指死亡之die~），躲也躲不掉。 好吧，接下来我要正式介绍一下朕的后宫！ 第一任：C++ 大一学习编程语言最开始上手的就是C++，不过学了半天也没学出个所以然来，经常挣扎于混杂的概念之中，却又缺少电脑无法有效开展实战编程。后来随着考试结束，我和她的感情也就慢慢的越来越淡了。 好吧，不过呢，作为系统编程和游戏开发的无可替代的语言，她的高性能与开发效率综合而言目前怕是所有所有的都比不上。 大部分企业面试笔试好像都会问到C++，感觉不会点C++真的不配说自己是程序员，但是我觉得人生苦短，何必找死呢。感觉C++是我等屁民不可轻易染指的高冷女神。 第二任：C 为了更好的学习C++，曾经又跑去学了C，因为我觉得她是C++的姐姐吧。不过还是由于驱动力不足之类的原因吧，最终也没能学出个卵子，不过也在某种程度上理解了C++相对于C而言做的改进，例如多态、名称空间、类等对于结构化的编程还是挺有用的吧。 C语言也可以做的很吊，但是由于语言的设计里本身并没有许多高级概念，所以用起来不是很直接方便，搞系统编程或者其他对效率要求极高的才会用吧。看上去没有C++的高冷，但是也不好追，并且语言特性相对低级，自己本身不太喜欢。 第三任：Java Java这门语言其实真的是相当不错的。如所有的类都有一个共同的基类，只能继承自一个父类，其他的通过接口来实现，相对于C++的名称空间来说要好不少。 Java的语言世界里，程序总是在一个个实例化的类或类本身中的属性和方法中穿梭，某种程度上来说其实挺好的。相比C++而言，普通的人很容易上手并能培养起初步良好的编程习惯。 我在Java实验课程中手撸一个什么系统，在此过程中，初步培养起了软件编程的极初始的经验体会。后来学习Android过程中，由于也要用到Java，所以功底还算可以。 但是在定义和使用的过程中，还是会感觉到比较冗杂，当然规矩多了某种程度上也算好事。那些之后在JVM之上出现的其他语言，也是同样在JVM上运行程序，但是相对而言可能更简洁、更少束缚、更强表现力，如Scala、Kotlin等等。 曾经问我家邻居学软件怎么入手，他说你就学Java吧。Java语言在企业级应用系统开发中的主导地位怕是很难被动摇的。 无论什么时候，Java都是一个值得选择拥抱的小姐姐。 第四任：C# C#开发最初的目的可能是微软用于应对Java吧。 C#这门语言在面向对象的语言家族中还是很不错的，某些设计理念比Java还要先进。然而正如以前的VBScript等更多的是被捆绑在Windows操作系统中，不开源，在某种程度上也算束缚了它的发展进步。虽说现在开源了，但是由于Java已经占领了大部分市场，C#的份额还是挺小的了。 因课结缘，课完缘尽。定位和Java类似，但是对Java更有好感。 第五任：Lua 不得不说，Lua真的是个好语言，只是之前没怎么好好学，并且它上手极为简单容易。现在的话，很少接触，因为基本上用不到。对于它的C源码还是有必要看一下的。感觉很有好感的小妹妹。 第六任：Python Python的设计哲学现在看来真的是极好，这些年的发展壮大足以说明一切，在大部分编程应用领域里几乎都有了它的身影。极强的代码阅读性，上手很容易。于我而言，她现在是一个贤妻良母型的。 第七任：Ruby Python和Ruby各领风骚，许多爱好者各爱各的好。Ruby我以前也学过一下下，Ruby好像是基于对象的，里面啥都是对象。相对而言，用法超级灵活，而且比较简洁炫酷，但是可能因为用的少，我看着Ruby代码好像阅读起来不是很爽，比Python而言差远了。开发效率极高，Ruby on Rails嘛，但是好像项目做大之后就会受限于它的运行效率。现在也早就分了。感觉Ruby和我三观不符，私人感觉Ruby是一个比较追求时尚新颖的女生，比较开放外向。 第八任：Scala 我曾经学习过很短时间的Scala，网上的说法是学习起来很复杂，我当时感觉还不错，后来因为用不到也没在学了，可能还是不太感兴趣吧。也是一个不错的小姐姐，但是如果太专情于此，怕是不好吃饭。。。 第九任：aardio 最爱，没有之一。虽然它生于Windows，长于Windows，最终也将死于Windows，但是这还是无法阻挠我对它的喜爱，况且正是由于仅限于Windows平台，她的美才更加表现出来。她的不开源我也理解，是我我也不开源，正是一鹤校长非常爱她，才不允许其他误解开源精神的染指搞事情。 动态语言，快速开发，名称空间、类，句法灵活，调用简单，几行代码就可以实现复杂的功能，并且很方便的就可以开发出控制台、桌面、网络、后台程序，可以方便与其他语言实现互相调用。 自从无意间了解到这门语言之后，感觉一下子就被吸引到了，因为她当时完全符合我对编程语言的所有需求，他妈的当时把老纸激动坏了，差点一宿没睡着。当时学了编程两年多，都写不出个什么桌面程序来，用Java写的太丑，用C#开发的还得运行在.Net框架上。一点成就感都没有，我对可视化的界面还是很执着的。 后来花钱报了班，看视频学习，有时会在群里探讨问题，有时会直接向校长询问，感觉真的很值。后来我数据库实验、编译原理实验、操作系统实验等等全都是用的aardio语言，因为用起来实在是太爽了。 于我而言，aardio是我的初恋，是我第一门喜欢上的语言，虽然现在由于没有需求已经暂停和她失去了联系，但是我会永远喜欢她！ 第十任：Processing Processing这门语言用来做艺术设计以及自然模拟都是很不错的，并且跨平台，可以生成可执行程序，本身使用Java语言写的，同时也有JavaScript版和Python版，也是用不到所以没在用了。在我看来，像是一个很可爱的萝莉。 第十一任：JavaScript 个人感觉，JavaScript最屌了。它的语言哲学相对来说和我的三观最为符合，原型继承，闭包等等概念都很不错。随着es6的出现，它的语法规范感觉十分高级，个人十分喜欢，在某种程度上，部分设计已经超越了aardio（感觉aardio是集许多好的语言的特性于一身）。事件驱动、非阻塞I/O、回调的NodeJs感觉也很棒。是一个我非常想深入学习的语言，对我来说是一个极具诱惑力的女神。 第十二任：Racket Racket是Scheme的一种方言，而Scheme则是Lisp的一种方言，语言中处处都是函数式编程，感觉非常新颖且高大上，但是程序复杂起来了之后阅读体验极差，远远比不上Python。接下来打算有时间接触一下下，多吸取里面的先进思想。 第十三任：Julia 2018/3/5刚接触Julia语言，感觉真的挺厉害的，元编程，并行计算，超高性能，语法像Python、用法像Lisp、速度像C，渍渍，真是厉害了~ 第十四任：？ 第十五任：？ 第十六任：？ 第十七任：？ …… 接下来的语言还没出现或者还没接触到，可能今后才会冒出来，或者是我自己设计定义实现的吧……","categories":[],"tags":[]},{"title":"Python学习笔记（1）骚操作之Unicode编码","slug":"Python学习笔记（1）骚操作之Unicode编码","date":"2018-03-04T11:53:42.000Z","updated":"2019-01-07T03:13:50.024Z","comments":true,"path":"articles/Python学习笔记（1）骚操作之Unicode编码/","link":"","permalink":"http://www.iamlightsmile.com/articles/Python学习笔记（1）骚操作之Unicode编码/","excerpt":"Python3内部使用的是Unicode编码，所以在变量定义的时候或许可以搞点事情。","text":"Python3内部使用的是Unicode编码，所以在变量定义的时候或许可以搞点事情。 例如创建一个文件,名为：人.py，里面的代码是：12class 人(object): 性别 = \"男\" 然后同目录下创建一个test.py文件，里面内容为：123from 人 import 人李明 = 人()print(李明.性别) 结果输出：男，是不是有点骚~","categories":[{"name":"Python","slug":"Python","permalink":"http://www.iamlightsmile.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://www.iamlightsmile.com/tags/Python/"}]},{"title":"《和谐辩证法》笔记与心得（3）吾之平衡辩证法","slug":"《和谐辩证法》笔记与心得（3）吾之平衡辩证法","date":"2018-03-04T01:14:31.000Z","updated":"2019-01-07T03:12:07.745Z","comments":true,"path":"articles/《和谐辩证法》笔记与心得（3）吾之平衡辩证法/","link":"","permalink":"http://www.iamlightsmile.com/articles/《和谐辩证法》笔记与心得（3）吾之平衡辩证法/","excerpt":"吾之所见为，宇宙的发展其实就是能量间从不平衡状态到平衡状态的迁移罢了。 事物的稳定与不稳定两种状态可理解为运动之趋势变化抑或不变耳。 事物总是从不平衡的状态朝向平衡的状态运动和发展的，而之所以运动不止，发展不断，则是由于外界的作用又使得事物可能从平衡又趋于不平衡。","text":"吾之所见为，宇宙的发展其实就是能量间从不平衡状态到平衡状态的迁移罢了。 事物的稳定与不稳定两种状态可理解为运动之趋势变化抑或不变耳。 事物总是从不平衡的状态朝向平衡的状态运动和发展的，而之所以运动不止，发展不断，则是由于外界的作用又使得事物可能从平衡又趋于不平衡。 事物内部或事物之间的平衡或不平衡的状态实际上就是指在没有外物的作用下，事物之间或事物内部各要素之间能否保持稳定而不变化的状态，对应的是时刻，状态或趋势。而运动或静止对应的是时间，过程或阶段。平衡或不平衡是决定事物或事物之间运动以相互作用的决定因素，而运动与静止则是从不平衡态向平衡态迁移或平衡态迁移到不平衡态的实现过程。 然而吾仍不解的是宇宙之始态为何，宇宙之终态又为何，同时为什么能量非要从不平衡态迁移到平衡态。 举例如：生产关系之调整是生产关系与生产力从不平衡到平衡的过程；生产力之进步是生产力与人类生存发展不平衡到平衡的过程；人类之生存发展是理想与现实之不平衡到平衡的过程；但理想之追求非平衡，而是贪念和占有，故此之平衡终为不可持续之平衡，故马克思理想之共产主义社会终究不会实现与持久，缘何？人类与大自然和环境之极大不平衡也。当然，最好的结果自然是人类延续，欲望无休止，继续开采资源直至到其他星球为止。最坏的结果当为人类自掘坟墓、自取灭亡。","categories":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/categories/哲学/"}],"tags":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/tags/哲学/"}]},{"title":"《和谐辩证法》笔记与心得（2）能量供求之己见","slug":"《和谐辩证法》笔记与心得（2）能量供求之己见","date":"2018-03-04T01:08:20.000Z","updated":"2019-01-07T03:11:39.169Z","comments":true,"path":"articles/《和谐辩证法》笔记与心得（2）能量供求之己见/","link":"","permalink":"http://www.iamlightsmile.com/articles/《和谐辩证法》笔记与心得（2）能量供求之己见/","excerpt":"本书作者以为存在两种性质不同的能量作用方式，即：1.单向能量供求关系2.双向能量供求关系。同时存在三种不同的能量供求结果，即：1.能量相容2.能量冲突3.能量中立。 单向能量供求关系可形成能量链，链条上的每一节都存在能量的转换与运动形式的转换。 而双向能量供求关系则表现为单两者之间的相互吸引或者相互对抗、相互满足或者相互伤害。 然吾不以为然，因为事物之间的彼此联系实际上就是说存在着相互作用的关系，彼此可以影响对方的运动和发展。","text":"本书作者以为存在两种性质不同的能量作用方式，即：1.单向能量供求关系2.双向能量供求关系。同时存在三种不同的能量供求结果，即：1.能量相容2.能量冲突3.能量中立。 单向能量供求关系可形成能量链，链条上的每一节都存在能量的转换与运动形式的转换。 而双向能量供求关系则表现为单两者之间的相互吸引或者相互对抗、相互满足或者相互伤害。 然吾不以为然，因为事物之间的彼此联系实际上就是说存在着相互作用的关系，彼此可以影响对方的运动和发展。 所以可以说“凡事皆为众因之果，万物齐作众果之因”。 吾以为凡世间种种因缘之变化，皆能量相互作用之结果。能量自然成链，无限接力也。然天下间万事万物皆作能量之一小环，有供有求，无时无刻不相互作用耳。作者之见，实为取何其浩大之细微，沧海之一粟，往来之一瞬。然后取关联以相接，得知为链，然作者岂不明乎：双向能量供求关系岂非特殊两者之链而成环哉！ 如下图：,此即细微也。圆圈指能量体或物质外显或抽象之概念，而这指向圆环的箭头其实也是外界相加于圆圈之能量，而自圆环外指的箭头当为圆环向外界其他能量体施加之能量。 本书作者之单向能量链为：。 而双向能量供求关系为：。 岂不过度抽象与特殊化乎。所谓单向或双向在我看来无非作用程度不同罢了。 所谓“大道至简”，因而绝非有两种存在。 同时参考吾之运动与静止理解，可得，万事万物，都会直接或者间接地影响和改变某一事物内在的结构和内容或其作为要素与整体及整体的其他要素之关系，从而推动事物的运动和发展。 若拿函数方程来粗浅理解，如三个变量X，Y，Z，有函数方程组：。 三个方程三个未知量，若是再知道初态X0 、Y0、Z0，那么从逻辑上或许总是可以得到依次的相互对应的X，Y，Z的值的。 然而现实是现实中的未知量实在是太多了，同时函数关系式也太复杂了，所以理论来说实际上是不可能得到准确的解的。然而我们通过观察现实中某些量或许是线性的，也就是说两者之间是线性联系或者呈现出其他的联系函数，正是在这个基础上，我们的认识才更加深化和进步。 故而可理解为世间或宇宙或许为能量之一个奇大无比之网，而我们万物无非其中一个小小结罢了。 同时关于三种能量供求结果，我的想法是可以从物理学上机械能做功的角度。 （机械宏观运动中）物体运动有其方向，即速度是一个矢量，而外力平行于该方向上的分力，若方向一致则做正功，推动事物原有方向上的发展。若相反，则做负功。阻碍事物原有方向的发展，甚至使方向发生180度变化。而垂直于运动方向上的力不做功，但仍有改变方向之效用，因而能量相容即外界之能量作用于事物产生的运动状态与物质本身、当前运动状态相一致则为相容。相反则为冲突。垂直则为中立，并非无用，可起到改变方向之作用，但中立应该是特殊状态，且仅一瞬的状态，因为运动方向一旦改变而作用方向不变，即产生非中立关系，因而是不可持续的，若是站在物理力学之坐公交度，则不存在此状态。","categories":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/categories/哲学/"}],"tags":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/tags/哲学/"}]},{"title":"《和谐辩证法》笔记与心得（1）物质运动与静止之己见","slug":"《和谐辩证法》笔记与心得（1）物质运动与静止之己见","date":"2018-03-04T01:07:10.000Z","updated":"2019-01-07T03:11:26.362Z","comments":true,"path":"articles/《和谐辩证法》笔记与心得（1）物质运动与静止之己见/","link":"","permalink":"http://www.iamlightsmile.com/articles/《和谐辩证法》笔记与心得（1）物质运动与静止之己见/","excerpt":"对应p35吾以为马克思之运动与静止之观点与本书作者之运动与静止之观点均有偏颇之处，或者各有所见。然吾之角度或许不同于二者乎。 马克思云运动之绝对而静止之相对，而本书作者云运动与静止均绝对。恩格斯之运动绝对体现在：1.任何事物都是运动的2.任何事物在任何时候都是运动的。 而本书作者之运动与静止之界限取决于事物是否在特定能量下作为一个整体的运动变化，同时还伴随着其特定能量的释放。","text":"对应p35吾以为马克思之运动与静止之观点与本书作者之运动与静止之观点均有偏颇之处，或者各有所见。然吾之角度或许不同于二者乎。 马克思云运动之绝对而静止之相对，而本书作者云运动与静止均绝对。恩格斯之运动绝对体现在：1.任何事物都是运动的2.任何事物在任何时候都是运动的。 而本书作者之运动与静止之界限取决于事物是否在特定能量下作为一个整体的运动变化，同时还伴随着其特定能量的释放。 而吾之运动静止观在于两方面：1.事物内部各要素之间联系是否发生改变，事物的结构与内容是否发生变化2.事物作为一个整体的要素是否与其他要素之间的联系是否发生改变，以及因此导致的整体的结构及内容的变化。 然而事物作为一个整体而言，其要素的划分方式并非绝对的，而是相对的，同时该事物作为一个要素而言，可以和别的事物组成一整体，而这种组合也是多变的，多样的。具体含义为：一个事物根据不同的角度可以被划分为多种不同的要素的有机组合，如abcd可以分为a、b、c、d或者ab、cd或者abc、d等等，同时这一事物可以和其他事物组成不同的多样的整体，如a和b可以组合成ab而a和c可以组合成ac，此外同为ab与ba尽管内容相同，但内部要素之间结构不同，因而算是不同事物。 故吾看来，事物之运动或静止取决于以上两方面，同时他们的根本属性并非绝对性而在于其相对性，运动是相对的，静止也是相对的。这里的相对并非事物与事物之间的相对，而是事物与角度之间的相对。","categories":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/categories/哲学/"}],"tags":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/tags/哲学/"}]},{"title":"《智慧之根》笔记与心得（2）事物之结构浅解","slug":"《智慧之根》笔记与心得（2）事物之结构浅解","date":"2018-03-04T00:47:56.000Z","updated":"2019-01-07T03:12:34.831Z","comments":true,"path":"articles/《智慧之根》笔记与心得（2）事物之结构浅解/","link":"","permalink":"http://www.iamlightsmile.com/articles/《智慧之根》笔记与心得（2）事物之结构浅解/","excerpt":"&emsp;&emsp;吾以为，事物之结构该当如下：（注意：这里的事物应当是能量的物质外显，抑或人们的抽象之概念，然吾之推测当人们所不明之结构也当如此） &emsp;&emsp;联系需要联系者，因为联系是联系者之间的联系。联系者具体地说就是具体事物，抽象地说就是联系环节，因而联系环节就是指抽象的联系者。联系的基本环节包括：系统和要素、结构和功能、形势和内容、本质和现象、原因和结果、内因和外因、质与量、肯定与否定。","text":"&emsp;&emsp;吾以为，事物之结构该当如下：（注意：这里的事物应当是能量的物质外显，抑或人们的抽象之概念，然吾之推测当人们所不明之结构也当如此） &emsp;&emsp;联系需要联系者，因为联系是联系者之间的联系。联系者具体地说就是具体事物，抽象地说就是联系环节，因而联系环节就是指抽象的联系者。联系的基本环节包括：系统和要素、结构和功能、形势和内容、本质和现象、原因和结果、内因和外因、质与量、肯定与否定。 一 系统和要素 任何系统本身既是个系统，又处于更大的系统之中。系统是由要素构成的，因而相对于更大系统而言的某一具体事物又成为构成更大系统的要素。 系统是由一定数量相互联系的要素资格，有特定结构、性质和功能的有机整体。 如上图中，可将这个大的系统结构的某个要素视为四个该要素之要素组成之系统（整体）：，四个要素之要素可视为相对这个要素（系统）之要素（部分）：，应当注意系统与要素之间的相对性。无论系统还是要素都是相对的。 二 结构和功能 宇宙万物不仅作为系统由一定要素所构成，而且构成一事物的要素因其相互联系而构成特定的结构，从而具有特定的功能。 结构是事物内在要素的联结方式。结构不仅具有整体性，也具有相对稳定性（根据皮亚杰的观点，结构的各要素互相制约、互为条件且不受外界因素的影响），因而具有转换型（结构的各要素可按一定规则互相替换而不改变结构本身）。注：关于相对稳定性，吾不以为然，吾以为结构之间的要素的关联是要受到外界的干预影响的。 功能是事物由其结构而产生的对外部对象或环境的作用能力，或者说，是事物通过对外部对象或环境的作用而表现出来的性能。 结构与功能之区别：结构属于事物内部的内部联系，而功能则是一事物通过与另一事物发生外部联系而产生或体现出来。 如最上图，这个大的系统的最上面的那个要素的结构是蓝色部分，： ，而这个要素的功能是绿色部分： 。 三 形式与内容 结构作为事物内在要素的联结方式也就是事物的形式，要素就是事物的形式所依存的内容。 所谓内容就是构成事物的一切内在要素的总和。内容有两大类：一是实体方面的（部分、成分），一是属性方面的（内在矛盾、发展趋势）。 所谓形式就是把食物的内在要素组合起来的结构。 形式和内容是有区别的，内容是事物存在的基础，相对于形式来说，它是“实”，是形式这张网上的“结”；形式则是事物存在的结构，相对于内容来说，它是“虚”，是编织内容的“网”。因此，内容和形式的关系可以说是实（结）与虚（网）的关系。 如最上图，这个大的系统的最上面的那个要素的内容即粉红色部分：，而形式实则为对应功能之结构，只不过形式对应的是内容，其实（形式与结构）实质一样，但对应之物不同，故侧重角度略有不同：。 四 本质和现象 现象是事物显现于外的形象或面貌。它是事物本质的诸方面表现，其实质是外部联系，就是说，一事物如果不与一定量的其他事物发生联系或相互作用，就没有现象可言。 本质是事物深藏于内的根本性质，其实质是事物内在的根本要素之必然联系，它是由事物的根本矛盾或特殊矛盾决定的。3.就本质与性质的关系来说，本质通常是就个体事物之为其自身而言的，性质是就个体的类属而言的。虽然本质与性质都很抽象，但性质比本质更抽象。 就本质与结构（形式）的关系来说，结构（形式）相对于功能也相对于要素（内容）而言，但本质仅相对于现象而言。现象不等于功能和要素，所以本质也不同于结构（形式）。本质和结构都很抽象，但相比而言，本质比结构更抽象（结构较本质更具体）。 从根本上说，本质与结构（形式）是一致的。 本质与结构（形式）的一致，在于他们都不能脱离要素（内容）而独立存在。 就本质与规律、趋势（必然性）的关系来说：三者虽然都是抽象的，但本质是从相对静态的角度来深刻认识事物的，他所要揭示的是事物之为事物的当前规定性；而规律和趋势（必然性）则是从相对动态的角度来深刻把握事物的，他所要解释的是事物未来的进程和归宿。不过，本质与规律和趋势（必然性0也是一致的：本质决定规律和趋势（必然性），规律和趋势（必然性）反映本质。就是说，一个事物的本质是怎么样的，其发展的进程（规律）和结局（趋势或必然性）就是怎么样的。 如最上图，系统之现象当如： ，，吾以为现象当为系统呈现出的外部状态。 系统之本质当如： 。 如此，圆圈和线条就构成了事物结构之四个基本环节。即： 圆圈（外）与圆圈（内）构成系统与要素。 圆圈（外）与线条（内）构成现象与本质。 线条（内）与线条（外）构成结构与功能。 线条（内）与圆圈（内）构成形式与内容。 从以上可知，圆圈内与线条外不构成直接联系。 联系的基本形式 联系的环节是要解决联系的主题承担者问题。显然，没有联系的环节，联系是无法想象的，是不可能存在的。但现实事物中的联系，不仅需要作为联系者的联系环节，也需要联系的形式。联系的形式就是要解决事物时怎样联系的问题。 联系的具体形式无限多样，但其基本形式是有限的。联系的基本形式，依其复杂程度可分为两类：相对简单的联系形式和相对复杂的联系形式。 相对简单的联系形式——内部联系和外部联系、纵向联系和横向联系、直接联系和间接联系。 相对复杂的联系形式——必然联系和偶然联系、因果联系和非因果联系。 所谓必然联系，就是事物在发展过程中所表现出的确定不移的趋势，它是事物内在的直接的本质的稳定的联系。必然联系的首要特征是确定性或稳定性，因为必然联系或必然性根源于事物的根本矛盾。也正因为如此，必然联系在事物存在的全局或发展的全程中居于支配地位。 所谓偶然联系，就是事物在发展过程中所表现出的并非确定不移的趋势（可以出现，也可以不出现；可以早出现，也可以晚出现；可以这样出现，也可以那样出现），它是事物外在的非本质的不稳定的联系。偶然联系的首要特征是不确定性或不稳定性，因为偶然联系或偶然性根源于事物的非根本矛盾或外部矛盾。也正因为如此，偶然联系在事物存在的全局或发展的全程中居于被支配的地位。 偶然联系和必然联系不仅同时存在于一个事物的发展过程之中，而且二者都有其具体形式。偶然联系（偶然现象）的具体形式在事物发展过程的开始、中间和结果三个基本阶段中都可能出现或存在。必然联系的具体形式主要有：有先后的历时性必然联系（因果联系）与无先后的共时性必然联系（函数关系）。 要知道某一事物或现象是原因还是结果，必须把它放在特定关系中才行。 可以把原因定义为能引起（能生）一定事物的事物（实体、状态、性质、过程），而结果可以定义为被一定事物引起（所生）的事物（实体、状态、性质、过程）。 因果联系有两个基本特征：（1）先后性（2）必然性。 因果联系是有先后性的必然联系。 世界万物是处于普遍联系之中的，没有孤立存在的事物。但普遍联系的事物之联系的环节和形式是多样性的，联系的类型也是多样的。联系的基本类型主要有：局部联系与全局联系，暂时联系与长久联系，简单联系与复杂联系，紧密联系与松散联系，客观联系与主观联系，主要联系（本质的联系）与次要联系（非本质的联系），等等。","categories":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/categories/哲学/"}],"tags":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/tags/哲学/"}]},{"title":"事物变化之缘由之己见","slug":"《智慧之根》笔记与心得（1）事物变化之缘由之己见","date":"2018-03-03T05:20:02.000Z","updated":"2019-01-07T03:12:25.944Z","comments":true,"path":"articles/《智慧之根》笔记与心得（1）事物变化之缘由之己见/","link":"","permalink":"http://www.iamlightsmile.com/articles/《智慧之根》笔记与心得（1）事物变化之缘由之己见/","excerpt":"&emsp;&emsp;唯物辩证法之基础思想在于事物——世间的万事万物都是彼此联系和不断发展的。而矛盾是最深刻最本质的联系，同时矛盾是事物变化发展的根本动力。那么到底何为矛盾，又为何矛盾？矛盾矛盾，以子之矛攻子之盾，虽然唯物辩证法中的矛盾并非现实的实物的矛和盾，但其中精妙也在于此，否则为何意译为矛盾呢。 &emsp;&emsp;吾之前不能理解矛盾之到底为何物遂去图书馆查阅相关书籍，然余性驽智愚，未尝更愈明晓也。 &emsp;&emsp;观本书p115处似有朦胧浅薄之理解，矛盾是既对立又同一的双重关系或属性，换句话说应当是一个事物的两个相反相成的性质或属性吧。也可看作是两个事物彼此之间相反相成，但是当这两个事物作为要素构成一个整体时，或者同时作为属性构成于一个统一的事物时，即又可看做是一个事物的内部矛盾。","text":"&emsp;&emsp;唯物辩证法之基础思想在于事物——世间的万事万物都是彼此联系和不断发展的。而矛盾是最深刻最本质的联系，同时矛盾是事物变化发展的根本动力。那么到底何为矛盾，又为何矛盾？矛盾矛盾，以子之矛攻子之盾，虽然唯物辩证法中的矛盾并非现实的实物的矛和盾，但其中精妙也在于此，否则为何意译为矛盾呢。 &emsp;&emsp;吾之前不能理解矛盾之到底为何物遂去图书馆查阅相关书籍，然余性驽智愚，未尝更愈明晓也。 &emsp;&emsp;观本书p115处似有朦胧浅薄之理解，矛盾是既对立又同一的双重关系或属性，换句话说应当是一个事物的两个相反相成的性质或属性吧。也可看作是两个事物彼此之间相反相成，但是当这两个事物作为要素构成一个整体时，或者同时作为属性构成于一个统一的事物时，即又可看做是一个事物的内部矛盾。 &emsp;&emsp;书中云矛盾之基本属性关系有二：1.对立性和同一性2.普遍性和特殊性。 &emsp;&emsp;书中p117页所提矛盾之对立性是事物内部两个根本相反方面的对立性。以鄙之愚见当为若一事物有a属性同时又具有！（非）a属性，则这个a和！a当为一对矛盾。拿人类性别这一抽象事物举例吧，人类性别包含男，女，双性，非男非女四个类别。之前只知晓男和女两种类别，后面两种是近来才发现和命名的。So，男之所以为男，正是因为女的存在，女的照样也是如此。正如老子《道德经》所云：”世人皆知美之为美 斯恶已 皆知善之为善 斯不善已“。这里怎么理解呢？在我看来应该是无论男和女作为一种抽象的概念对具体事物的划分抑或美和丑作为抽象概念对认知感觉的划分，他们都是由人们的观念所决定的，换句话说意思就是在没有“男”和“女”这两个概念出来之前，世上自然就是不分男女的，当有了这样的变化或者说是人们的认知需要，才出现了男和女这两个对立而又同一的概念，它们两者自诞生以来便是同时的，无时无刻不相互对立而又同一，对立性体现在男的对立面即为女，女的对立面即为男，同一性体现在男和女所共同组成的整体正是全部人类，更确切的是其性别属性。当然以上概念可能有些不准确，我这里举着个例子并非含有任何性别歧视的含义。 &emsp;&emsp;从另一个角度来看，人的性别属性也是男性或女性，然而更确切的来看应该是既含男性又有女性，何也，万事无绝对也。只是男（女）性的男（女)性基因或男(女）性表现在其所处环境下得到了充分的表达而已。 &emsp;&emsp;从更抽象的角度来理解矛盾的关键抑或核心即4个字“一分为二”。 &emsp;&emsp;正如下图：一条线总能将这个圆分成两部分。然而这条线之划分并非绝对的，两边也是动态发展的，发展过程中，既可能你中有我我中有你，同时也会相互转化。最好的例证既是阴阳太极图： &emsp;&emsp;然而阴阳太极图以其对称形式体现且是静态图，所以要体现其动态变化性应该再加上五行和四象，五行相生相克，当为太极阴阳调和变化之道。 &emsp;&emsp;此即中西关于矛盾观念之相合且高度一致。 而在《和谐辩证法》中，事物之变化当取决于能量供求原理。物质之运动有六大逻辑要素，即物质、能量、数量、结构、程序、信息。而物质有三大显著特征：1.质量实体2.能量母体3.运动载体。而能量作用有三种不同形式：能量相容、能量冲突和能量中立。同时其供求两极正好对应太极阴阳之两化。同时其核心观点当为“能量推动物质运动、物质运动释放能量” 而在看了关于宇宙大爆炸理论、黑洞、奇点以及空心菜创作的《世界就是一个游戏》以及其在腾讯视频中《用道德经破解宇宙起源》的观点和理念让余产生了新的观点和看法。 吾之观念当为宇宙之中当为处处都有能量，而整个宇宙的发展过程，正是宇宙间各能量间从不平衡到平衡的发展过渡过程。而所谓的物质，则是人类可观察的能量的一种特殊形式，可称之为能量的物质外显，同时能量也可以不表现为物质的形式，如场以及我们尚不知晓的形式等等。 吾之理解中，宇宙之始吾不知，宇宙之末吾不晓。或如空心菜般更高层智慧所创造，或如进化论般偶然之所进化，或如为何能量之不平衡至平衡，吾尚不明也。 然吾等人类仅为银河系太阳系地球数百亿年历程之小小一瞬，当有自知之明，而非自傲视己为万物之灵长，敢于天地造化同在。 若宇宙如空心菜所言，自当为一游戏，此生当以修心为主。 若宇宙如大爆炸所云，则另有一可能如高层智慧之一团烟火，自燃时起，至黯时灭。思想观念来源于《黑衣人》 而吾之所见，当为无论物质之外显，抑或思维中抽象之概念，其发展阶段都为自不平衡至平衡发展的状态。而之所以运动不止，发展不断，则是由于外界的作用，使事物又趋于不平衡。","categories":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/categories/哲学/"}],"tags":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/tags/哲学/"}]},{"title":"算法学习笔记（2）数组","slug":"算法学习笔记（2）数组","date":"2018-03-02T09:15:47.000Z","updated":"2019-01-07T03:18:23.032Z","comments":true,"path":"articles/算法学习笔记（2）数组/","link":"","permalink":"http://www.iamlightsmile.com/articles/算法学习笔记（2）数组/","excerpt":"学习自B站算法面试精讲 查找和排序 二分查找 元素交换 排序，中位数 归并 位运算 前缀和的应用 动态规划 排列组合","text":"学习自B站算法面试精讲 查找和排序 二分查找 元素交换 排序，中位数 归并 位运算 前缀和的应用 动态规划 排列组合","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"http://www.iamlightsmile.com/tags/算法/"}]},{"title":"算法学习笔记（1）字符串","slug":"算法学习笔记（1）字符串","date":"2018-03-02T08:18:14.000Z","updated":"2019-01-07T03:24:16.399Z","comments":true,"path":"articles/算法学习笔记（1）字符串/","link":"","permalink":"http://www.iamlightsmile.com/articles/算法学习笔记（1）字符串/","excerpt":"学习自https://www.bilibili.com/video/av11739347/ 算法面试精讲 和数组相关，内容广泛 概念理解：字典序 简单操作：插入、删除字符，旋转 规则判断（罗马数字转换、是否是合法的整数、浮点数） 数字运算（大数加法、二进制加法） 排序、交换（partition过程） 字符计数（hash）：变位词 匹配（正则表达式、全串匹配、KMP、周期判断） 动态规划（LCS、编辑距离、最长回文子串） 搜索（单词变换、排列组合）","text":"学习自https://www.bilibili.com/video/av11739347/ 算法面试精讲 和数组相关，内容广泛 概念理解：字典序 简单操作：插入、删除字符，旋转 规则判断（罗马数字转换、是否是合法的整数、浮点数） 数字运算（大数加法、二进制加法） 排序、交换（partition过程） 字符计数（hash）：变位词 匹配（正则表达式、全串匹配、KMP、周期判断） 动态规划（LCS、编辑距离、最长回文子串） 搜索（单词变换、排列组合）","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"http://www.iamlightsmile.com/tags/算法/"}]},{"title":"浏览器插件小试","slug":"浏览器插件小试","date":"2017-12-25T02:40:27.000Z","updated":"2019-01-07T03:17:31.365Z","comments":true,"path":"articles/浏览器插件小试/","link":"","permalink":"http://www.iamlightsmile.com/articles/浏览器插件小试/","excerpt":"由于时间比较久远，现在浏览器插件编写的工作都已经忘记了一大半，不过之前获取的只是图片的绝对地址，用起来还是不太方便，并且比如说插件的title、description、icon等都是原来的如testMenu等比较low的，于是后来又修改了一下，包括以上的项目属性，以及目前复制到剪贴板的直接是Markdown中的图片地址格式，如! [image_title] (image_absolute_path)的格式，并且已经把相关的源码上传到我的GitHub上面，链接地址为：https://github.com/smilelight/GithubImagePace 。","text":"由于时间比较久远，现在浏览器插件编写的工作都已经忘记了一大半，不过之前获取的只是图片的绝对地址，用起来还是不太方便，并且比如说插件的title、description、icon等都是原来的如testMenu等比较low的，于是后来又修改了一下，包括以上的项目属性，以及目前复制到剪贴板的直接是Markdown中的图片地址格式，如! [image_title] (image_absolute_path)的格式，并且已经把相关的源码上传到我的GitHub上面，链接地址为：https://github.com/smilelight/GithubImagePace 。 欢迎有需求的看客下载、使用、交流、分享等。","categories":[],"tags":[{"name":"浏览器插件","slug":"浏览器插件","permalink":"http://www.iamlightsmile.com/tags/浏览器插件/"}]},{"title":"关于Markdown引用图片地址的尝试","slug":"关于Markdown引用图片地址的尝试","date":"2017-12-24T14:43:44.000Z","updated":"2019-01-07T03:14:32.587Z","comments":true,"path":"articles/关于Markdown引用图片地址的尝试/","link":"","permalink":"http://www.iamlightsmile.com/articles/关于Markdown引用图片地址的尝试/","excerpt":"前两天某刻突然想到学习不能只有输入，更应该有输出，有时历程和心得更需要记录下来，于是就萌发起了写博客的念头，如今写博客的平台很多，比如博客园、CSDN、简书、新浪博客，甚至知乎等等，也可以通过GitHub pages来实现，甚至于购买云服务器搭建自己的博客网站。由于个人知识浅薄，并且想要拥有更高的自由度，以及一些其他因素限制，最终选择了通过GitHub pages+hexo来搭建自己的个人博客站点，借助于网上丰富详致的相关搭建博客教程终于把自己的小窝搭建起来了，吼吼！","text":"前两天某刻突然想到学习不能只有输入，更应该有输出，有时历程和心得更需要记录下来，于是就萌发起了写博客的念头，如今写博客的平台很多，比如博客园、CSDN、简书、新浪博客，甚至知乎等等，也可以通过GitHub pages来实现，甚至于购买云服务器搭建自己的博客网站。由于个人知识浅薄，并且想要拥有更高的自由度，以及一些其他因素限制，最终选择了通过GitHub pages+hexo来搭建自己的个人博客站点，借助于网上丰富详致的相关搭建博客教程终于把自己的小窝搭建起来了，吼吼！ 随后学习一些Markdown的语法发现这个图片的链接还真是一个问题，网上搜索的结果主要是通过七牛云啥的云图网站来实现的，然而一方面可能有数据量的限制，另外还要注册一些信息啥的，好屌麻烦，于是就想能不能直接通过GitHub仓库来实现呢？经过简单的分析之后，发现这个是可以实现的： 首先在GitHub上新建自己的图片仓库，以后的图片都可以放到这个里面。 随后可以把仓库克隆到本地计算机文件系统中，需要添加图片时，可以通过创建相关文件夹，然后在里面放入需要链接的图片。 通过Git命令实现仓库的提交。 123git add .git commit -m \"change the images\"git push 通过对应GitHub上图片资源绝对地址的引用实现自己Markdown文章图片的引用工作。 在实际应用中，以上第1步只需要做一次，而每次需要在Markdown文章引用图片时，都需要做第4步，同时视是否需要引入新的图片可能也需要2、3步的工作。 在具体实现的过程中，发现第3步和第四步存在着一些问题和待优化之处。如第3步的提交过程中，可以通过浏览器的GitHub仓库页面实现添加，也可以通过本地的Git命令调用，本地的Git命令调用也可分为图形式的命令调用和shell下的命令句调用，以上无论哪种操作，都比较繁琐，这是绝对不行的。 由于通过Git命令的操作主要就是:123git add .git commit -m \"change the images\"git push 这三步，所以很容易可以想到通过编写批处理文件来完成这样重复的工作。 所以我们可以创建一个名字为auto.bat的文件放到本地仓库的根目录下，里面的内容就是：123git add .git commit -m \"change the images\"git push 经过测试发现，如果能把命令行的显示尽量隐藏掉就好了，于是可以在开头加上一个@echo off关闭命令的回显，然而毕竟Git提交是将本地数据提交到远端，需要有一个过程，同时也应该有一个结果提示，所以基本上不可避免的还是会弹出显示上传结果的shell界面，如： 然而，其工作一旦完成，便自动退出，也算可以接受的吧。 以后把要上传的图片放进该文件夹下，然后双击auto.bat文件就可以啦！ 而在第4步下，发现直接通过右键获取图片链接地址，得到的并不是图片的绝对链接地址，不是很准确，直接拿来引用的话不行，我们还需要对路径进行进一步的处理才可以。 如：这张图片，如果直接复制的话，我们将得到的路径为:https://github.com/smilelight/images/blob/master/lightsmile.png(通过右击链接)或者https://github.com/smilelight/images/blob/master/lightsmile.png?raw=true(通过右击图片)，而其实际路径可以为：https://github.com/smilelight/images/raw/master/lightsmile.png或者https://raw.githubusercontent.com/smilelight/images/master/lightsmile.png或者https://github.com/smilelight/images/blob/master/lightsmile.png?raw=true。因此我们具体要做的工作也很简单，就是原本得到的路径转化为有效的绝对路径，我这里采取的策略是将其转化为第一种有效路径，即如果路径带有?raw=true，我就把它去掉，然后通过字符串替换，将blob替换为raw。工作是知道怎么做，可是到底怎么具体实现呢？写一个桌面小程序？之前查看知乎上有人就自己写了插件，我想我也可以试一试啊！说做就做，于是就上网寻找关于编写chormium拓展插件的博客教程和视频资源，经过一系列的尝试和探索之后，终于完成了功能简洁、有时还会报错的小插件。。。 如图： 这是在测试右键菜单的小插件的基础上编写的，名字懒得改了，反正也是自己用着玩的。它的功能是在GitHub网站下右击链接或图片，可以复制该图片的有效地址到剪贴板中，同时在浏览器的右下角或者左下角弹出notification(通知)如下： 经过测试发现确实是可以工作的，并且在不是GitHub下的网站下，右键是不会弹出这个菜单项的。关于我的具体的插件编写工作请参考我的另外一篇博文。","categories":[],"tags":[{"name":"markdown","slug":"markdown","permalink":"http://www.iamlightsmile.com/tags/markdown/"},{"name":"图片绝对地址","slug":"图片绝对地址","permalink":"http://www.iamlightsmile.com/tags/图片绝对地址/"},{"name":"浏览器插件","slug":"浏览器插件","permalink":"http://www.iamlightsmile.com/tags/浏览器插件/"},{"name":"github","slug":"github","permalink":"http://www.iamlightsmile.com/tags/github/"}]},{"title":"哲学小记","slug":"哲学小记","date":"2017-12-24T14:05:25.000Z","updated":"2019-01-07T03:15:02.743Z","comments":true,"path":"articles/哲学小记/","link":"","permalink":"http://www.iamlightsmile.com/articles/哲学小记/","excerpt":"&emsp;&emsp;人类社会永恒不变的主题便是生存与发展，如何能够更好的生存和发展，成为人类文明最强大、最根本的进步驱动力。 &emsp;&emsp;我们生活的世界是一个具体的现实的世界，它是非常复杂的，而人类智慧虽然发展迅速，但仍只能通过思维理解一些明显的客观规律，这里的理解也是相对于我们的思维逻辑而言的，世界的真正规律可能并不是这个样子，只是恰好被描述的比较准确罢了。 &emsp;&emsp;我们观察任何事物都是站在某个角度或某些角度上的，不能够全面透彻具体的了解某个事物，我们的逻辑便是不完全逻辑。而全面的又是困难的，有时也是没有必要的。","text":"&emsp;&emsp;人类社会永恒不变的主题便是生存与发展，如何能够更好的生存和发展，成为人类文明最强大、最根本的进步驱动力。 &emsp;&emsp;我们生活的世界是一个具体的现实的世界，它是非常复杂的，而人类智慧虽然发展迅速，但仍只能通过思维理解一些明显的客观规律，这里的理解也是相对于我们的思维逻辑而言的，世界的真正规律可能并不是这个样子，只是恰好被描述的比较准确罢了。 &emsp;&emsp;我们观察任何事物都是站在某个角度或某些角度上的，不能够全面透彻具体的了解某个事物，我们的逻辑便是不完全逻辑。而全面的又是困难的，有时也是没有必要的。 &emsp;&emsp;我们之所以会在某一认知系统中去定义一些事物，是因为它的属性不同于该认知系统中的其他事物，我们需要标记以加以区分，从此该标记就指代某个或一类事物，这种标记是人为规定的。 &emsp;&emsp;定义的标准和来源或来自于与系统中某些已定义事物的相同之处，抑或不同之处，符合某种映射规律，抑或简单的无逻辑映射。在复杂事物有机整体的定义上，也是自简单的无逻辑映射构建出复杂的有逻辑映射。有无逻辑是指事物之间是否存在某些定性的或者定量的关系，而这关系的确立也是建立在定义和观察之上的。 &emsp;&emsp;唯有建立在定义的基础上，方能进行逻辑活动，推演出某种事物内部或者事物与事物之间的某种定量的或者定性的属性或关系，即“规律”，也可以理解为具体的“道”，在此基础上，我们去适应和遵守甚至利用这种规律为我们所服务。 &emsp;&emsp;从某种程度上讲，我们认识任何事物的根本目的都是“为我所用”，为了解决某种问题，为了达到某种目的。 &emsp;&emsp;定义的基础和关键就在于抽象，抽象是目前为止我们人类认识世界的最强大的工具。 何为抽象？ 抽象，即特征提取并加以泛化的过程。 &emsp;&emsp;用自己的话说，就是我们在看待事物和考虑问题时，把重点只放在事物的相关属性上，而不去关注那些无关概念上，实现问题的由复杂到简单、由现象到本质的转化和事物的从特殊到一般的概念延伸和规律拓展，由此所得到的认识和规律适用于一系列的具有这样的本质的共性的事物，而不依赖于他们相互分别的非本质的个性。 &emsp;&emsp;抽象的意义在于它通过某些角度剥离出事物在某一问题或领域中的关键属性并屏蔽掉其他的不相关的或者相对不重要的属性，降低该事物在该问题域中的复杂程度，使得人们能够关注于事物的关键点从而实现问题聚焦的事物对象的简化工作，从而便于问题的解决。 &emsp;&emsp;许多事情能够抽象为数学问题，是因为存在某个具体的定义的量化标准，而那些过于复杂的，只能通过观察和试验来解决，只能做定性的抽象，抽象的具体了便是各具体科学，抽象的抽象了便是哲学。从这个角度而言，哲学可以描述数学，而数学无法描述哲学。 定义与抽象的关系 定性的抽象的尽头是哲学 定量的抽象的尽头是数学","categories":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/categories/哲学/"}],"tags":[{"name":"哲学","slug":"哲学","permalink":"http://www.iamlightsmile.com/tags/哲学/"},{"name":"抽象","slug":"抽象","permalink":"http://www.iamlightsmile.com/tags/抽象/"}]},{"title":"lua解析","slug":"lua解析","date":"2017-12-22T02:12:05.000Z","updated":"2019-01-07T03:13:37.623Z","comments":true,"path":"articles/lua解析/","link":"","permalink":"http://www.iamlightsmile.com/articles/lua解析/","excerpt":"本lua版本为5.2.1 在lua的源码中，lua.c 实现了可执行的解释器，用于解释执行.out文件，luac.c 实现了字节码的编译器，用于将.lua文件编译为.out文件，即字节码文件。更加宏观的东西，则在打印的两份文档里，此处不详述，由于整个读入解析到解释执行的过程是先从读入.lua文件开始的，所以我这份lua源码解析大业便先从这luac.c文件的阅读学习开始了，以下是它的main程序源代码。","text":"本lua版本为5.2.1 在lua的源码中，lua.c 实现了可执行的解释器，用于解释执行.out文件，luac.c 实现了字节码的编译器，用于将.lua文件编译为.out文件，即字节码文件。更加宏观的东西，则在打印的两份文档里，此处不详述，由于整个读入解析到解释执行的过程是先从读入.lua文件开始的，所以我这份lua源码解析大业便先从这luac.c文件的阅读学习开始了，以下是它的main程序源代码。 123456789101112131415int main(int argc, char* argv[])&#123; lua_State* L; int i=doargs(argc,argv); argc-=i; argv+=i; if (argc&lt;=0) usage(\"no input files given\"); L=luaL_newstate(); if (L==NULL) fatal(\"cannot create state: not enough memory\"); lua_pushcfunction(L,&amp;pmain); lua_pushinteger(L,argc); lua_pushlightuserdata(L,argv); if (lua_pcall(L,2,0,0)!=LUA_OK) fatal(lua_tostring(L,-1)); lua_close(L); return EXIT_SUCCESS;&#125; tip1：在C语言中，函数内部的局部变量需要在开头定义首先先定义了一个代表lua虚拟机的数据结构lua_State，然后执行参数解析工作，即调用doargs函数，并将命令行参数传了进去以下是其源代码。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#define IS(s) (strcmp(argv[i],s)==0)static int doargs(int argc, char* argv[])&#123; int i; int version=0; if (argv[0]!=NULL &amp;&amp; *argv[0]!=0) progname=argv[0]; for (i=1; i&lt;argc; i++) &#123; if (*argv[i]!='-') /* end of options; keep it */ break; else if (IS(\"--\")) /* end of options; skip it */ &#123; ++i; if (version) ++version; break; &#125; else if (IS(\"-\")) /* end of options; use stdin */ break; else if (IS(\"-l\")) /* list */ ++listing; else if (IS(\"-o\")) /* output file */ &#123; output=argv[++i]; if (output==NULL || *output==0 || (*output=='-' &amp;&amp; output[1]!=0)) usage(\"'-o' needs argument\"); if (IS(\"-\")) output=NULL; &#125; else if (IS(\"-p\")) /* parse only */ dumping=0; else if (IS(\"-s\")) /* strip debug information */ stripping=1; else if (IS(\"-v\")) /* show version */ ++version; else /* unknown option */ usage(argv[i]); &#125; if (i==argc &amp;&amp; (listing || !dumping)) &#123; dumping=0; argv[--i]=Output; &#125; if (version) &#123; printf(\"%s\\n\",LUA_COPYRIGHT); if (version==argc-1) exit(EXIT_SUCCESS); &#125; return i;&#125; 在说明doargs函数之前，先列出在luac.c文件中前面的局部变量：123456789#define PROGNAME \"luac\" /* default program name */#define OUTPUT PROGNAME \".out\" /* default output file */static int listing=0; /* list bytecodes? */static int dumping=1; /* dump bytecodes? */static int stripping=0; /* strip debug information? */static char Output[]=&#123; OUTPUT &#125;; /* default output file name */static const char* output=Output; /* actual output file name */static const char* progname=PROGNAME; /* actual program name */ 未完待续……","categories":[{"name":"lua","slug":"lua","permalink":"http://www.iamlightsmile.com/categories/lua/"}],"tags":[{"name":"lua","slug":"lua","permalink":"http://www.iamlightsmile.com/tags/lua/"}]}]}